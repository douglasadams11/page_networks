{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CU7UPB3a7Lzm"
   },
   "source": [
    "# NER inference\n",
    "\n",
    "This is a notebook for performing inference from the NER model on the list of pages being trialled for the new whole user journey approaches.\n",
    "\n",
    "Before you start, make sure your GPU is running.\n",
    "\n",
    "This version of the notebook aims to handle the no value exceptions that arise from scraping some pages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P2rz0J4t7OyV",
    "outputId": "6556fa1f-d530-4cca-8127-43ad686f7d85"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets seqeval >/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5D0sDrjA7uut",
    "outputId": "93cdecd8-a8de-4e64-d188-af69f2818823"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-08d49e908069>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdisplacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from urllib.request import urlopen\n",
    "import os\n",
    "from google.colab import drive\n",
    "import time\n",
    "\n",
    "drive.mount(\"/content/gdrive\")\n",
    "\n",
    "MODEL_DIR = os.path.join(\"/content/gdrive/Shared drives/\",\n",
    "                         \"GOV.UK teams/2020-2021/Data labs/content-metadata-2021/Models\"\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTYmb1Hm8Rj9"
   },
   "source": [
    "## Load model\n",
    "\n",
    "Choose a model. These are available here:\n",
    "\n",
    "https://drive.google.com/drive/folders/1-6n2iyiUpicm2BK4ybycJd2lY424U2BP \n",
    "\n",
    "Replace the `checkpoint` variable below if you wish to use a different model checkpoint.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YDvz1Ad28LPi"
   },
   "outputs": [],
   "source": [
    "checkpoint = 'distilbert-base-uncased-selfsupervised-ner-govuk-08-02-2022-govuk'\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2upVoJx58phT"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_PATH)\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "nlp = pipeline(\"ner\",\n",
    "               model=model,\n",
    "               tokenizer=tokenizer,\n",
    "               aggregation_strategy=\"first\",\n",
    "               device=0\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JfWZwMb_-EnP"
   },
   "source": [
    "## Inference test for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3fA4GTdE8rGG"
   },
   "outputs": [],
   "source": [
    "def show_entities(examples):\n",
    "  '''\n",
    "  This function visualises the named entities in some text.\n",
    "\n",
    "  Arguments:\n",
    "    examples: a list of strings, where each string is a document (e.g., sentence)\n",
    "  '''\n",
    "\n",
    "  # identify named entities using the model\n",
    "  ner_results = nlp(examples) \n",
    "\n",
    "  s = spacy.blank(\"en\")\n",
    "\n",
    "  # format the text and named entities to comply with displacy\n",
    "  for example, results in zip(examples, ner_results):\n",
    "    doc = s(example)\n",
    "\n",
    "    ents = []\n",
    "\n",
    "    if results:\n",
    "      for result in results: \n",
    "        ents.append(doc.char_span(result['start'],\n",
    "                                  result['end'],\n",
    "                                  result['entity_group']))\n",
    "      doc.ents = ents\n",
    "\n",
    "      displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "id": "8rOSG53O_BSb",
    "outputId": "06134a3d-56fe-42be-9f3e-692eac786157"
   },
   "outputs": [],
   "source": [
    "# In reality, each item in this list will be the parsed content from a page\n",
    "examples = [\"My name is David, I live in London and today is a Monday\",\n",
    "           \"Welcome to the jungle, my name is John and I am unable to receive Universal Credit\",\n",
    "           \"The DIO awarded contracts worth a total of £150 million to the firms to deliver a range of new buildings for service personnel from 1 and 16 Signal Regiment who have moved to Stafford from Germany.\",\n",
    "           \"Statement by Ambassador Karen Pierce, UK Permanent Representative to the UN, at the Security Council briefing on Women, Peace and Security.\",\n",
    "           \"PHE are warning pregnant women against using a potentially poisonous product, 'Calabash chalk', as a nutritional supplement or morning sickness ‘antidote’\",\n",
    "           \"How the government will make teaching an even higher status profession that attracts even more of the best graduates.\"]\n",
    "\n",
    "show_entities(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6ZuBWa2Nkh4"
   },
   "source": [
    "# Main script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AA8eSz2B_41H"
   },
   "outputs": [],
   "source": [
    "def extract_entities(examples):\n",
    "    '''\n",
    "    This function extracts the named entities into a pandas df.\n",
    "\n",
    "    Arguments:\n",
    "        - examples: a list of strings, where each string is a document (e.g., sentence)\n",
    "    '''\n",
    "    \n",
    "    # identify named entities using the model\n",
    "    ner_results = nlp(examples)\n",
    "    \n",
    "    # this line converts the list of lists of dictionaries to a pandas df\n",
    "    # with the keys of each dictionary as the columns and indexed to the \n",
    "    # webpage index and the entity index on that webpage\n",
    "    df = pd.DataFrame(ner_results).stack().apply(pd.Series)\n",
    "\n",
    "    # removes the columns `score`, `start` and `end` from the df as these are irrelevant in this case\n",
    "    df = df.drop(columns=[\"score\", \"start\", \"end\"])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iFKN-NUifWVS"
   },
   "outputs": [],
   "source": [
    "pages = [\"https://www.gov.uk/foreign-travel-advice/slovakia\",\n",
    "         \"https://www.gov.uk/foreign-travel-advice/ukraine\",\n",
    "         \"https://www.gov.uk/foreign-travel-advice/czech-republic\",\n",
    "         \"https://www.gov.uk/foreign-travel-advice/poland\",\n",
    "         \"https://www.gov.uk/foreign-travel-advice/hungary\",\n",
    "         \"https://www.gov.uk/guidance/find-help-and-support-if-you-have-long-covid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I2APqI1JqZvN"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# To pull in the list of webpages from a .csv file and convert them to a list\n",
    "with open('ER_pages_cleaned.csv', 'r') as f:\n",
    "    pages = [row[1] for row in csv.reader(f)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HNRgfE0qrMdw"
   },
   "outputs": [],
   "source": [
    "print(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m1yuWa8isfRE"
   },
   "outputs": [],
   "source": [
    "n = 2\n",
    "pages = pages[n:]\n",
    "print(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3OO-BrDiu6hV"
   },
   "outputs": [],
   "source": [
    "# pages = [\"https://www.gov.uk\" + s for s in pages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_bEi1wyaHEkT"
   },
   "outputs": [],
   "source": [
    "# necessary as these pages no longer exist and redirect to NHS\n",
    "pages.remove(\"https://www.gov.uk/find-covid-19-lateral-flow-test-site\")\n",
    "pages.remove(\"https://www.gov.uk/guidance/coronavirus-covid-19-getting-tested\")\n",
    "pages.remove(\"https://www.gov.uk/register-coronavirus-antibody-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qfhciX3PrGhV"
   },
   "outputs": [],
   "source": [
    "def extract_entities_to_csv(pages):\n",
    "    '''\n",
    "    This function extracts all the named-entities from the list of pages into\n",
    "    a .csv file in the format:\n",
    "\n",
    "    || page || index_of_named_entity || type_of_named_entity || word  ||\n",
    "    ||  0   ||           0           ||     organisation     ||  nhs  ||\n",
    "    ||  0   ||           1           ||         date         || today ||\n",
    "\n",
    "    Arguments:\n",
    "        - pages: a list of html links for pages\n",
    "    '''\n",
    "\n",
    "    # this scrapes the raw html of all the pages in `pages` into the `pages_html` list\n",
    "    pages_html = []\n",
    "    pages_id = []\n",
    "    \n",
    "    for page in range(len(pages)):\n",
    "        print(pages[page], page, \"/\", len(pages))\n",
    "        pages_html.append(urlopen(pages[page]).read().decode('utf-8'))\n",
    "\n",
    "        # this extracts all of the text from the html stored in `pages_html`\n",
    "        # this text is then appended to `data_extracted_strs`\n",
    "    \n",
    "        data_extracted_strs = []\n",
    "\n",
    "\n",
    "        for html in pages_html:\n",
    "            topic_soup = soup(html, \"html.parser\")  # necessary to convert to this form for extraction of text\n",
    "            \n",
    "            data = '' \n",
    "\n",
    "            # container = topic_soup.select_one('#wrapper')\n",
    "            b_tags = topic_soup.find_all(\"div\",{\"data-module\":\"govspeak\"})\n",
    "\n",
    "            text = ''.join(b.get_text(strip=True) for b in b_tags)\n",
    "\n",
    "            data_extracted_strs.append(text)\n",
    "\n",
    "        # for data in topic_soup.find_all(\"div\", {\"class\": {\"gem-c-govspeak govuk-govspeak \", \"gem-c-govspeak govuk-govspeak direction-ltr\"}}):  \n",
    "        # I think the argument of find_all below is the general wrapper for all text but I'm leaving the above commented incase it isn't\n",
    "        # for data in topic_soup.findAll(\"div\",{\"data-module\":\"govspeak\"}): \n",
    "            \n",
    "        #     data_extracted = data.get_text()\n",
    "            \n",
    "        #     # print(pages[page], len(data_extracted))\n",
    "\n",
    "        #     # print(\"Data extracted: \", len(data_extracted))\n",
    "        #     data_extracted_strs.append(str(data_extracted))  # it is necessary to convert to str format here for named-entity extraction\n",
    "            \n",
    "        #     pages_id.append(pages[page])\n",
    "\n",
    "    '''\n",
    "    - if `data_extracted` is less than e.g., 20 in size, need to not put it into data_extracted_strs\n",
    "    - need a way to retain unique id of page\n",
    "    '''\n",
    "    print(len(data_extracted_strs), len(pages))\n",
    "\n",
    "    # This line removes pages for which little/no text is extracted wheb scraping \n",
    "    # This prevents no-value exceptions when creating the df\n",
    "    data_extracted_strs_cleaned = [word for word in data_extracted_strs if len(word) >= 20]        \n",
    "\n",
    "    # getting the index of removed pages\n",
    "    def find_indices(lst):\n",
    "        return [i for i, elem in enumerate(lst) if len(elem) < 20]\n",
    "\n",
    "    data_extracted_idx_strs_removed = find_indices(data_extracted_strs)\n",
    "\n",
    "    # this finds and displays the named entities from the text stored in `data_extracted_strs`\n",
    "    # uncomment if want to display the named-entities, but will produce a long output if `pages` is long\n",
    "    # for i in range(len(data_extracted_strs)):\n",
    "    #     show_entities([data_extracted_strs[i]])\n",
    "\n",
    "    # this finds and extracts the named entities from the text stored in `data_extracted_strs`\n",
    "    df = extract_entities(data_extracted_strs_cleaned)\n",
    "\n",
    "    # return df, data_extracted_idx_strs_removed, data_extracted_strs_cleaned, pages_id\n",
    "    return df, data_extracted_strs, data_extracted_idx_strs_removed, data_extracted_strs_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fWWpar8bLFsq"
   },
   "outputs": [],
   "source": [
    "# df = extract_entities_to_csv(pages[1:10])\n",
    "df, data_extracted_strs, data_extracted_idx_strs_removed, data_extracted_strs_cleaned = extract_entities_to_csv(pages[:1001])\n",
    "# len(pages) # = 10,627\n",
    "#  none type error in 51-100 and 101-150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GhHe4jtJM2os"
   },
   "outputs": [],
   "source": [
    "df\n",
    "# indexes_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "At8lnARY0LsD"
   },
   "outputs": [],
   "source": [
    "pages_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xIHut-p8QQCj",
    "outputId": "71158bd3-1e2c-4a9f-8ad7-92acc2c0709f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 3,\n",
       " 14,\n",
       " 15,\n",
       " 21,\n",
       " 26,\n",
       " 28,\n",
       " 33,\n",
       " 37,\n",
       " 41,\n",
       " 42,\n",
       " 50,\n",
       " 63,\n",
       " 65,\n",
       " 67,\n",
       " 70,\n",
       " 73,\n",
       " 78,\n",
       " 80,\n",
       " 92,\n",
       " 95,\n",
       " 96]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_extracted_idx_strs_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ijpf-1pcLKv9"
   },
   "outputs": [],
   "source": [
    "df.to_csv('df_1_100.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gdbr9MXoORsJ"
   },
   "outputs": [],
   "source": [
    "indexes_removed = pd.DataFrame(indexes_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ddNQ7Mt6OeTG"
   },
   "outputs": [],
   "source": [
    "indexes_removed.to_csv('indexes_removed_1_100.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zdRY8E0bOf9d"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NER no-value exception",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
