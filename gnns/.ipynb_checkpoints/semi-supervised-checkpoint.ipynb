{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1091,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('../data/inputs/NER2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape\n",
    "print('NER', df.shape[0])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating column name\n",
    "df.rename(columns = {'Unnamed: 0':'index'}, inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing null values (pages with no relevant named entities) with zeroes\n",
    "df = df.fillna(0)\n",
    "# Replacing any anomolous entries with 0\n",
    "df = df.replace(r'^\\s*$', 0, regex=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we have no duplicate nodes\n",
    "max(df[\"url\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the index\n",
    "df.set_index('index')\n",
    "df = df.drop(columns = ['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy\n",
    "x = df.to_numpy(dtype=np.compat.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10614, 1556)\n",
      "[[    0     0     0 ...     0     0     0]\n",
      " [    1     0     0 ...     0     0     0]\n",
      " [    2     0     0 ...     0     0     0]\n",
      " ...\n",
      " [10611     0     0 ...     0     0     0]\n",
      " [10612     0     0 ...     0     0     0]\n",
      " [10613     0     0 ...     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>url</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://www.gov.uk/view-prove-immigration-status</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://www.gov.uk/prove-right-to-work</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>https://www.gov.uk/browse/working</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>https://www.gov.uk/find-a-job</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>https://www.gov.uk/check-state-pension</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                               url  label\n",
       "0      0  https://www.gov.uk/view-prove-immigration-status      0\n",
       "1      1            https://www.gov.uk/prove-right-to-work      1\n",
       "2      2                 https://www.gov.uk/browse/working      1\n",
       "3      3                     https://www.gov.uk/find-a-job      1\n",
       "4      4            https://www.gov.uk/check-state-pension      0"
      ]
     },
     "execution_count": 1308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bringing in the node classification labels \n",
    "df_labels = pd.read_csv(\"../data/inputs/pages_ranked_with_data_labelled.csv\")\n",
    "# df_labels = df_labels[[\"page path\", \"label\"]]\n",
    "df_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to have labels for each node (economic recovery or not economic recovery)\n",
    "'''\n",
    "In the form:\n",
    "\n",
    "|| page_link   || relevant (y/n) ||\n",
    "|| ------------||----------------||\n",
    "|| page_link_0 ||        1       ||\n",
    "|| page_link_1 ||        0       ||\n",
    "|| page_link_2 ||        1       ||\n",
    "...\n",
    "'''\n",
    "# Make sure we have no duplicate nodes\n",
    "max(df_labels[\"url\"].value_counts())\n",
    "\n",
    "# Remove irrelevant columns\n",
    "df_labels = df_labels.drop(columns=[\"url\", \"index\"])\n",
    "\n",
    "df_labels.shape[0]\n",
    "\n",
    "# To numpy again... \n",
    "# y = df_labels.to_numpy(dtype=np.compat.long)\n",
    "y = df_labels.label.tolist()\n",
    "# print(y.shape)\n",
    "print(y[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating train/val/test idx masks\n",
    "tm = pd.read_csv(\"../data/inputs/test_masks.csv\")\n",
    "tm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving masks into lists\n",
    "train_mask = tm.index[0:100].tolist()\n",
    "val_mask = tm.index[100:224].tolist()\n",
    "test_mask = tm.index[224:].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 1312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(val_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now creating the edge df\n",
    "df_edges = pd.read_csv(\"../edge_list.csv\")\n",
    "df_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1314,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Some pages were removed from the list prior to NER work\n",
    "due to the pae no longer existing or there being < 20 \n",
    "characters in the page's text field.\n",
    "\n",
    "It is now necessary to remove these nodes from the edge dataset,\n",
    "as this was attained from N2V approach.\n",
    "'''\n",
    "\n",
    "# Creating a list of pages to be removed\n",
    "pages_to_remove = [\n",
    "    \"/\",\n",
    "    \"/search/all\",\n",
    "    \"/find-covid-19-lateral-flow-test-site\",\n",
    "    \"/guidance/coronavirus-covid-19-getting-tested\",\n",
    "    \"/register-coronavirus-antibody-test\",\n",
    "    \"/entering-staying-uk/foreign-nationals-working-in-uk\",\n",
    "    \"/business-finance-support/business-cash-advance-uk\",\n",
    "    \"/government/publications/applying-to-the-register-of-apprenticeship-training-providers-roatp\",\n",
    "    \"/guidance/esfa-business-operations-help-and-support\",\n",
    "    \"/business-finance-support/business-growth-calderdale\",\n",
    "    \"/business-finance-support/low-carbon-workspaces-buckinghamshire\",\n",
    "    \"/log-test-site-covid19-results\",\n",
    "    \"/guidance/apprenticeships-resources-for-teachers-and-advisers\",\n",
    "    \"/business-finance-support/south-east-creatives-seccads\",\n",
    "    \"/business-finance-support/construction-industry-training-board-citb-grants-scheme-england\",\n",
    "    \"/government/publications/turkey-list-of-lawyers/list-of-lawyers-in-ankara-and-gaziantep\",\n",
    "    \"/business-finance-support/agri-tech-cornwall-cornwall-and-the-isles-of-scilly\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing pages from sources nodes\n",
    "df_edges_1 = df_edges[~df_edges[\"source\"].isin(pages_to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing pages from target nodes\n",
    "df_edges_2 = df_edges_1[~df_edges_1[\"target\"].isin(pages_to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing null rows\n",
    "df_edges_3 = df_edges_2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71781"
      ]
     },
     "execution_count": 1318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now need to extract the edges\n",
    "''' \n",
    "In the form:\n",
    "\n",
    "[[0, 0, 0, 0, 0 , 0 , 0 , ...],\n",
    " [1, 5, 7, 9, 11, 14, 16, ...]]\n",
    "\n",
    "Where this represents links existing between page 0 and 1, 5, 7, 9, 11, 14, 16...\n",
    "'''\n",
    "\n",
    "# gives df with edge weights (3 rows)\n",
    "# want to use this later when get all working\n",
    "# df_edges_3 = df_edges_3.drop(columns=[\"index\", \"source\", \"target\"])\n",
    "\n",
    "# gives df without edge weights (2 rows)\n",
    "df_edges_3 = df_edges_3.drop(columns=[\"index\", \"source\", \"target\", \"edgeWeight\"])\n",
    "\n",
    "\n",
    "df_edges_3.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transposing and changing type\n",
    "df_edges = df_edges_3.transpose()\n",
    "df_edges = df_edges.astype(np.compat.long)\n",
    "df_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To numpy again\n",
    "z = df_edges.to_numpy(dtype=np.compat.long)\n",
    "print(z.shape)\n",
    "print(z[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating pytorch-geometric dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1321,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the number of classes\n",
    "# In this case 1 = relevant, 0 = not relevant\n",
    "# Therefore, 2 classes\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting np arrays to tensors\n",
    "x = torch.Tensor(x)\n",
    "y = torch.Tensor(y)\n",
    "edge_index = torch.Tensor(z)\n",
    "# train_mask = torch.Tensor(train_mask)\n",
    "# val_mask = torch.Tensor(val_mask)\n",
    "# test_mask = torch.Tensor(test_mask)\n",
    "num_classes = torch.Tensor(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing tensor type from float to long\n",
    "# Uncomment for GCN\n",
    "x = x.type(torch.LongTensor)\n",
    "y = y.type(torch.LongTensor)\n",
    "edge_index = edge_index.type(torch.LongTensor)\n",
    "num_classes = num_classes.type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_mask.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating pytorch-geometric dataset\n",
    "dataset = Data(x = x, \n",
    "            edge_index=edge_index,\n",
    "            y=y,\n",
    "            # train_mask=train_mask,\n",
    "            # val_mask=val_mask,\n",
    "            # test_mask=test_mask,\n",
    "            num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 1327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.x\n",
    "type(dataset.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1328,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.x = dataset.x.type(torch.long)\n",
    "dataset.y = dataset.y.type(torch.long)\n",
    "dataset.edge_index = dataset.edge_index.type(torch.long)\n",
    "dataset.num_classes = dataset.num_classes.type(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 1329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.edge_index.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1330,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1331,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(pd.Series(list(x[:,0])), train_size=0.02117, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1333,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = torch.zeros(10614, dtype=torch.bool)\n",
    "test_mask = torch.zeros(10614, dtype=torch.bool)\n",
    "train_mask[X_train.index] = True\n",
    "test_mask[X_test.index] = True\n",
    "dataset['train_mask'] = train_mask\n",
    "dataset['test_mask'] = test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset.train_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather some statistics about the graph.\n",
    "print(f'Number of nodes: {dataset.num_nodes}')\n",
    "print(f'Number of edges: {dataset.num_edges}')\n",
    "print(f'Average node degree: {dataset.num_edges / dataset.num_nodes:.2f}')\n",
    "print(f'Number of training nodes: {dataset.train_mask.sum()}')\n",
    "print(f'Training node label rate: {int(dataset.train_mask.sum()) / dataset.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {dataset.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {dataset.has_self_loops()}')\n",
    "print(f'Is undirected: {dataset.is_undirected()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building out the GNN\n",
    "\n",
    "https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1337,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "from torch_geometric.nn import GCNConv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_geometric.datasets import Planetoid\n",
    "\n",
    "# dataset = Planetoid(root='/tmp/Cora', name='Cora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating GCN\n",
    "from numpy import dtype\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(1556, 16)\n",
    "        self.conv2 = GCNConv(16, 2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        \n",
    "        x, edge_index = data.x.type(dtype=torch.float64), data.edge_index.type(dtype=torch.int64)\n",
    "        # x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 200.0054\n",
      "Epoch: 001, Loss: 105.5870\n",
      "Epoch: 002, Loss: 10.0756\n",
      "Epoch: 003, Loss: 21.5778\n",
      "Epoch: 004, Loss: 39.8721\n",
      "Epoch: 005, Loss: 22.8183\n",
      "Epoch: 006, Loss: 13.5297\n",
      "Epoch: 007, Loss: 12.5046\n",
      "Epoch: 008, Loss: 20.8097\n",
      "Epoch: 009, Loss: 11.1051\n",
      "Epoch: 010, Loss: 15.0373\n",
      "Epoch: 011, Loss: 12.8902\n",
      "Epoch: 012, Loss: 11.4885\n",
      "Epoch: 013, Loss: 12.9546\n",
      "Epoch: 014, Loss: 12.7748\n",
      "Epoch: 015, Loss: 14.5309\n",
      "Epoch: 016, Loss: 11.9609\n",
      "Epoch: 017, Loss: 16.5935\n",
      "Epoch: 018, Loss: 9.0783\n",
      "Epoch: 019, Loss: 15.0847\n",
      "Epoch: 020, Loss: 13.3456\n",
      "Epoch: 021, Loss: 12.7321\n",
      "Epoch: 022, Loss: 15.1803\n",
      "Epoch: 023, Loss: 11.0215\n",
      "Epoch: 024, Loss: 12.2430\n",
      "Epoch: 025, Loss: 13.6810\n",
      "Epoch: 026, Loss: 9.0897\n",
      "Epoch: 027, Loss: 7.6513\n",
      "Epoch: 028, Loss: 9.7775\n",
      "Epoch: 029, Loss: 9.6132\n",
      "Epoch: 030, Loss: 9.5323\n",
      "Epoch: 031, Loss: 7.2759\n",
      "Epoch: 032, Loss: 8.5851\n",
      "Epoch: 033, Loss: 9.5027\n",
      "Epoch: 034, Loss: 8.6761\n",
      "Epoch: 035, Loss: 6.7980\n",
      "Epoch: 036, Loss: 9.5635\n",
      "Epoch: 037, Loss: 8.0647\n",
      "Epoch: 038, Loss: 6.8204\n",
      "Epoch: 039, Loss: 5.3746\n",
      "Epoch: 040, Loss: 5.0944\n",
      "Epoch: 041, Loss: 4.4255\n",
      "Epoch: 042, Loss: 3.0564\n",
      "Epoch: 043, Loss: 3.7852\n",
      "Epoch: 044, Loss: 4.3248\n",
      "Epoch: 045, Loss: 2.9478\n",
      "Epoch: 046, Loss: 2.7649\n",
      "Epoch: 047, Loss: 3.5028\n",
      "Epoch: 048, Loss: 3.6450\n",
      "Epoch: 049, Loss: 4.3200\n",
      "Epoch: 050, Loss: 3.9572\n",
      "Epoch: 051, Loss: 1.5395\n",
      "Epoch: 052, Loss: 5.8035\n",
      "Epoch: 053, Loss: 2.3161\n",
      "Epoch: 054, Loss: 3.8735\n",
      "Epoch: 055, Loss: 4.6912\n",
      "Epoch: 056, Loss: 5.1369\n",
      "Epoch: 057, Loss: 3.6686\n",
      "Epoch: 058, Loss: 1.4544\n",
      "Epoch: 059, Loss: 0.6862\n",
      "Epoch: 060, Loss: 0.6871\n",
      "Epoch: 061, Loss: 0.6877\n",
      "Epoch: 062, Loss: 0.6880\n",
      "Epoch: 063, Loss: 0.6881\n",
      "Epoch: 064, Loss: 0.6880\n",
      "Epoch: 065, Loss: 0.6877\n",
      "Epoch: 066, Loss: 0.6872\n",
      "Epoch: 067, Loss: 0.6865\n",
      "Epoch: 068, Loss: 0.6858\n",
      "Epoch: 069, Loss: 0.6849\n",
      "Epoch: 070, Loss: 0.6838\n",
      "Epoch: 071, Loss: 0.6828\n",
      "Epoch: 072, Loss: 0.6816\n",
      "Epoch: 073, Loss: 0.6804\n",
      "Epoch: 074, Loss: 0.6791\n",
      "Epoch: 075, Loss: 0.6778\n",
      "Epoch: 076, Loss: 0.6764\n",
      "Epoch: 077, Loss: 0.6750\n",
      "Epoch: 078, Loss: 0.6737\n",
      "Epoch: 079, Loss: 0.6723\n",
      "Epoch: 080, Loss: 0.6709\n",
      "Epoch: 081, Loss: 0.6695\n",
      "Epoch: 082, Loss: 0.6682\n",
      "Epoch: 083, Loss: 0.6668\n",
      "Epoch: 084, Loss: 0.6655\n",
      "Epoch: 085, Loss: 0.6642\n",
      "Epoch: 086, Loss: 0.6630\n",
      "Epoch: 087, Loss: 0.6617\n",
      "Epoch: 088, Loss: 0.6605\n",
      "Epoch: 089, Loss: 0.6593\n",
      "Epoch: 090, Loss: 0.6582\n",
      "Epoch: 091, Loss: 0.6571\n",
      "Epoch: 092, Loss: 0.6560\n",
      "Epoch: 093, Loss: 0.6550\n",
      "Epoch: 094, Loss: 0.6540\n",
      "Epoch: 095, Loss: 0.6530\n",
      "Epoch: 096, Loss: 0.6521\n",
      "Epoch: 097, Loss: 0.6512\n",
      "Epoch: 098, Loss: 0.6503\n",
      "Epoch: 099, Loss: 0.6495\n",
      "Epoch: 100, Loss: 0.6487\n",
      "Epoch: 101, Loss: 0.6480\n",
      "Epoch: 102, Loss: 0.6472\n",
      "Epoch: 103, Loss: 0.6466\n",
      "Epoch: 104, Loss: 0.6459\n",
      "Epoch: 105, Loss: 0.6453\n",
      "Epoch: 106, Loss: 0.6447\n",
      "Epoch: 107, Loss: 0.6441\n",
      "Epoch: 108, Loss: 0.6435\n",
      "Epoch: 109, Loss: 0.6430\n",
      "Epoch: 110, Loss: 0.6425\n",
      "Epoch: 111, Loss: 0.6421\n",
      "Epoch: 112, Loss: 0.6416\n",
      "Epoch: 113, Loss: 0.6412\n",
      "Epoch: 114, Loss: 0.6408\n",
      "Epoch: 115, Loss: 0.6404\n",
      "Epoch: 116, Loss: 0.6401\n",
      "Epoch: 117, Loss: 0.6397\n",
      "Epoch: 118, Loss: 0.6394\n",
      "Epoch: 119, Loss: 0.6391\n",
      "Epoch: 120, Loss: 0.6388\n",
      "Epoch: 121, Loss: 0.6385\n",
      "Epoch: 122, Loss: 0.6383\n",
      "Epoch: 123, Loss: 0.6380\n",
      "Epoch: 124, Loss: 0.6378\n",
      "Epoch: 125, Loss: 0.6376\n",
      "Epoch: 126, Loss: 0.6374\n",
      "Epoch: 127, Loss: 0.6372\n",
      "Epoch: 128, Loss: 0.6370\n",
      "Epoch: 129, Loss: 0.6369\n",
      "Epoch: 130, Loss: 0.6367\n",
      "Epoch: 131, Loss: 0.6365\n",
      "Epoch: 132, Loss: 0.6364\n",
      "Epoch: 133, Loss: 0.6363\n",
      "Epoch: 134, Loss: 0.6361\n",
      "Epoch: 135, Loss: 0.6360\n",
      "Epoch: 136, Loss: 0.6359\n",
      "Epoch: 137, Loss: 0.6358\n",
      "Epoch: 138, Loss: 0.6357\n",
      "Epoch: 139, Loss: 0.6356\n",
      "Epoch: 140, Loss: 0.6356\n",
      "Epoch: 141, Loss: 0.6355\n",
      "Epoch: 142, Loss: 0.6354\n",
      "Epoch: 143, Loss: 0.6353\n",
      "Epoch: 144, Loss: 0.6353\n",
      "Epoch: 145, Loss: 0.6352\n",
      "Epoch: 146, Loss: 0.6352\n",
      "Epoch: 147, Loss: 0.6351\n",
      "Epoch: 148, Loss: 0.6350\n",
      "Epoch: 149, Loss: 0.6350\n",
      "Epoch: 150, Loss: 0.6350\n",
      "Epoch: 151, Loss: 0.6349\n",
      "Epoch: 152, Loss: 0.6349\n",
      "Epoch: 153, Loss: 0.6349\n",
      "Epoch: 154, Loss: 0.6348\n",
      "Epoch: 155, Loss: 0.6348\n",
      "Epoch: 156, Loss: 0.6348\n",
      "Epoch: 157, Loss: 0.6347\n",
      "Epoch: 158, Loss: 0.6347\n",
      "Epoch: 159, Loss: 0.6347\n",
      "Epoch: 160, Loss: 0.6347\n",
      "Epoch: 161, Loss: 0.6347\n",
      "Epoch: 162, Loss: 0.6346\n",
      "Epoch: 163, Loss: 0.6346\n",
      "Epoch: 164, Loss: 0.6346\n",
      "Epoch: 165, Loss: 0.6346\n",
      "Epoch: 166, Loss: 0.6346\n",
      "Epoch: 167, Loss: 0.6346\n",
      "Epoch: 168, Loss: 0.6346\n",
      "Epoch: 169, Loss: 0.6345\n",
      "Epoch: 170, Loss: 0.6345\n",
      "Epoch: 171, Loss: 0.6345\n",
      "Epoch: 172, Loss: 0.6345\n",
      "Epoch: 173, Loss: 0.6345\n",
      "Epoch: 174, Loss: 0.6345\n",
      "Epoch: 175, Loss: 0.6345\n",
      "Epoch: 176, Loss: 0.6345\n",
      "Epoch: 177, Loss: 0.6345\n",
      "Epoch: 178, Loss: 0.6345\n",
      "Epoch: 179, Loss: 0.6345\n",
      "Epoch: 180, Loss: 0.6345\n",
      "Epoch: 181, Loss: 0.6345\n",
      "Epoch: 182, Loss: 0.6345\n",
      "Epoch: 183, Loss: 0.6345\n",
      "Epoch: 184, Loss: 0.6345\n",
      "Epoch: 185, Loss: 0.6345\n",
      "Epoch: 186, Loss: 0.6345\n",
      "Epoch: 187, Loss: 0.6345\n",
      "Epoch: 188, Loss: 0.6345\n",
      "Epoch: 189, Loss: 0.6344\n",
      "Epoch: 190, Loss: 0.6344\n",
      "Epoch: 191, Loss: 0.6344\n",
      "Epoch: 192, Loss: 0.6344\n",
      "Epoch: 193, Loss: 0.6344\n",
      "Epoch: 194, Loss: 0.6344\n",
      "Epoch: 195, Loss: 0.6344\n",
      "Epoch: 196, Loss: 0.6344\n",
      "Epoch: 197, Loss: 0.6344\n",
      "Epoch: 198, Loss: 0.6344\n",
      "Epoch: 199, Loss: 0.6344\n",
      "Epoch: 200, Loss: 0.6344\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "model = GCN().to(device)\n",
    "d = dataset.to(device)\n",
    "# print(type(d))\n",
    "# d = d.type(torch.LongTensor)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-5)\n",
    "\n",
    "model.train()\n",
    "model = model.double()\n",
    "for epoch in range(201):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(d)\n",
    "    loss = F.nll_loss(out[d.train_mask], d.y[d.train_mask])\n",
    "    loss.backward()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "pred = model(dataset).argmax(dim=1)\n",
    "correct = (pred[dataset.test_mask] == dataset.y[dataset.test_mask]).sum()\n",
    "acc = int(correct) / int(dataset.test_mask.sum())\n",
    "print(f'Accuracy: {acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "feb72aa86adf3dbe491716fb35fe8b95aef07c373b2aed386ae31cecd3b83cf2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
