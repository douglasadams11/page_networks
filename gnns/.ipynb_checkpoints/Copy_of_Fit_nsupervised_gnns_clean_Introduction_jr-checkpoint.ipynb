{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zF5bw3m9UrMy",
    "outputId": "c7dfa774-349b-4edc-dcea-381f0f6a9429"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1+cu113\n",
      "\u001b[K     |████████████████████████████████| 7.9 MB 7.0 MB/s \n",
      "\u001b[K     |████████████████████████████████| 3.5 MB 6.1 MB/s \n",
      "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "# Install required packages.\n",
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
    "\n",
    "# Helper function for visualization.\n",
    "%matplotlib inline\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jeQixhCfV4Y8"
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "from torch._C import parse_schema\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import NNConv, global_add_pool\n",
    "import torch.nn as nn\n",
    "\n",
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from itertools import chain\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0RMLoX_BCcj4"
   },
   "outputs": [],
   "source": [
    "# Source : https://pytorch-geometric.readthedocs.io/en/latest/notes/colabs.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MXIOHMI0R3CY",
    "outputId": "09ea0322-a86c-4ceb-c481-9a4634175d7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R8VKDT-dR-UF"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_pickle_file(file_name, dir_path=\"./data/outputs\"):\n",
    "    file_path = Path(dir_path + \"/\" + file_name)\n",
    "    f = open(file_path, 'rb')\n",
    "    file = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    return (file)\n",
    "\n",
    "\n",
    "G2 = load_pickle_file(\"Graph_er\", dir_path=\"/content/gdrive/MyDrive/GDS/pickles/\")\n",
    "A2 = load_pickle_file(\"Adjacency_er\", dir_path=\"/content/gdrive/MyDrive/GDS/pickles/\")\n",
    "T2 = load_pickle_file(\"Transition_er\", dir_path=\"/content/gdrive/MyDrive/GDS/pickles/\")\n",
    "matrix_features = load_pickle_file(\"matrix_features\", dir_path=\"/content/gdrive/MyDrive/GDS/pickles/\")\n",
    "\n",
    "G3 = load_pickle_file(\"Graph_er_weighted_wfeatures\", dir_path=\"/content/gdrive/MyDrive/GDS/pickles/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20whX7By-Q0t"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_df= pd.read_csv(\"/content/gdrive/MyDrive/GDS/pickles/train_all_v2.csv\")\n",
    "val_df = pd.read_csv(\"/content/gdrive/MyDrive/GDS/pickles/val_v2.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oxhPRb1MKwRE"
   },
   "outputs": [],
   "source": [
    "labelled_df = pd.concat([train_df, val_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bjgwb4vP6-W1"
   },
   "outputs": [],
   "source": [
    "pages_to_remove = [\n",
    "    \"/\",\n",
    "    \"/search/all\",\n",
    "    \"/find-covid-19-lateral-flow-test-site\",\n",
    "    \"/guidance/coronavirus-covid-19-getting-tested\",\n",
    "    \"/register-coronavirus-antibody-test\",\n",
    "    \"/entering-staying-uk/foreign-nationals-working-in-uk\",\n",
    "    \"/business-finance-support/business-cash-advance-uk\",\n",
    "    \"/government/publications/applying-to-the-register-of-apprenticeship-training-providers-roatp\",\n",
    "    \"/guidance/esfa-business-operations-help-and-support\",\n",
    "    \"/business-finance-support/business-growth-calderdale\",\n",
    "    \"/business-finance-support/low-carbon-workspaces-buckinghamshire\",\n",
    "    \"/log-test-site-covid19-results\",\n",
    "    \"/guidance/apprenticeships-resources-for-teachers-and-advisers\",\n",
    "    \"/business-finance-support/south-east-creatives-seccads\",\n",
    "    \"/business-finance-support/construction-industry-training-board-citb-grants-scheme-england\",\n",
    "    \"/government/publications/turkey-list-of-lawyers/list-of-lawyers-in-ankara-and-gaziantep\",\n",
    "    \"/business-finance-support/agri-tech-cornwall-cornwall-and-the-isles-of-scilly\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OAx36KKLkSG7"
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OC0ibwBX1Sip"
   },
   "source": [
    "## Graph object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jh5QwRQcszqC"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Graph():\n",
    "    \n",
    "    def __init__(self, nx_G, is_directed, p, q):\n",
    "        self.G = nx_G\n",
    "        self.is_directed = is_directed\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "\n",
    "    def node2vec_walk(self, walk_length, start_node):\n",
    "        '''\n",
    "        Simulate a random walk starting from start node.\n",
    "        '''\n",
    "        G = self.G\n",
    "        alias_nodes = self.alias_nodes\n",
    "        alias_edges = self.alias_edges\n",
    "\n",
    "        walk = [start_node]\n",
    "\n",
    "        while len(walk) < walk_length:\n",
    "            cur = walk[-1]                            # current position (node)\n",
    "            cur_nbrs = sorted(G.neighbors(cur))       # neighbs of current node\n",
    "            if len(cur_nbrs) > 0:\n",
    "                \n",
    "                if len(walk) == 1:\n",
    "                    # for the first step of the walk: no previous node\n",
    "                    # hence we draw only from the connected edges\n",
    "                    walk.append(cur_nbrs[alias_draw(alias_nodes[cur][0], alias_nodes[cur][1])])\n",
    "                else:\n",
    "                    # for further steps, precious node edges\n",
    "                    # and hence w ecan use Node2Vec modified transition probabilities\n",
    "                    prev = walk[-2]                   # previous position\n",
    "                    next = cur_nbrs[alias_draw(alias_edges[(prev, cur)][0],  \n",
    "                        alias_edges[(prev, cur)][1])]\n",
    "                    walk.append(next)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return walk\n",
    "    \n",
    "    \n",
    "    def simulate_walks(self, num_walks, walk_length, start_nodes = None):\n",
    "        '''\n",
    "        Repeatedly simulate random walks from each node.\n",
    "        '''\n",
    "        G = self.G\n",
    "        walks = []\n",
    "        \n",
    "        if start_nodes == None:\n",
    "            start_nodes = list(G.nodes())\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        print('Walk iteration:')\n",
    "        for walk_iter in range(num_walks):\n",
    "            print(str(walk_iter+1), '/', str(num_walks))\n",
    "            random.shuffle(start_nodes)\n",
    "            for node in start_nodes:\n",
    "                # print(node)\n",
    "                walks.append(self.node2vec_walk(walk_length=walk_length, start_node=node))\n",
    "\n",
    "        return walks\n",
    "    \n",
    "    def simulate_walks_from_seeds(self, num_walks, walk_length, seeds):\n",
    "        \n",
    "        walks_from_seeds = [self.simulate_walks(num_walks = num_walks, walk_length = walk_length, \n",
    "                                               start_nodes = (seed,) ) for seed in list(seeds)]\n",
    "        \n",
    "        \n",
    "        \n",
    "        pages_visited = {page for paths in walks_from_seeds for path in paths for page in path}\n",
    "\n",
    "        results_rws = dict()\n",
    "        results_rws[\"seeds\"] = list(seeds)\n",
    "        results_rws[\"pages_visited\"] = pages_visited\n",
    "        results_rws[\"paths_taken\"] = walks_from_seeds\n",
    "\n",
    "        \n",
    "        return results_rws\n",
    "    \n",
    "    def convert_tfdf_to_n2v(self, results):\n",
    "        \n",
    "        walks_from_seeds = list(chain.from_iterable(results[\"paths_taken\"]))\n",
    "        \n",
    "        return walks_from_seeds\n",
    "\n",
    "    def get_alias_edge(self, src, dst):\n",
    "        '''\n",
    "        Get the alias edge setup lists for a given edge.\n",
    "        \n",
    "        src = source node\n",
    "        dst = destination node\n",
    "        '''\n",
    "        G = self.G\n",
    "        p = self.p\n",
    "        q = self.q\n",
    "\n",
    "        unnormalized_probs = []\n",
    "        for dst_nbr in sorted(G.neighbors(dst)):\n",
    "            if dst_nbr == src:\n",
    "                # prob of returning to the source node\n",
    "                unnormalized_probs.append(G[dst][dst_nbr]['edgeWeight']/p)\n",
    "            elif G.has_edge(dst_nbr, src):\n",
    "                # prob of going to node connected to both src and dst\n",
    "                unnormalized_probs.append(G[dst][dst_nbr]['edgeWeight'])\n",
    "            else:\n",
    "                # prob of going further from src\n",
    "                unnormalized_probs.append(G[dst][dst_nbr]['edgeWeight']/q)\n",
    "        # normalise the new probabilities\n",
    "        norm_const = sum(unnormalized_probs)\n",
    "        normalized_probs =  [float(u_prob)/norm_const for u_prob in unnormalized_probs]\n",
    "\n",
    "        return alias_setup(normalized_probs)\n",
    "\n",
    "    def preprocess_transition_probs(self):\n",
    "        '''\n",
    "        Preprocessing of transition probabilities for guiding the random walks.\n",
    "        '''\n",
    "        G = self.G\n",
    "        is_directed = self.is_directed\n",
    "\n",
    "        # alias nodes are used to set up the alias edges (using \".get_alias_edge\")\n",
    "        alias_nodes = {}\n",
    "        for node in G.nodes():\n",
    "            # loop through all nodes\n",
    "            unnormalized_probs = [G[node][nbr]['edgeWeight'] for nbr in sorted(G.neighbors(node))]\n",
    "            norm_const = sum(unnormalized_probs)\n",
    "            normalized_probs =  [float(u_prob)/norm_const for u_prob in unnormalized_probs]\n",
    "            alias_nodes[node] = alias_setup(normalized_probs)\n",
    "\n",
    "        alias_edges = {}\n",
    "        triads = {}\n",
    "\n",
    "        # now loop through edges in G\n",
    "        if is_directed:\n",
    "            for edge in G.edges():\n",
    "                alias_edges[edge] = self.get_alias_edge(edge[0], edge[1])\n",
    "        else:\n",
    "            # if undirected, set two values in each iteration (T is symmetric)\n",
    "            for edge in G.edges():\n",
    "                alias_edges[edge] = self.get_alias_edge(edge[0], edge[1])\n",
    "                alias_edges[(edge[1], edge[0])] = self.get_alias_edge(edge[1], edge[0])\n",
    "\n",
    "        self.alias_nodes = alias_nodes\n",
    "        self.alias_edges = alias_edges\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "def alias_setup(probs):\n",
    "    '''\n",
    "    Compute utility lists for non-uniform sampling from discrete distributions.\n",
    "    Refer to https://hips.seas.harvard.edu/blog/2013/03/03/the-alias-method-efficient-sampling-with-many-discrete-outcomes/\n",
    "    for details\n",
    "    \n",
    "    Mine: Efficient sampling from a multivariate discrete distribution, with a potentially\n",
    "    unknown normalisign constant\n",
    "    \n",
    "    Source: https://lips.cs.princeton.edu/the-alias-method-efficient-sampling-with-many-discrete-outcomes/\n",
    "    '''\n",
    "    K = len(probs)\n",
    "    q = np.zeros(K)\n",
    "    J = np.zeros(K, dtype=np.int)\n",
    "\n",
    "    smaller = []\n",
    "    larger = []\n",
    "    for kk, prob in enumerate(probs):\n",
    "        q[kk] = K*prob\n",
    "        if q[kk] < 1.0:\n",
    "            smaller.append(kk)\n",
    "        else:\n",
    "            larger.append(kk)\n",
    "\n",
    "    while len(smaller) > 0 and len(larger) > 0:\n",
    "        small = smaller.pop()\n",
    "        large = larger.pop()\n",
    "\n",
    "        J[small] = large\n",
    "        q[large] = q[large] + q[small] - 1.0\n",
    "        if q[large] < 1.0:\n",
    "            smaller.append(large)\n",
    "        else:\n",
    "            larger.append(large)\n",
    "\n",
    "    return J, q\n",
    "\n",
    "def alias_draw(J, q):\n",
    "    '''\n",
    "    Draw sample from a non-uniform discrete distribution using alias sampling.\n",
    "    '''\n",
    "    K = len(J)\n",
    "\n",
    "    kk = int(np.floor(np.random.rand()*K))\n",
    "    if np.random.rand() < q[kk]:\n",
    "        return kk\n",
    "    else:\n",
    "        return J[kk]\n",
    "\n",
    "def main(args):\n",
    "    '''\n",
    "    Pipeline for representational learning for all nodes in a graph.\n",
    "    '''\n",
    "    nx_G = read_graph()\n",
    "    G = node2vec.Graph(nx_G, args.directed, args.p, args.q)\n",
    "    G.preprocess_transition_probs()\n",
    "    walks = G.simulate_walks(args.num_walks, args.walk_length)\n",
    "    learn_embeddings(walks)\n",
    "\n",
    "def read_graph():\n",
    "    '''\n",
    "    Reads the input network in networkx.\n",
    "    '''\n",
    "    if args.weighted:\n",
    "        G = nx.read_edgelist(args.input, nodetype=int, data=(('weight',float),), create_using=nx.DiGraph())\n",
    "    else:\n",
    "        G = nx.read_edgelist(args.input, nodetype=int, create_using=nx.DiGraph())\n",
    "        for edge in G.edges():\n",
    "            G[edge[0]][edge[1]]['weight'] = 1\n",
    "\n",
    "    if not args.directed:\n",
    "        G = G.to_undirected()\n",
    "\n",
    "    return G\n",
    "\n",
    "def learn_embeddings(walks):\n",
    "    '''\n",
    "    Learn embeddings by optimizing the Skipgram objective using SGD.\n",
    "    '''\n",
    "    walks = [map(str, walk) for walk in walks]\n",
    "    model = Word2Vec(walks, size=args.dimensions, window=args.window_size, min_count=0, sg=1, workers=args.workers, iter=args.iter)\n",
    "    model.save_word2vec_format(args.output)\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def set_seed(seed_number):\n",
    "    '''Set random seeds for replicability'''\n",
    "    random.seed(seed_number)\n",
    "    np.random.seed(seed_number)\n",
    "\n",
    "    return ()\n",
    "\n",
    "def calculate_l2_distance(embeddings, reference_vectors_embeddings):\n",
    "    '''\n",
    "    For given embeddings, and embeddings of reterence vectors, calulate mean and median distances\n",
    "    from each embedding vector to reference vector embeddings.\n",
    "    '''\n",
    "    mean_distances = [np.linalg.norm(x - reference_vectors_embeddings, axis = 1).mean() for x in embeddings]\n",
    "    median_distances = [np.median( np.linalg.norm(x - reference_vectors_embeddings, axis = 1) ) for x in embeddings]\n",
    "\n",
    "    return(mean_distances, median_distances)\n",
    "\n",
    "def calculate_n2v_distance(G, model, seeds, stats = np.max):\n",
    "    '''\n",
    "    For any given node in G, calculate similairty to each seed page and return the given summary \n",
    "    statistics of these scores.\n",
    "    Returns scores for all nodes in G.\n",
    "    '''\n",
    "\n",
    "    rankings = list()\n",
    "    page_paths = list()\n",
    "\n",
    "    for node in G.nodes:\n",
    "\n",
    "        rankings_node = list()\n",
    "\n",
    "        for seed in seeds:\n",
    "            rankings_node.append( model.wv.similarity(node, seed) )\n",
    "\n",
    "        rankings.append( stats( rankings_node ) )\n",
    "        page_paths.append( node )\n",
    "\n",
    "    return( rankings, page_paths )\n",
    "\n",
    "\n",
    "\n",
    "def dump_pickle_file(file, file_name, dir_path=\"./outputs/pickle\"):\n",
    "    \n",
    "    # check directory exists (if not, create it)\n",
    "    isdir = os.path.isdir(dir_path)\n",
    "\n",
    "    if isdir == False:\n",
    "        os.mkdir(dir_path)\n",
    "        print(\"Directory created.\")\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    file_path = os.path.join(dir_path, file_name)\n",
    "\n",
    "    f = open(file_path, 'wb')\n",
    "    pickle.dump(file, f)\n",
    "\n",
    "    return ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CiSmRhrd1YPd"
   },
   "source": [
    "## Sampling + misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wAckLeUi_fv9"
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess_graph(graph_name, dir, p = 1, q = 1, pages_to_remove = None):\n",
    "  '''\n",
    "  Load graph object and rerpocess transition probabilities in N2V way.\n",
    "\n",
    "  Returns N2V graph object.\n",
    "  '''\n",
    "  G = load_pickle_file(graph_name, dir_path=dir)\n",
    "\n",
    "  if pages_to_remove != None:\n",
    "    G.remove_nodes_from(pages_to_remove)\n",
    "\n",
    "  G_n2v = Graph(G, is_directed = True, p = p, q = q)\n",
    "  G_n2v.preprocess_transition_probs()\n",
    "\n",
    "\n",
    "\n",
    "  return( G_n2v ) \n",
    "\n",
    "\n",
    "def draw_neighbors(graph_nodes, G_primary, G_secondary, num_neighbors, walk_length, num_walks_primary, min_neighbors_tolerance, max_num_secondary_rws, dict_node_index):\n",
    "  '''\n",
    "  For a list of graph nodes, generate positive examples for negative sampling.\n",
    "\n",
    "  Goal:\n",
    "  - Generate at least \"min_neighbors_tolerance\" and at most \"num_neighbors\" positive samples for each node.\n",
    "  - This is done as follows:\n",
    "\n",
    "    1. Draw \"num_walks_primary\" number of random walks from graph \"G_primary\". Positive samples are \"num_neighbors\" unique nodes visited in these walks (selected randomly\n",
    "    if more nodes were visited than the required number of positive examples).\n",
    "    If number of generated samples for some node is less than \"num_neighbors\", for this node carry out the next step:\n",
    "    2. While number of positive samples for a node is not equal to \"num_neighbors\":\n",
    "      - Run 40 random walks from this node using graph \"G_secondary\". Walk length increases by 2\n",
    "      - Do this at most \"max_num_secondary_rws\" times.\n",
    "    3. Discard all nodes for which we have less examples than specified by \"min_neighbors_tolerance\".\n",
    "\n",
    "\n",
    "  Inputs:\n",
    "  - graph_nodes: list of graph nodes\n",
    "  - G_primary: graph object used to draw primary random walks (most of positive samples come from these walks)\n",
    "  - G_secondary: graph object for secondary RWs\n",
    "  - num_neighbors: number of positive samples required per node\n",
    "  - walk_length: length of primary RWs\n",
    "  - num_walks_primary: number of primary RWs\n",
    "  - min_neighbors_tolerance: minimum number of positive smaples tolerated\n",
    "  - max_num_secondary_rws: maximum number of secondary RWs (higher number means we have more positive samples but takes longer time to run)\n",
    "  - dict_node_index: mapping of node names to indices\n",
    "\n",
    "  Returns:\n",
    "  - ngh_dict: dictionary of positive samples. Key = starting node, item = list of visited nodes\n",
    "  - failed_nodes: list of nodes for which too few positive samples were generated (these are discarded from later analysis).\n",
    "\n",
    "  '''\n",
    "\n",
    "  rws_primary = G_primary.simulate_walks_from_seeds(num_walks_primary, walk_length, seeds = graph_nodes)\n",
    "\n",
    "  failed_nodes = []\n",
    "  failed_nodes_index = []\n",
    "  ngh_dict = dict() # list()\n",
    "\n",
    "  for node in graph_nodes:\n",
    "\n",
    "    node_index = dict_node_index[node]\n",
    "\n",
    "    pages_visited_node = [page for path in rws_primary[\"paths_taken\"][node_index] for page in path[1:]]\n",
    "    pages_visited_node = np.unique(pages_visited_node)\n",
    "\n",
    "\n",
    "    # try to get a desired number of distinct neighbors\n",
    "    i = 0 # count number of extra iterations\n",
    "    while len(pages_visited_node) < num_neighbors:\n",
    "      # print( node, \": \", len(pages_visited_node) )\n",
    "      rws_secondary = G_secondary.simulate_walks_from_seeds(num_walks = 40, walk_length = walk_length, seeds = [node])\n",
    "      rws_secondary[\"pages_visited\"].remove(node)\n",
    "      rw_nodes_index = np.array([page for page in rws_secondary[\"pages_visited\"]])\n",
    "      pages_visited_node = np.unique( np.concatenate([pages_visited_node, rw_nodes_index]) )\n",
    "\n",
    "      i += 1\n",
    "\n",
    "      if i == int(max_num_secondary_rws + 1):\n",
    "        break\n",
    "\n",
    "    # draw neighbors at random if possible\n",
    "    # those that don't meet threshold are accumulated in \"failed_nodes\" list\n",
    "    if len(list(pages_visited_node)) >= min_neighbors_tolerance:\n",
    "      print( len(list(pages_visited_node)) )\n",
    "      try:\n",
    "        ngh_dict[node] = random.sample(list(pages_visited_node), num_neighbors)\n",
    "\n",
    "      except:\n",
    "        ngh_dict[node] = list(pages_visited_node) \n",
    "\n",
    "\n",
    "      if len(ngh_dict[node]) == 0:\n",
    "        print(\"Stoppping due to an empty array\")\n",
    "        break\n",
    "\n",
    "    else:\n",
    "      failed_nodes.append(node)\n",
    "      failed_nodes_index.append(node_index)\n",
    "      print(\"Failed for node\", node)\n",
    "      print(len(failed_nodes))\n",
    "    \n",
    "  return(ngh_dict, failed_nodes)\n",
    "\n",
    "\n",
    "def get_neighbor_matrix(G, ngh_dict, failed_nodes, num_neighbors):\n",
    "  '''\n",
    "  Construct a neighborhood matrix. Each row represents a neighborhood of a single node. \n",
    "  In columns are indices of the node's neighbors.\n",
    "\n",
    "  Inputs:\n",
    "  - G: graph\n",
    "  - ngh_dict: dictionary (output of \"draw_neighbors\"). Keys = start nodes, items = list of neighbors\n",
    "  - failed_nodes: nodes to disregard (due to insufficient number of neighbors)\n",
    "  - num_neighbors: desired number of neighbors\n",
    "\n",
    "  Returns:\n",
    "  - Graph (with failed_nodes removed)\n",
    "  - neighb_matrix (matrix of neighbors). Row = starting node, Columns = neighbors' indices\n",
    "  '''\n",
    "\n",
    "  G.remove_nodes_from(failed_nodes)\n",
    "  \n",
    "\n",
    "  graph_nodes = list(G.nodes())\n",
    "\n",
    "  for node in ngh_dict.keys():\n",
    "    if node not in graph_nodes:\n",
    "      del ngh_dict[node]\n",
    "    else:\n",
    "      pass\n",
    "\n",
    "  # remove failed nodes from paths starting in other nodes\n",
    "  for node in ngh_dict.keys():\n",
    "    ngh_dict[node] = [x for x in ngh_dict[node] if x in graph_nodes] \n",
    "\n",
    "  dict_node_index = dict()\n",
    "\n",
    "  # mapping of nodes to integers/indices\n",
    "  for node in list(G.nodes()):\n",
    "    dict_node_index[node] = graph_nodes.index(node)\n",
    "\n",
    "  # matrix of neigh indices\n",
    "  neigh_matrix = np.full(( len(graph_nodes ), num_neighbors), np.nan)\n",
    "\n",
    "  for index, node in enumerate( G.nodes() ):\n",
    "    # print(node)\n",
    "    neigh_inices_arr = np.array([dict_node_index[x] for x in ngh_dict[node]])\n",
    "    try:\n",
    "      neigh_matrix[index, :] = neigh_inices_arr\n",
    "    except:\n",
    "      neigh_matrix[index, :] = np.concatenate([ neigh_inices_arr, np.full(num_neighbors - len(ngh_dict[node]), np.nan)])\n",
    "\n",
    "  return( G, neigh_matrix )\n",
    "\n",
    "\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "import pandas as pd\n",
    "\n",
    "def get_pyg_graph(G, train_set_fraction = 0.9):\n",
    "  '''\n",
    "  Format graph so that it's suitable for torch_geometric module.\n",
    "\n",
    "  Inputs:\n",
    "  - G: networkx graph\n",
    "  - train_set_fraction: fraction of train samples\n",
    "\n",
    "  Outputs:\n",
    "  - torch geometric graph. Attributes are:\n",
    "      - train_mask (np.array): 0 for val nodes, 1 for train nodes\n",
    "      - val_mask (np.array)\n",
    "      - node_feature: matrix of node features (torch.float32 format)\n",
    "  '''\n",
    "  # convert to torch_geometric graph object\n",
    "  pyg_graph = from_networkx(G)\n",
    "\n",
    "\n",
    "  data_set_size = len( list(G.nodes()) )\n",
    "\n",
    "\n",
    "  train_set_size = np.floor(train_set_fraction * data_set_size)\n",
    "  train_mask = np.zeros( int(data_set_size) )\n",
    "\n",
    "  train_set =   random.sample( list(np.arange( data_set_size )), int(train_set_size) )\n",
    "  train_mask[train_set] = 1\n",
    "  val_mask = 1 - train_mask\n",
    "\n",
    "  pyg_graph.train_mask = np.array(train_mask) == 1\n",
    "  pyg_graph.val_mask = np.array(val_mask) == 1\n",
    "\n",
    "  pyg_graph.node_feature = pyg_graph.node_feature.to(torch.float32)\n",
    "\n",
    "  return( pyg_graph )\n",
    "\n",
    "\n",
    "\n",
    "def get_negative_examples(G, neigh_matrix):\n",
    "  '''\n",
    "  For each node of graph G, draw negative samples.\n",
    "\n",
    "  For any given node,s negative samples are randomly selected graph nodes, with positive samples exlcuded.\n",
    "\n",
    "  Inputs:\n",
    "  - G: graph\n",
    "  - neigh_matrix: matirx of positive sample indices\n",
    "\n",
    "  Returns:\n",
    "  - matrix of negative samples\n",
    "  '''\n",
    "\n",
    "  indices_nodes = np.arange(len(G.nodes()))\n",
    "\n",
    "\n",
    "  neg_indices = list()\n",
    "\n",
    "  for row in np.arange(neigh_matrix.shape[0]):\n",
    "    print(row)\n",
    "    pi_i = neigh_matrix[row, :]\n",
    "    pi_i = pi_i[~np.isnan(pi_i)]\n",
    "    remaining_indices_i = np.delete( indices_nodes, pi_i.astype(int))\n",
    "    neg_indices_i = random.choices( remaining_indices_i, k = 30 ) \n",
    "    neg_indices.append(neg_indices_i)\n",
    "\n",
    "  negative_array = np.array(neg_indices)\n",
    "\n",
    "  return(negative_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RWeuBG71C_S-"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_pyg_object(graph_wfeatures_name, graph_orig_name, dir, p_primary, q_primary, p_secondary, q_secondary,\n",
    "                   num_neighbors, walk_length, num_walks_primary, min_neighbors_tolerance, max_num_secondary_rws, pages_to_remove = None):\n",
    "\n",
    "  '''\n",
    "  Construct a torch_geometric-compatible graph object, along with matrices of positive and negative samples.\n",
    "\n",
    "  Inputs:\n",
    "  - graph_wfeatures_name: graph containing node features\n",
    "  - graph_orig_name: original graph object\n",
    "  - dir: path to the graph objects\n",
    "  - p_primary, q_primary, p_secondary, q_secondary: parameters of primary and secondary biased random walks\n",
    "  - num_neighbors: number of desired neighbors\n",
    "  - walk_length: length of primary RWs\n",
    "  - num_walks_primary: number of primary RWs\n",
    "  - min_neighbors_tolerance: minimum number of positive smaples tolerated\n",
    "  - max_num_secondary_rws: maximum number of secondary RWs (higher number means we have more positive samples but takes longer time to run)\n",
    "  - pages_to_remove: list of any extra nodes to remove (e.g. if NER analysis shows some pages no longer exist)\n",
    "\n",
    "  Returns:\n",
    "  - pyg_graph: torch_geometric graph object\n",
    "  - neigh_matrix: matrix of positive samples\n",
    "  - negative_array: matrix of negative samples\n",
    "  - node_list\n",
    "  '''\n",
    "  G3 = load_pickle_file(graph_wfeatures_name, dir_path=dir)\n",
    "\n",
    "  if pages_to_remove != None:\n",
    "    G3.remove_nodes_from(pages_to_remove)\n",
    "\n",
    "\n",
    "  graph_nodes = list(G3.nodes())\n",
    "\n",
    "  dict_node_index = dict()\n",
    "\n",
    "  # mapping of nodes to integers/indices\n",
    "  for node in graph_nodes:\n",
    "    dict_node_index[node] = graph_nodes.index(node)\n",
    "\n",
    "  G_n2v_weighted = load_and_preprocess_graph(graph_orig_name, dir = dir, p = p_primary, q = q_primary, pages_to_remove = pages_to_remove)\n",
    "  G_n2v_standard = load_and_preprocess_graph(graph_orig_name, dir = dir, p = p_secondary, q = q_secondary, pages_to_remove = pages_to_remove)\n",
    "\n",
    "  ngh_list, failed_nodes = draw_neighbors(graph_nodes, G_n2v_weighted, G_n2v_standard, num_neighbors, walk_length, num_walks_primary, min_neighbors_tolerance, max_num_secondary_rws, dict_node_index)\n",
    "\n",
    "  G, neigh_matrix = get_neighbor_matrix(G3, ngh_list, failed_nodes, num_neighbors)\n",
    "\n",
    "  node_list = list(G.nodes())\n",
    "\n",
    "  negative_array = get_negative_examples(G, neigh_matrix)\n",
    "\n",
    "  pyg_graph = get_pyg_graph(G, train_set_fraction = 0.9)\n",
    "  pyg_graph.node_list = node_list\n",
    "\n",
    "  return( pyg_graph, neigh_matrix, negative_array, node_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uW9D6yFNb2DT"
   },
   "outputs": [],
   "source": [
    "def scores(model, pyg_graph, neigh_matrix, negative_array, seed_pages_used, edge_weight = False):\n",
    "\n",
    "    if edge_weight == True:\n",
    "      h = model(pyg_graph.node_feature, pyg_graph.edge_index, pyg_graph.weight)\n",
    "    else:\n",
    "      h = model(pyg_graph.node_feature, pyg_graph.edge_index)\n",
    "\n",
    "    out_numpy = h.detach().numpy()\n",
    "\n",
    "    val_loss = criterion( h, neigh_matrix, negative_array, pyg_graph.val_mask)\n",
    "\n",
    "    scores_df = ranking_df_seeds(out_numpy, seed_pages_used, pyg_graph.node_list)\n",
    "\n",
    "\n",
    "\n",
    "    scores_df_rankings = scores_df.sort_values(by = \"max\", ascending = False).reset_index(drop = False)\n",
    "    scores_df_rankings.rename(columns = {\"index\": \"page\"}, inplace = True)\n",
    "\n",
    "    score = calc_median_difference_n2v(scores_df_rankings, labelled_data_1, \n",
    "                            standardise = True, page_path = \"page\")\n",
    "    \n",
    "    return( val_loss, scores_df_rankings, score )\n",
    "\n",
    "\n",
    "def get_model_path_name(hidden_channels, encoding_dim, dir_path):\n",
    "    model_name = \"\"\n",
    "    for i in hidden_channels: \n",
    "      model_name = model_name + str(i) + \"_\"\n",
    "    model_name = model_name + \"emb\" + str(encoding_dim) + \".pt\"\n",
    "    path_save = dir_path + model_name\n",
    "\n",
    "    return(model_name, path_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FaSFwjgA1aby"
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s6_Lj7pzQqiz"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MSELoss3(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(MSELoss3, self).__init__()\n",
    "\n",
    "    def forward(self, embeddings, neighbors_array, negative_array, mask_array):\n",
    "        # get neighbors for input node \n",
    "        # print(embeddings.grad_fn)\n",
    "        # c = embeddings\n",
    "        # c = Variable(embeddings, requires_grad=True)\n",
    "\n",
    "        loss = 0\n",
    "\n",
    "        for i in np.arange(embeddings.shape[0]):\n",
    "          \n",
    "          if mask_array[i] == 1:\n",
    "            neighbors = neighbors_array[i]\n",
    "            neighbors = neighbors[~np.isnan(neighbors_array[i])]\n",
    "\n",
    "            pos_products = torch.matmul(embeddings[neighbors], embeddings[i])\n",
    "            neg_products = torch.matmul(embeddings[negative_array[i]], embeddings[i])\n",
    "\n",
    "            EPS = 1e-15\n",
    "            pos_loss = -torch.log(torch.sigmoid(pos_products) + EPS).mean()\n",
    "            neg_loss = -torch.log(1 - torch.sigmoid(neg_products) + EPS).mean()\n",
    "            \n",
    "            if np.isnan((pos_loss + neg_loss).item()): \n",
    "              pass\n",
    "            else:\n",
    "              # if mask_array[i]:\n",
    "              loss += pos_loss + neg_loss\n",
    "              # else:\n",
    "                # pass\n",
    "          else:\n",
    "            pass\n",
    "          # print(loss)\n",
    "          # loss.append(loss_i)\n",
    "          # Positive loss.\n",
    "\n",
    "        return loss / np.sum(mask_array)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYNc-kvm1cOB"
   },
   "source": [
    "## Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fve6NB92bXri"
   },
   "outputs": [],
   "source": [
    "\n",
    "def ranking_df_seeds(out_numpy, seed_list, node_list):\n",
    "\n",
    "  scores_df = pd.DataFrame(index =node_list)\n",
    "\n",
    "\n",
    "  for node in seed_list:\n",
    "    node_index = np.where(np.array(node_list) == node)\n",
    "\n",
    "    node_embedding = out_numpy[node_index, :].squeeze()\n",
    "    node_embedding_product = calc_cosine_similarity_matrix(out_numpy, node_embedding) # out_numpy @ node_embedding\n",
    "\n",
    "    scores_df[node] = 0\n",
    "    # print(scores_df.loc[:, node])\n",
    "    print(node_embedding_product)\n",
    "    scores_df.loc[:, node] = node_embedding_product\n",
    "\n",
    "  scores_df[\"max\"]= scores_df[seed_list].max(axis = 1)\n",
    "  scores_df[\"median\"] = scores_df[seed_list].median(axis = 1)\n",
    "  scores_df[\"mean\"] = scores_df[seed_list].mean(axis = 1)\n",
    "  scores_df[\"min\"]= scores_df[seed_list].min(axis = 1)\n",
    "\n",
    "  return( scores_df )\n",
    "\n",
    "\n",
    "\n",
    "def calc_cosine_similarity_matrix(X, y):\n",
    "    '''\n",
    "    X is a matatrix of embeddings, with nodes in rows (i.e number of rows = number of nodes, \n",
    "    number of columns = number of latent dimensions).\n",
    "    '''\n",
    "    cosine_similarity = np.dot(X, y)/(np.linalg.norm(X, axis = 1)* np.linalg.norm(y))\n",
    "    \n",
    "    return( cosine_similarity )\n",
    "\n",
    "def get_seed_page_index(G, seed_page):\n",
    "    seed_node_index = np.where(np.array(G.nodes) == seed_page)[0]\n",
    "    seed_node_index = seed_node_index.astype(int)[0]\n",
    "\n",
    "    # print( \"Seed node:\", list(G2.nodes)[seed_node_index] )\n",
    "    \n",
    "    return(seed_node_index)\n",
    "\n",
    "def get_rankings_dict(G, embeddings, seeds, metric = \"cosine\"):\n",
    "    \n",
    "    rankings = dict()\n",
    "\n",
    "    rankings[\"pages\"] = list(G.nodes)\n",
    "\n",
    "    for seed_page in seeds:\n",
    "\n",
    "        seed_node_index = get_seed_page_index(G, seed_page)\n",
    "        seed_node_embedding = embeddings[seed_node_index]\n",
    "        \n",
    "        if metric == \"cosine\":\n",
    "            node_similarities = calc_cosine_similarity_matrix(embeddings, seed_node_embedding)\n",
    "        else:\n",
    "            node_similarities = np.linalg.norm(embeddings - seed_node_embedding)\n",
    "\n",
    "        rankings[seed_page] = node_similarities\n",
    "\n",
    "\n",
    "    return(rankings)\n",
    "\n",
    "\n",
    "def get_rankings_df(G, embeddings, seeds, metric = \"cosine\"):\n",
    "\n",
    "    emb_dict = get_rankings_dict(G, embeddings, seeds, metric= metric)\n",
    "        \n",
    "    emb_df = pd.DataFrame.from_dict(emb_dict)\n",
    "    \n",
    "    emb_df.set_index(\"pages\", inplace = True)\n",
    "    seed_cols = emb_df.columns\n",
    "    \n",
    "    emb_df[\"max\"]= emb_df[seed_cols].max(axis = 1)\n",
    "    emb_df[\"median\"] = emb_df[seed_cols].median(axis = 1)\n",
    "    emb_df[\"mean\"] = emb_df[seed_cols].mean(axis = 1)\n",
    "    emb_df[\"min\"]= emb_df[seed_cols].min(axis = 1)\n",
    "    \n",
    "    return(emb_df)\n",
    "    \n",
    "labelled_data_1 = pd.read_csv('/content/gdrive/MyDrive/GDS/pickles/pages_ranked_with_data_labelled.csv')\n",
    "labelled_data_1 = labelled_data_1.loc[:,[\"page path\", \"label\"]]\n",
    "\n",
    "\n",
    "# THIS IS THE SAME FUNCTION AND IN N2V\n",
    "def calc_median_difference_n2v(df, labelled_data, standardise = True, page_path = \"pagePath\"):\n",
    "    '''df needs to be a result of calling rw.page_freq_path_freq_ranking()\n",
    "    \n",
    "    df needs to be ranked from top page to the worst page (i.e. index represents ranking).'''\n",
    "    df.reset_index(inplace = True, drop = True)\n",
    "    \n",
    "    df_labels = df.merge(labelled_data_1, left_on = page_path, right_on = \"page path\")\n",
    "    df_labels.reset_index(inplace = True, drop = False)\n",
    "    df_labels.rename(columns = {\"index\": \"rank\"}, inplace = True)\n",
    "\n",
    "    med_ranking_label1 = df_labels[df_labels[\"label\"] == 1][\"rank\"].median()\n",
    "    med_ranking_label0 = df_labels[df_labels[\"label\"] == 0][\"rank\"].median()\n",
    "    \n",
    "    if standardise == True:\n",
    "        score = (med_ranking_label0 - med_ranking_label1) / ( df_labels[df_labels[\"label\"] == 1][\"rank\"].std() +\n",
    "                                                            df_labels[df_labels[\"label\"] == 0][\"rank\"].std())\n",
    "    else:\n",
    "        score = med_ranking_label0 - med_ranking_label1\n",
    "    \n",
    "    return( score )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w65C9y0EZ94B"
   },
   "outputs": [],
   "source": [
    "class GCN_edgeweight(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, encoding_dim):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        # self.conv1 = GCNConv(dataset.num_features, hidden_channels)\n",
    "        self.conv1 = GCNConv(pyg_graph.node_feature.shape[1], hidden_channels[0])\n",
    "        self.conv2 = GCNConv(hidden_channels[0], hidden_channels[1])\n",
    "        # self.classifier = Linear(2, dataset.num_classes)\n",
    "        self.linear = Linear(hidden_channels[1], encoding_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        x = self.conv1(x, edge_index, edge_weight )\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        x = x.relu()\n",
    "        h = self.linear(x)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V2BdQx3teQEs"
   },
   "outputs": [],
   "source": [
    "class GCN_3layers_edgeweight(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, encoding_dim):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        # self.conv1 = GCNConv(dataset.num_features, hidden_channels)\n",
    "        self.conv1 = GCNConv(pyg_graph.node_feature.shape[1], hidden_channels[0])\n",
    "        self.conv2 = GCNConv(hidden_channels[0], hidden_channels[1])\n",
    "        self.conv3 = GCNConv(hidden_channels[1], hidden_channels[2])\n",
    "        # self.classifier = Linear(2, dataset.num_classes)\n",
    "        self.linear = Linear(hidden_channels[2], encoding_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.conv3(x, edge_index, edge_weight)\n",
    "        # h = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qWPFIeugeQEs"
   },
   "outputs": [],
   "source": [
    "class GCN_nolinear(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, encoding_dim):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        # self.conv1 = GCNConv(dataset.num_features, hidden_channels)\n",
    "        self.conv1 = GCNConv(pyg_graph.node_feature.shape[1], hidden_channels[0])\n",
    "        self.conv2 = GCNConv(hidden_channels[0], hidden_channels[1])\n",
    "        # self.classifier = Linear(2, dataset.num_classes)\n",
    "        self.linear = Linear(hidden_channels[1], encoding_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        # x = F.dropout(x, p=0.0, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        # x = x.relu()\n",
    "        # h = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_wvIEHDPZhUU"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, data, neigh_matrix, negative_array):\n",
    "    optimizer.zero_grad()  # Clear gradients.\n",
    "    h = model(data.node_feature, data.edge_index, edge_weight = data.weight)\n",
    "\n",
    "    loss = criterion( h, neigh_matrix, negative_array, data.train_mask)\n",
    "    val_loss = criterion( h, neigh_matrix, negative_array, data.val_mask)\n",
    "\n",
    "    print(loss)\n",
    "    loss.backward()  # Derive gradients.\n",
    "    optimizer.step()  # Update parameters based on gradients.\n",
    "    return loss, h, val_loss\n",
    "\n",
    "def train(model, data, neigh_matrix, negative_array, num_epochs, dir_path, patience = 20, model_name = None):\n",
    "\n",
    "\n",
    "    best_val_loss = np.infty\n",
    "\n",
    "    loss_hist_train = list()\n",
    "    accuracy_hist_train = list()\n",
    "    loss_hist_val = list()\n",
    "    accuracy_hist_valid = list()\n",
    "\n",
    "\n",
    "    cnt_wait = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        loss, h, val_loss = train_epoch(model, data, neigh_matrix, negative_array)\n",
    "        print(val_loss)\n",
    "        if val_loss < best_val_loss:\n",
    "          best_val_loss = val_loss\n",
    "          print(\"Saving at epoch\", epoch)\n",
    "\n",
    "          if model_name == None:\n",
    "            model_name = \"/model\"\n",
    "            for i in hidden_channels: \n",
    "              model_name = model_name + str(i) + \"_\"\n",
    "            model_name = model_name + \"emb\" + str(h.shape[1]) + \".pt\"\n",
    "          else:\n",
    "            pass\n",
    "\n",
    "          path_save = dir_path + model_name\n",
    "          torch.save(model.state_dict(), path_save)\n",
    "\n",
    "          cnt_wait = 0\n",
    "\n",
    "        else:\n",
    "            cnt_wait += 1\n",
    "\n",
    "        if cnt_wait == patience:\n",
    "            print('Early stopping!')\n",
    "            break\n",
    "\n",
    "\n",
    "        h = model(data.node_feature, data.edge_index, data.weight)\n",
    "\n",
    "        loss_hist_train.append(loss.item())\n",
    "        loss_hist_val.append(val_loss.item())\n",
    "\n",
    "         \n",
    "        print(\"Epoch:\", epoch, \"loss: \", loss.item()) \n",
    "      \n",
    "    return(loss_hist_train,  loss_hist_val, h)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQcCkeM9lXbP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sWGNhrglAYB"
   },
   "source": [
    "# Many random walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "d9PdhRN-lB-W",
    "outputId": "3052f3e4-fdb6-4af1-9807-5ccdf2369135"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wl_5_2_2_2_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:168: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "4068\n",
      "4069\n",
      "4070\n",
      "4071\n",
      "4072\n",
      "4073\n",
      "4074\n",
      "4075\n",
      "4076\n",
      "4077\n",
      "4078\n",
      "4079\n",
      "4080\n",
      "4081\n",
      "4082\n",
      "4083\n",
      "4084\n",
      "4085\n",
      "4086\n",
      "4087\n",
      "4088\n",
      "4089\n",
      "4090\n",
      "4091\n",
      "4092\n",
      "4093\n",
      "4094\n",
      "4095\n",
      "4096\n",
      "4097\n",
      "4098\n",
      "4099\n",
      "4100\n",
      "4101\n",
      "4102\n",
      "4103\n",
      "4104\n",
      "4105\n",
      "4106\n",
      "4107\n",
      "4108\n",
      "4109\n",
      "4110\n",
      "4111\n",
      "4112\n",
      "4113\n",
      "4114\n",
      "4115\n",
      "4116\n",
      "4117\n",
      "4118\n",
      "4119\n",
      "4120\n",
      "4121\n",
      "4122\n",
      "4123\n",
      "4124\n",
      "4125\n",
      "4126\n",
      "4127\n",
      "4128\n",
      "4129\n",
      "4130\n",
      "4131\n",
      "4132\n",
      "4133\n",
      "4134\n",
      "4135\n",
      "4136\n",
      "4137\n",
      "4138\n",
      "4139\n",
      "4140\n",
      "4141\n",
      "4142\n",
      "4143\n",
      "4144\n",
      "4145\n",
      "4146\n",
      "4147\n",
      "4148\n",
      "4149\n",
      "4150\n",
      "4151\n",
      "4152\n",
      "4153\n",
      "4154\n",
      "4155\n",
      "4156\n",
      "4157\n",
      "4158\n",
      "4159\n",
      "4160\n",
      "4161\n",
      "4162\n",
      "4163\n",
      "4164\n",
      "4165\n",
      "4166\n",
      "4167\n",
      "4168\n",
      "4169\n",
      "4170\n",
      "4171\n",
      "4172\n",
      "4173\n",
      "4174\n",
      "4175\n",
      "4176\n",
      "4177\n",
      "4178\n",
      "4179\n",
      "4180\n",
      "4181\n",
      "4182\n",
      "4183\n",
      "4184\n",
      "4185\n",
      "4186\n",
      "4187\n",
      "4188\n",
      "4189\n",
      "4190\n",
      "4191\n",
      "4192\n",
      "4193\n",
      "4194\n",
      "4195\n",
      "4196\n",
      "4197\n",
      "4198\n",
      "4199\n",
      "4200\n",
      "4201\n",
      "4202\n",
      "4203\n",
      "4204\n",
      "4205\n",
      "4206\n",
      "4207\n",
      "4208\n",
      "4209\n",
      "4210\n",
      "4211\n",
      "4212\n",
      "4213\n",
      "4214\n",
      "4215\n",
      "4216\n",
      "4217\n",
      "4218\n",
      "4219\n",
      "4220\n",
      "4221\n",
      "4222\n",
      "4223\n",
      "4224\n",
      "4225\n",
      "4226\n",
      "4227\n",
      "4228\n",
      "4229\n",
      "4230\n",
      "4231\n",
      "4232\n",
      "4233\n",
      "4234\n",
      "4235\n",
      "4236\n",
      "4237\n",
      "4238\n",
      "4239\n",
      "4240\n",
      "4241\n",
      "4242\n",
      "4243\n",
      "4244\n",
      "4245\n",
      "4246\n",
      "4247\n",
      "4248\n",
      "4249\n",
      "4250\n",
      "4251\n",
      "4252\n",
      "4253\n",
      "4254\n",
      "4255\n",
      "4256\n",
      "4257\n",
      "4258\n",
      "4259\n",
      "4260\n",
      "4261\n",
      "4262\n",
      "4263\n",
      "4264\n",
      "4265\n",
      "4266\n",
      "4267\n",
      "4268\n",
      "4269\n",
      "4270\n",
      "4271\n",
      "4272\n",
      "4273\n",
      "4274\n",
      "4275\n",
      "4276\n",
      "4277\n",
      "4278\n",
      "4279\n",
      "4280\n",
      "4281\n",
      "4282\n",
      "4283\n",
      "4284\n",
      "4285\n",
      "4286\n",
      "4287\n",
      "4288\n",
      "4289\n",
      "4290\n",
      "4291\n",
      "4292\n",
      "4293\n",
      "4294\n",
      "4295\n",
      "4296\n",
      "4297\n",
      "4298\n",
      "4299\n",
      "4300\n",
      "4301\n",
      "4302\n",
      "4303\n",
      "4304\n",
      "4305\n",
      "4306\n",
      "4307\n",
      "4308\n",
      "4309\n",
      "4310\n",
      "4311\n",
      "4312\n",
      "4313\n",
      "4314\n",
      "4315\n",
      "4316\n",
      "4317\n",
      "4318\n",
      "4319\n",
      "4320\n",
      "4321\n",
      "4322\n",
      "4323\n",
      "4324\n",
      "4325\n",
      "4326\n",
      "4327\n",
      "4328\n",
      "4329\n",
      "4330\n",
      "4331\n",
      "4332\n",
      "4333\n",
      "4334\n",
      "4335\n",
      "4336\n",
      "4337\n",
      "4338\n",
      "4339\n",
      "4340\n",
      "4341\n",
      "4342\n",
      "4343\n",
      "4344\n",
      "4345\n",
      "4346\n",
      "4347\n",
      "4348\n",
      "4349\n",
      "4350\n",
      "4351\n",
      "4352\n",
      "4353\n",
      "4354\n",
      "4355\n",
      "4356\n",
      "4357\n",
      "4358\n",
      "4359\n",
      "4360\n",
      "4361\n",
      "4362\n",
      "4363\n",
      "4364\n",
      "4365\n",
      "4366\n",
      "4367\n",
      "4368\n",
      "4369\n",
      "4370\n",
      "4371\n",
      "4372\n",
      "4373\n",
      "4374\n",
      "4375\n",
      "4376\n",
      "4377\n",
      "4378\n",
      "4379\n",
      "4380\n",
      "4381\n",
      "4382\n",
      "4383\n",
      "4384\n",
      "4385\n",
      "4386\n",
      "4387\n",
      "4388\n",
      "4389\n",
      "4390\n",
      "4391\n",
      "4392\n",
      "4393\n",
      "4394\n",
      "4395\n",
      "4396\n",
      "4397\n",
      "4398\n",
      "4399\n",
      "4400\n",
      "4401\n",
      "4402\n",
      "4403\n",
      "4404\n",
      "4405\n",
      "4406\n",
      "4407\n",
      "4408\n",
      "4409\n",
      "4410\n",
      "4411\n",
      "4412\n",
      "4413\n",
      "4414\n",
      "4415\n",
      "4416\n",
      "4417\n",
      "4418\n",
      "4419\n",
      "4420\n",
      "4421\n",
      "4422\n",
      "4423\n",
      "4424\n",
      "4425\n",
      "4426\n",
      "4427\n",
      "4428\n",
      "4429\n",
      "4430\n",
      "4431\n",
      "4432\n",
      "4433\n",
      "4434\n",
      "4435\n",
      "4436\n",
      "4437\n",
      "4438\n",
      "4439\n",
      "4440\n",
      "4441\n",
      "4442\n",
      "4443\n",
      "4444\n",
      "4445\n",
      "4446\n",
      "4447\n",
      "4448\n",
      "4449\n",
      "4450\n",
      "4451\n",
      "4452\n",
      "4453\n",
      "4454\n",
      "4455\n",
      "4456\n",
      "4457\n",
      "4458\n",
      "4459\n",
      "4460\n",
      "4461\n",
      "4462\n",
      "4463\n",
      "4464\n",
      "4465\n",
      "4466\n",
      "4467\n",
      "4468\n",
      "4469\n",
      "4470\n",
      "4471\n",
      "4472\n",
      "4473\n",
      "4474\n",
      "4475\n",
      "4476\n",
      "4477\n",
      "4478\n",
      "4479\n",
      "4480\n",
      "4481\n",
      "4482\n",
      "4483\n",
      "4484\n",
      "4485\n",
      "4486\n",
      "4487\n",
      "4488\n",
      "4489\n",
      "4490\n",
      "4491\n",
      "4492\n",
      "4493\n",
      "4494\n",
      "4495\n",
      "4496\n",
      "4497\n",
      "4498\n",
      "4499\n",
      "4500\n",
      "4501\n",
      "4502\n",
      "4503\n",
      "4504\n",
      "4505\n",
      "4506\n",
      "4507\n",
      "4508\n",
      "4509\n",
      "4510\n",
      "4511\n",
      "4512\n",
      "4513\n",
      "4514\n",
      "4515\n",
      "4516\n",
      "4517\n",
      "4518\n",
      "4519\n",
      "4520\n",
      "4521\n",
      "4522\n",
      "4523\n",
      "4524\n",
      "4525\n",
      "4526\n",
      "4527\n",
      "4528\n",
      "4529\n",
      "4530\n",
      "4531\n",
      "4532\n",
      "4533\n",
      "4534\n",
      "4535\n",
      "4536\n",
      "4537\n",
      "4538\n",
      "4539\n",
      "4540\n",
      "4541\n",
      "4542\n",
      "4543\n",
      "4544\n",
      "4545\n",
      "4546\n",
      "4547\n",
      "4548\n",
      "4549\n",
      "4550\n",
      "4551\n",
      "4552\n",
      "4553\n",
      "4554\n",
      "4555\n",
      "4556\n",
      "4557\n",
      "4558\n",
      "4559\n",
      "4560\n",
      "4561\n",
      "4562\n",
      "4563\n",
      "4564\n",
      "4565\n",
      "4566\n",
      "4567\n",
      "4568\n",
      "4569\n",
      "4570\n",
      "4571\n",
      "4572\n",
      "4573\n",
      "4574\n",
      "4575\n",
      "4576\n",
      "4577\n",
      "4578\n",
      "4579\n",
      "4580\n",
      "4581\n",
      "4582\n",
      "4583\n",
      "4584\n",
      "4585\n",
      "4586\n",
      "4587\n",
      "4588\n",
      "4589\n",
      "4590\n",
      "4591\n",
      "4592\n",
      "4593\n",
      "4594\n",
      "4595\n",
      "4596\n",
      "4597\n",
      "4598\n",
      "4599\n",
      "4600\n",
      "4601\n",
      "4602\n",
      "4603\n",
      "4604\n",
      "4605\n",
      "4606\n",
      "4607\n",
      "4608\n",
      "4609\n",
      "4610\n",
      "4611\n",
      "4612\n",
      "4613\n",
      "4614\n",
      "4615\n",
      "4616\n",
      "4617\n",
      "4618\n",
      "4619\n",
      "4620\n",
      "4621\n",
      "4622\n",
      "4623\n",
      "4624\n",
      "4625\n",
      "4626\n",
      "4627\n",
      "4628\n",
      "4629\n",
      "4630\n",
      "4631\n",
      "4632\n",
      "4633\n",
      "4634\n",
      "4635\n",
      "4636\n",
      "4637\n",
      "4638\n",
      "4639\n",
      "4640\n",
      "4641\n",
      "4642\n",
      "4643\n",
      "4644\n",
      "4645\n",
      "4646\n",
      "4647\n",
      "4648\n",
      "4649\n",
      "4650\n",
      "4651\n",
      "4652\n",
      "4653\n",
      "4654\n",
      "4655\n",
      "4656\n",
      "4657\n",
      "4658\n",
      "4659\n",
      "4660\n",
      "4661\n",
      "4662\n",
      "4663\n",
      "4664\n",
      "4665\n",
      "4666\n",
      "4667\n",
      "4668\n",
      "4669\n",
      "4670\n",
      "4671\n",
      "4672\n",
      "4673\n",
      "4674\n",
      "4675\n",
      "4676\n",
      "4677\n",
      "4678\n",
      "4679\n",
      "4680\n",
      "4681\n",
      "4682\n",
      "4683\n",
      "4684\n",
      "4685\n",
      "4686\n",
      "4687\n",
      "4688\n",
      "4689\n",
      "4690\n",
      "4691\n",
      "4692\n",
      "4693\n",
      "4694\n",
      "4695\n",
      "4696\n",
      "4697\n",
      "4698\n",
      "4699\n",
      "4700\n",
      "4701\n",
      "4702\n",
      "4703\n",
      "4704\n",
      "4705\n",
      "4706\n",
      "4707\n",
      "4708\n",
      "4709\n",
      "4710\n",
      "4711\n",
      "4712\n",
      "4713\n",
      "4714\n",
      "4715\n",
      "4716\n",
      "4717\n",
      "4718\n",
      "4719\n",
      "4720\n",
      "4721\n",
      "4722\n",
      "4723\n",
      "4724\n",
      "4725\n",
      "4726\n",
      "4727\n",
      "4728\n",
      "4729\n",
      "4730\n",
      "4731\n",
      "4732\n",
      "4733\n",
      "4734\n",
      "4735\n",
      "4736\n",
      "4737\n",
      "4738\n",
      "4739\n",
      "4740\n",
      "4741\n",
      "4742\n",
      "4743\n",
      "4744\n",
      "4745\n",
      "4746\n",
      "4747\n",
      "4748\n",
      "4749\n",
      "4750\n",
      "4751\n",
      "4752\n",
      "4753\n",
      "4754\n",
      "4755\n",
      "4756\n",
      "4757\n",
      "4758\n",
      "4759\n",
      "4760\n",
      "4761\n",
      "4762\n",
      "4763\n",
      "4764\n",
      "4765\n",
      "4766\n",
      "4767\n",
      "4768\n",
      "4769\n",
      "4770\n",
      "4771\n",
      "4772\n",
      "4773\n",
      "4774\n",
      "4775\n",
      "4776\n",
      "4777\n",
      "4778\n",
      "4779\n",
      "4780\n",
      "4781\n",
      "4782\n",
      "4783\n",
      "4784\n",
      "4785\n",
      "4786\n",
      "4787\n",
      "4788\n",
      "4789\n",
      "4790\n",
      "4791\n",
      "4792\n",
      "4793\n",
      "4794\n",
      "4795\n",
      "4796\n",
      "4797\n",
      "4798\n",
      "4799\n",
      "4800\n",
      "4801\n",
      "4802\n",
      "4803\n",
      "4804\n",
      "4805\n",
      "4806\n",
      "4807\n",
      "4808\n",
      "4809\n",
      "4810\n",
      "4811\n",
      "4812\n",
      "4813\n",
      "4814\n",
      "4815\n",
      "4816\n",
      "4817\n",
      "4818\n",
      "4819\n",
      "4820\n",
      "4821\n",
      "4822\n",
      "4823\n",
      "4824\n",
      "4825\n",
      "4826\n",
      "4827\n",
      "4828\n",
      "4829\n",
      "4830\n",
      "4831\n",
      "4832\n",
      "4833\n",
      "4834\n",
      "4835\n",
      "4836\n",
      "4837\n",
      "4838\n",
      "4839\n",
      "4840\n",
      "4841\n",
      "4842\n",
      "4843\n",
      "4844\n",
      "4845\n",
      "4846\n",
      "4847\n",
      "4848\n",
      "4849\n",
      "4850\n",
      "4851\n",
      "4852\n",
      "4853\n",
      "4854\n",
      "4855\n",
      "4856\n",
      "4857\n",
      "4858\n",
      "4859\n",
      "4860\n",
      "4861\n",
      "4862\n",
      "4863\n",
      "4864\n",
      "4865\n",
      "4866\n",
      "4867\n",
      "4868\n",
      "4869\n",
      "4870\n",
      "4871\n",
      "4872\n",
      "4873\n",
      "4874\n",
      "4875\n",
      "4876\n",
      "4877\n",
      "4878\n",
      "4879\n",
      "4880\n",
      "4881\n",
      "4882\n",
      "4883\n",
      "4884\n",
      "4885\n",
      "4886\n",
      "4887\n",
      "4888\n",
      "4889\n",
      "4890\n",
      "4891\n",
      "4892\n",
      "4893\n",
      "4894\n",
      "4895\n",
      "4896\n",
      "4897\n",
      "4898\n",
      "4899\n",
      "4900\n",
      "4901\n",
      "4902\n",
      "4903\n",
      "4904\n",
      "4905\n",
      "4906\n",
      "4907\n",
      "4908\n",
      "4909\n",
      "4910\n",
      "4911\n",
      "4912\n",
      "4913\n",
      "4914\n",
      "4915\n",
      "4916\n",
      "4917\n",
      "4918\n",
      "4919\n",
      "4920\n",
      "4921\n",
      "4922\n",
      "4923\n",
      "4924\n",
      "4925\n",
      "4926\n",
      "4927\n",
      "4928\n",
      "4929\n",
      "4930\n",
      "4931\n",
      "4932\n",
      "4933\n",
      "4934\n",
      "4935\n",
      "4936\n",
      "4937\n",
      "4938\n",
      "4939\n",
      "4940\n",
      "4941\n",
      "4942\n",
      "4943\n",
      "4944\n",
      "4945\n",
      "4946\n",
      "4947\n",
      "4948\n",
      "4949\n",
      "4950\n",
      "4951\n",
      "4952\n",
      "4953\n",
      "4954\n",
      "4955\n",
      "4956\n",
      "4957\n",
      "4958\n",
      "4959\n",
      "4960\n",
      "4961\n",
      "4962\n",
      "4963\n",
      "4964\n",
      "4965\n",
      "4966\n",
      "4967\n",
      "4968\n",
      "4969\n",
      "4970\n",
      "4971\n",
      "4972\n",
      "4973\n",
      "4974\n",
      "4975\n",
      "4976\n",
      "4977\n",
      "4978\n",
      "4979\n",
      "4980\n",
      "4981\n",
      "4982\n",
      "4983\n",
      "4984\n",
      "4985\n",
      "4986\n",
      "4987\n",
      "4988\n",
      "4989\n",
      "4990\n",
      "4991\n",
      "4992\n",
      "4993\n",
      "4994\n",
      "4995\n",
      "4996\n",
      "4997\n",
      "4998\n",
      "4999\n",
      "5000\n",
      "5001\n",
      "5002\n",
      "5003\n",
      "5004\n",
      "5005\n",
      "5006\n",
      "5007\n",
      "5008\n",
      "5009\n",
      "5010\n",
      "5011\n",
      "5012\n",
      "5013\n",
      "5014\n",
      "5015\n",
      "5016\n",
      "5017\n",
      "5018\n",
      "5019\n",
      "5020\n",
      "5021\n",
      "5022\n",
      "5023\n",
      "5024\n",
      "5025\n",
      "5026\n",
      "5027\n",
      "5028\n",
      "5029\n",
      "5030\n",
      "5031\n",
      "5032\n",
      "5033\n",
      "5034\n",
      "5035\n",
      "5036\n",
      "5037\n",
      "5038\n",
      "5039\n",
      "5040\n",
      "5041\n",
      "5042\n",
      "5043\n",
      "5044\n",
      "5045\n",
      "5046\n",
      "5047\n",
      "5048\n",
      "5049\n",
      "5050\n",
      "5051\n",
      "5052\n",
      "5053\n",
      "5054\n",
      "5055\n",
      "5056\n",
      "5057\n",
      "5058\n",
      "5059\n",
      "5060\n",
      "5061\n",
      "5062\n",
      "5063\n",
      "5064\n",
      "5065\n",
      "5066\n",
      "5067\n",
      "5068\n",
      "5069\n",
      "5070\n",
      "5071\n",
      "5072\n",
      "5073\n",
      "5074\n",
      "5075\n",
      "5076\n",
      "5077\n",
      "5078\n",
      "5079\n",
      "5080\n",
      "5081\n",
      "5082\n",
      "5083\n",
      "5084\n",
      "5085\n",
      "5086\n",
      "5087\n",
      "5088\n",
      "5089\n",
      "5090\n",
      "5091\n",
      "5092\n",
      "5093\n",
      "5094\n",
      "5095\n",
      "5096\n",
      "5097\n",
      "5098\n",
      "5099\n",
      "5100\n",
      "5101\n",
      "5102\n",
      "5103\n",
      "5104\n",
      "5105\n",
      "5106\n",
      "5107\n",
      "5108\n",
      "5109\n",
      "5110\n",
      "5111\n",
      "5112\n",
      "5113\n",
      "5114\n",
      "5115\n",
      "5116\n",
      "5117\n",
      "5118\n",
      "5119\n",
      "5120\n",
      "5121\n",
      "5122\n",
      "5123\n",
      "5124\n",
      "5125\n",
      "5126\n",
      "5127\n",
      "5128\n",
      "5129\n",
      "5130\n",
      "5131\n",
      "5132\n",
      "5133\n",
      "5134\n",
      "5135\n",
      "5136\n",
      "5137\n",
      "5138\n",
      "5139\n",
      "5140\n",
      "5141\n",
      "5142\n",
      "5143\n",
      "5144\n",
      "5145\n",
      "5146\n",
      "5147\n",
      "5148\n",
      "5149\n",
      "5150\n",
      "5151\n",
      "5152\n",
      "5153\n",
      "5154\n",
      "5155\n",
      "5156\n",
      "5157\n",
      "5158\n",
      "5159\n",
      "5160\n",
      "5161\n",
      "5162\n",
      "5163\n",
      "5164\n",
      "5165\n",
      "5166\n",
      "5167\n",
      "5168\n",
      "5169\n",
      "5170\n",
      "5171\n",
      "5172\n",
      "5173\n",
      "5174\n",
      "5175\n",
      "5176\n",
      "5177\n",
      "5178\n",
      "5179\n",
      "5180\n",
      "5181\n",
      "5182\n",
      "5183\n",
      "5184\n",
      "5185\n",
      "5186\n",
      "5187\n",
      "5188\n",
      "5189\n",
      "5190\n",
      "5191\n",
      "5192\n",
      "5193\n",
      "5194\n",
      "5195\n",
      "5196\n",
      "5197\n",
      "5198\n",
      "5199\n",
      "5200\n",
      "5201\n",
      "5202\n",
      "5203\n",
      "5204\n",
      "5205\n",
      "5206\n",
      "5207\n",
      "5208\n",
      "5209\n",
      "5210\n",
      "5211\n",
      "5212\n",
      "5213\n",
      "5214\n",
      "5215\n",
      "5216\n",
      "5217\n",
      "5218\n",
      "5219\n",
      "5220\n",
      "5221\n",
      "5222\n",
      "5223\n",
      "5224\n",
      "5225\n",
      "5226\n",
      "5227\n",
      "5228\n",
      "5229\n",
      "5230\n",
      "5231\n",
      "5232\n",
      "5233\n",
      "5234\n",
      "5235\n",
      "5236\n",
      "5237\n",
      "5238\n",
      "5239\n",
      "5240\n",
      "5241\n",
      "5242\n",
      "5243\n",
      "5244\n",
      "5245\n",
      "5246\n",
      "5247\n",
      "5248\n",
      "5249\n",
      "5250\n",
      "5251\n",
      "5252\n",
      "5253\n",
      "5254\n",
      "5255\n",
      "5256\n",
      "5257\n",
      "5258\n",
      "5259\n",
      "5260\n",
      "5261\n",
      "5262\n",
      "5263\n",
      "5264\n",
      "5265\n",
      "5266\n",
      "5267\n",
      "5268\n",
      "5269\n",
      "5270\n",
      "5271\n",
      "5272\n",
      "5273\n",
      "5274\n",
      "5275\n",
      "5276\n",
      "5277\n",
      "5278\n",
      "5279\n",
      "5280\n",
      "5281\n",
      "5282\n",
      "5283\n",
      "5284\n",
      "5285\n",
      "5286\n",
      "5287\n",
      "5288\n",
      "5289\n",
      "5290\n",
      "5291\n",
      "5292\n",
      "5293\n",
      "5294\n",
      "5295\n",
      "5296\n",
      "5297\n",
      "5298\n",
      "5299\n",
      "5300\n",
      "5301\n",
      "5302\n",
      "5303\n",
      "5304\n",
      "5305\n",
      "5306\n",
      "5307\n",
      "5308\n",
      "5309\n",
      "5310\n",
      "5311\n",
      "5312\n",
      "5313\n",
      "5314\n",
      "5315\n",
      "5316\n",
      "5317\n",
      "5318\n",
      "5319\n",
      "5320\n",
      "5321\n",
      "5322\n",
      "5323\n",
      "5324\n",
      "5325\n",
      "5326\n",
      "5327\n",
      "5328\n",
      "5329\n",
      "5330\n",
      "5331\n",
      "5332\n",
      "5333\n",
      "5334\n",
      "5335\n",
      "5336\n",
      "5337\n",
      "5338\n",
      "5339\n",
      "5340\n",
      "5341\n",
      "5342\n",
      "5343\n",
      "5344\n",
      "5345\n",
      "5346\n",
      "5347\n",
      "5348\n",
      "5349\n",
      "5350\n",
      "5351\n",
      "5352\n",
      "5353\n",
      "5354\n",
      "5355\n",
      "5356\n",
      "5357\n",
      "5358\n",
      "5359\n",
      "5360\n",
      "5361\n",
      "5362\n",
      "5363\n",
      "5364\n",
      "5365\n",
      "5366\n",
      "5367\n",
      "5368\n",
      "5369\n",
      "5370\n",
      "5371\n",
      "5372\n",
      "5373\n",
      "5374\n",
      "5375\n",
      "5376\n",
      "5377\n",
      "5378\n",
      "5379\n",
      "5380\n",
      "5381\n",
      "5382\n",
      "5383\n",
      "5384\n",
      "5385\n",
      "5386\n",
      "5387\n",
      "5388\n",
      "5389\n",
      "5390\n",
      "5391\n",
      "5392\n",
      "5393\n",
      "5394\n",
      "5395\n",
      "5396\n",
      "5397\n",
      "5398\n",
      "5399\n",
      "5400\n",
      "5401\n",
      "5402\n",
      "5403\n",
      "5404\n",
      "5405\n",
      "5406\n",
      "5407\n",
      "5408\n",
      "5409\n",
      "5410\n",
      "5411\n",
      "5412\n",
      "5413\n",
      "5414\n",
      "5415\n",
      "5416\n",
      "5417\n",
      "5418\n",
      "5419\n",
      "5420\n",
      "5421\n",
      "5422\n",
      "5423\n",
      "5424\n",
      "5425\n",
      "5426\n",
      "5427\n",
      "5428\n",
      "5429\n",
      "5430\n",
      "5431\n",
      "5432\n",
      "5433\n",
      "5434\n",
      "5435\n",
      "5436\n",
      "5437\n",
      "5438\n",
      "5439\n",
      "5440\n",
      "5441\n",
      "5442\n",
      "5443\n",
      "5444\n",
      "5445\n",
      "5446\n",
      "5447\n",
      "5448\n",
      "5449\n",
      "5450\n",
      "5451\n",
      "5452\n",
      "5453\n",
      "5454\n",
      "5455\n",
      "5456\n",
      "5457\n",
      "5458\n",
      "5459\n",
      "5460\n",
      "5461\n",
      "5462\n",
      "5463\n",
      "5464\n",
      "5465\n",
      "5466\n",
      "5467\n",
      "5468\n",
      "5469\n",
      "5470\n",
      "5471\n",
      "5472\n",
      "5473\n",
      "5474\n",
      "5475\n",
      "5476\n",
      "5477\n",
      "5478\n",
      "5479\n",
      "5480\n",
      "5481\n",
      "5482\n",
      "5483\n",
      "5484\n",
      "5485\n",
      "5486\n",
      "5487\n",
      "5488\n",
      "5489\n",
      "5490\n",
      "5491\n",
      "5492\n",
      "5493\n",
      "5494\n",
      "5495\n",
      "5496\n",
      "5497\n",
      "5498\n",
      "5499\n",
      "5500\n",
      "5501\n",
      "5502\n",
      "5503\n",
      "5504\n",
      "5505\n",
      "5506\n",
      "5507\n",
      "5508\n",
      "5509\n",
      "5510\n",
      "5511\n",
      "5512\n",
      "5513\n",
      "5514\n",
      "5515\n",
      "5516\n",
      "5517\n",
      "5518\n",
      "5519\n",
      "5520\n",
      "5521\n",
      "5522\n",
      "5523\n",
      "5524\n",
      "5525\n",
      "5526\n",
      "5527\n",
      "5528\n",
      "5529\n",
      "5530\n",
      "5531\n",
      "5532\n",
      "5533\n",
      "5534\n",
      "5535\n",
      "5536\n",
      "5537\n",
      "5538\n",
      "5539\n",
      "5540\n",
      "5541\n",
      "5542\n",
      "5543\n",
      "5544\n",
      "5545\n",
      "5546\n",
      "5547\n",
      "5548\n",
      "5549\n",
      "5550\n",
      "5551\n",
      "5552\n",
      "5553\n",
      "5554\n",
      "5555\n",
      "5556\n",
      "5557\n",
      "5558\n",
      "5559\n",
      "5560\n",
      "5561\n",
      "5562\n",
      "5563\n",
      "5564\n",
      "5565\n",
      "5566\n",
      "5567\n",
      "5568\n",
      "5569\n",
      "5570\n",
      "5571\n",
      "5572\n",
      "5573\n",
      "5574\n",
      "5575\n",
      "5576\n",
      "5577\n",
      "5578\n",
      "5579\n",
      "5580\n",
      "5581\n",
      "5582\n",
      "5583\n",
      "5584\n",
      "5585\n",
      "5586\n",
      "5587\n",
      "5588\n",
      "5589\n",
      "5590\n",
      "5591\n",
      "5592\n",
      "5593\n",
      "5594\n",
      "5595\n",
      "5596\n",
      "5597\n",
      "5598\n",
      "5599\n",
      "5600\n",
      "5601\n",
      "5602\n",
      "5603\n",
      "5604\n",
      "5605\n",
      "5606\n",
      "5607\n",
      "5608\n",
      "5609\n",
      "5610\n",
      "5611\n",
      "5612\n",
      "5613\n",
      "5614\n",
      "5615\n",
      "5616\n",
      "5617\n",
      "5618\n",
      "5619\n",
      "5620\n",
      "5621\n",
      "5622\n",
      "5623\n",
      "5624\n",
      "5625\n",
      "5626\n",
      "5627\n",
      "5628\n",
      "5629\n",
      "5630\n",
      "5631\n",
      "5632\n",
      "5633\n",
      "5634\n",
      "5635\n",
      "5636\n",
      "5637\n",
      "5638\n",
      "5639\n",
      "5640\n",
      "5641\n",
      "5642\n",
      "5643\n",
      "5644\n",
      "5645\n",
      "5646\n",
      "5647\n",
      "5648\n",
      "5649\n",
      "5650\n",
      "5651\n",
      "5652\n",
      "5653\n",
      "5654\n",
      "5655\n",
      "5656\n",
      "5657\n",
      "5658\n",
      "5659\n",
      "5660\n",
      "5661\n",
      "5662\n",
      "5663\n",
      "5664\n",
      "5665\n",
      "5666\n",
      "5667\n",
      "5668\n",
      "5669\n",
      "5670\n",
      "5671\n",
      "5672\n",
      "5673\n",
      "5674\n",
      "5675\n",
      "5676\n",
      "5677\n",
      "5678\n",
      "5679\n",
      "5680\n",
      "5681\n",
      "5682\n",
      "5683\n",
      "5684\n",
      "5685\n",
      "5686\n",
      "5687\n",
      "5688\n",
      "5689\n",
      "5690\n",
      "5691\n",
      "5692\n",
      "5693\n",
      "5694\n",
      "5695\n",
      "5696\n",
      "5697\n",
      "5698\n",
      "5699\n",
      "5700\n",
      "5701\n",
      "5702\n",
      "5703\n",
      "5704\n",
      "5705\n",
      "5706\n",
      "5707\n",
      "5708\n",
      "5709\n",
      "5710\n",
      "5711\n",
      "5712\n",
      "5713\n",
      "5714\n",
      "5715\n",
      "5716\n",
      "5717\n",
      "5718\n",
      "5719\n",
      "5720\n",
      "5721\n",
      "5722\n",
      "5723\n",
      "5724\n",
      "5725\n",
      "5726\n",
      "5727\n",
      "5728\n",
      "5729\n",
      "5730\n",
      "5731\n",
      "5732\n",
      "5733\n",
      "5734\n",
      "5735\n",
      "5736\n",
      "5737\n",
      "5738\n",
      "5739\n",
      "5740\n",
      "5741\n",
      "5742\n",
      "5743\n",
      "5744\n",
      "5745\n",
      "5746\n",
      "5747\n",
      "5748\n",
      "5749\n",
      "5750\n",
      "5751\n",
      "5752\n",
      "5753\n",
      "5754\n",
      "5755\n",
      "5756\n",
      "5757\n",
      "5758\n",
      "5759\n",
      "5760\n",
      "5761\n",
      "5762\n",
      "5763\n",
      "5764\n",
      "5765\n",
      "5766\n",
      "5767\n",
      "5768\n",
      "5769\n",
      "5770\n",
      "5771\n",
      "5772\n",
      "5773\n",
      "5774\n",
      "5775\n",
      "5776\n",
      "5777\n",
      "5778\n",
      "5779\n",
      "5780\n",
      "5781\n",
      "5782\n",
      "5783\n",
      "5784\n",
      "5785\n",
      "5786\n",
      "5787\n",
      "5788\n",
      "5789\n",
      "5790\n",
      "5791\n",
      "5792\n",
      "5793\n",
      "5794\n",
      "5795\n",
      "5796\n",
      "5797\n",
      "5798\n",
      "5799\n",
      "5800\n",
      "5801\n",
      "5802\n",
      "5803\n",
      "5804\n",
      "5805\n",
      "5806\n",
      "5807\n",
      "5808\n",
      "5809\n",
      "5810\n",
      "5811\n",
      "5812\n",
      "5813\n",
      "5814\n",
      "5815\n",
      "5816\n",
      "5817\n",
      "5818\n",
      "5819\n",
      "5820\n",
      "5821\n",
      "5822\n",
      "5823\n",
      "5824\n",
      "5825\n",
      "5826\n",
      "5827\n",
      "5828\n",
      "5829\n",
      "5830\n",
      "5831\n",
      "5832\n",
      "5833\n",
      "5834\n",
      "5835\n",
      "5836\n",
      "5837\n",
      "5838\n",
      "5839\n",
      "5840\n",
      "5841\n",
      "5842\n",
      "5843\n",
      "5844\n",
      "5845\n",
      "5846\n",
      "5847\n",
      "5848\n",
      "5849\n",
      "5850\n",
      "5851\n",
      "5852\n",
      "5853\n",
      "5854\n",
      "5855\n",
      "5856\n",
      "5857\n",
      "5858\n",
      "5859\n",
      "5860\n",
      "5861\n",
      "5862\n",
      "5863\n",
      "5864\n",
      "5865\n",
      "5866\n",
      "5867\n",
      "5868\n",
      "5869\n",
      "5870\n",
      "5871\n",
      "5872\n",
      "5873\n",
      "5874\n",
      "5875\n",
      "5876\n",
      "5877\n",
      "5878\n",
      "5879\n",
      "5880\n",
      "5881\n",
      "5882\n",
      "5883\n",
      "5884\n",
      "5885\n",
      "5886\n",
      "5887\n",
      "5888\n",
      "5889\n",
      "5890\n",
      "5891\n",
      "5892\n",
      "5893\n",
      "5894\n",
      "5895\n",
      "5896\n",
      "5897\n",
      "5898\n",
      "5899\n",
      "5900\n",
      "5901\n",
      "5902\n",
      "5903\n",
      "5904\n",
      "5905\n",
      "5906\n",
      "5907\n",
      "5908\n",
      "5909\n",
      "5910\n",
      "5911\n",
      "5912\n",
      "5913\n",
      "5914\n",
      "5915\n",
      "5916\n",
      "5917\n",
      "5918\n",
      "5919\n",
      "5920\n",
      "5921\n",
      "5922\n",
      "5923\n",
      "5924\n",
      "5925\n",
      "5926\n",
      "5927\n",
      "5928\n",
      "5929\n",
      "5930\n",
      "5931\n",
      "5932\n",
      "5933\n",
      "5934\n",
      "5935\n",
      "5936\n",
      "5937\n",
      "5938\n",
      "5939\n",
      "5940\n",
      "5941\n",
      "5942\n",
      "5943\n",
      "5944\n",
      "5945\n",
      "5946\n",
      "5947\n",
      "5948\n",
      "5949\n",
      "5950\n",
      "5951\n",
      "5952\n",
      "5953\n",
      "5954\n",
      "5955\n",
      "5956\n",
      "5957\n",
      "5958\n",
      "5959\n",
      "5960\n",
      "5961\n",
      "5962\n",
      "5963\n",
      "5964\n",
      "5965\n",
      "5966\n",
      "5967\n",
      "5968\n",
      "5969\n",
      "5970\n",
      "5971\n",
      "5972\n",
      "5973\n",
      "5974\n",
      "5975\n",
      "5976\n",
      "5977\n",
      "5978\n",
      "5979\n",
      "5980\n",
      "5981\n",
      "5982\n",
      "5983\n",
      "5984\n",
      "5985\n",
      "5986\n",
      "5987\n",
      "5988\n",
      "5989\n",
      "5990\n",
      "5991\n",
      "5992\n",
      "5993\n",
      "5994\n",
      "5995\n",
      "5996\n",
      "5997\n",
      "5998\n",
      "5999\n",
      "6000\n",
      "6001\n",
      "6002\n",
      "6003\n",
      "6004\n",
      "6005\n",
      "6006\n",
      "6007\n",
      "6008\n",
      "6009\n",
      "6010\n",
      "6011\n",
      "6012\n",
      "6013\n",
      "6014\n",
      "6015\n",
      "6016\n",
      "6017\n",
      "6018\n",
      "6019\n",
      "6020\n",
      "6021\n",
      "6022\n",
      "6023\n",
      "6024\n",
      "6025\n",
      "6026\n",
      "6027\n",
      "6028\n",
      "6029\n",
      "6030\n",
      "6031\n",
      "6032\n",
      "6033\n",
      "6034\n",
      "6035\n",
      "6036\n",
      "6037\n",
      "6038\n",
      "6039\n",
      "6040\n",
      "6041\n",
      "6042\n",
      "6043\n",
      "6044\n",
      "6045\n",
      "6046\n",
      "6047\n",
      "6048\n",
      "6049\n",
      "6050\n",
      "6051\n",
      "6052\n",
      "6053\n",
      "6054\n",
      "6055\n",
      "6056\n",
      "6057\n",
      "6058\n",
      "6059\n",
      "6060\n",
      "6061\n",
      "6062\n",
      "6063\n",
      "6064\n",
      "6065\n",
      "6066\n",
      "6067\n",
      "6068\n",
      "6069\n",
      "6070\n",
      "6071\n",
      "6072\n",
      "6073\n",
      "6074\n",
      "6075\n",
      "6076\n",
      "6077\n",
      "6078\n",
      "6079\n",
      "6080\n",
      "6081\n",
      "6082\n",
      "6083\n",
      "6084\n",
      "6085\n",
      "6086\n",
      "6087\n",
      "6088\n",
      "6089\n",
      "6090\n",
      "6091\n",
      "6092\n",
      "6093\n",
      "6094\n",
      "6095\n",
      "6096\n",
      "6097\n",
      "6098\n",
      "6099\n",
      "6100\n",
      "6101\n",
      "6102\n",
      "6103\n",
      "6104\n",
      "6105\n",
      "6106\n",
      "6107\n",
      "6108\n",
      "6109\n",
      "6110\n",
      "6111\n",
      "6112\n",
      "6113\n",
      "6114\n",
      "6115\n",
      "6116\n",
      "6117\n",
      "6118\n",
      "6119\n",
      "6120\n",
      "6121\n",
      "6122\n",
      "6123\n",
      "6124\n",
      "6125\n",
      "6126\n",
      "6127\n",
      "6128\n",
      "6129\n",
      "6130\n",
      "6131\n",
      "6132\n",
      "6133\n",
      "6134\n",
      "6135\n",
      "6136\n",
      "6137\n",
      "6138\n",
      "6139\n",
      "6140\n",
      "6141\n",
      "6142\n",
      "6143\n",
      "6144\n",
      "6145\n",
      "6146\n",
      "6147\n",
      "6148\n",
      "6149\n",
      "6150\n",
      "6151\n",
      "6152\n",
      "6153\n",
      "6154\n",
      "6155\n",
      "6156\n",
      "6157\n",
      "6158\n",
      "6159\n",
      "6160\n",
      "6161\n",
      "6162\n",
      "6163\n",
      "6164\n",
      "6165\n",
      "6166\n",
      "6167\n",
      "6168\n",
      "6169\n",
      "6170\n",
      "6171\n",
      "6172\n",
      "6173\n",
      "6174\n",
      "6175\n",
      "6176\n",
      "6177\n",
      "6178\n",
      "6179\n",
      "6180\n",
      "6181\n",
      "6182\n",
      "6183\n",
      "6184\n",
      "6185\n",
      "6186\n",
      "6187\n",
      "6188\n",
      "6189\n",
      "6190\n",
      "6191\n",
      "6192\n",
      "6193\n",
      "6194\n",
      "6195\n",
      "6196\n",
      "6197\n",
      "6198\n",
      "6199\n",
      "6200\n",
      "6201\n",
      "6202\n",
      "6203\n",
      "6204\n",
      "6205\n",
      "6206\n",
      "6207\n",
      "6208\n",
      "6209\n",
      "6210\n",
      "6211\n",
      "6212\n",
      "6213\n",
      "6214\n",
      "6215\n",
      "6216\n",
      "6217\n",
      "6218\n",
      "6219\n",
      "6220\n",
      "6221\n",
      "6222\n",
      "6223\n",
      "6224\n",
      "6225\n",
      "6226\n",
      "6227\n",
      "6228\n",
      "6229\n",
      "6230\n",
      "6231\n",
      "6232\n",
      "6233\n",
      "6234\n",
      "6235\n",
      "6236\n",
      "6237\n",
      "6238\n",
      "6239\n",
      "6240\n",
      "6241\n",
      "6242\n",
      "6243\n",
      "6244\n",
      "6245\n",
      "6246\n",
      "6247\n",
      "6248\n",
      "6249\n",
      "6250\n",
      "6251\n",
      "6252\n",
      "6253\n",
      "6254\n",
      "6255\n",
      "6256\n",
      "6257\n",
      "6258\n",
      "6259\n",
      "6260\n",
      "6261\n",
      "6262\n",
      "6263\n",
      "6264\n",
      "6265\n",
      "6266\n",
      "6267\n",
      "6268\n",
      "6269\n",
      "6270\n",
      "6271\n",
      "6272\n",
      "6273\n",
      "6274\n",
      "6275\n",
      "6276\n",
      "6277\n",
      "6278\n",
      "6279\n",
      "6280\n",
      "6281\n",
      "6282\n",
      "6283\n",
      "6284\n",
      "6285\n",
      "6286\n",
      "6287\n",
      "6288\n",
      "6289\n",
      "6290\n",
      "6291\n",
      "6292\n",
      "6293\n",
      "6294\n",
      "6295\n",
      "6296\n",
      "6297\n",
      "6298\n",
      "6299\n",
      "6300\n",
      "6301\n",
      "6302\n",
      "6303\n",
      "6304\n",
      "6305\n",
      "6306\n",
      "6307\n",
      "6308\n",
      "6309\n",
      "6310\n",
      "6311\n",
      "6312\n",
      "6313\n",
      "6314\n",
      "6315\n",
      "6316\n",
      "6317\n",
      "6318\n",
      "6319\n",
      "6320\n",
      "6321\n",
      "6322\n",
      "6323\n",
      "6324\n",
      "6325\n",
      "6326\n",
      "6327\n",
      "6328\n",
      "6329\n",
      "6330\n",
      "6331\n",
      "6332\n",
      "6333\n",
      "6334\n",
      "6335\n",
      "6336\n",
      "6337\n",
      "6338\n",
      "6339\n",
      "6340\n",
      "6341\n",
      "6342\n",
      "6343\n",
      "6344\n",
      "6345\n",
      "6346\n",
      "6347\n",
      "6348\n",
      "6349\n",
      "6350\n",
      "6351\n",
      "6352\n",
      "6353\n",
      "6354\n",
      "6355\n",
      "6356\n",
      "6357\n",
      "6358\n",
      "6359\n",
      "6360\n",
      "6361\n",
      "6362\n",
      "6363\n",
      "6364\n",
      "6365\n",
      "6366\n",
      "6367\n",
      "6368\n",
      "6369\n",
      "6370\n",
      "6371\n",
      "6372\n",
      "6373\n",
      "6374\n",
      "6375\n",
      "6376\n",
      "6377\n",
      "6378\n",
      "6379\n",
      "6380\n",
      "6381\n",
      "6382\n",
      "6383\n",
      "6384\n",
      "6385\n",
      "6386\n",
      "6387\n",
      "6388\n",
      "6389\n",
      "6390\n",
      "6391\n",
      "6392\n",
      "6393\n",
      "6394\n",
      "6395\n",
      "6396\n",
      "6397\n",
      "6398\n",
      "6399\n",
      "6400\n",
      "6401\n",
      "6402\n",
      "6403\n",
      "6404\n",
      "6405\n",
      "6406\n",
      "6407\n",
      "6408\n",
      "6409\n",
      "6410\n",
      "6411\n",
      "6412\n",
      "6413\n",
      "6414\n",
      "6415\n",
      "6416\n",
      "6417\n",
      "6418\n",
      "6419\n",
      "6420\n",
      "6421\n",
      "6422\n",
      "6423\n",
      "6424\n",
      "6425\n",
      "6426\n",
      "6427\n",
      "6428\n",
      "6429\n",
      "6430\n",
      "6431\n",
      "6432\n",
      "6433\n",
      "6434\n",
      "6435\n",
      "6436\n",
      "6437\n",
      "6438\n",
      "6439\n",
      "6440\n",
      "6441\n",
      "6442\n",
      "6443\n",
      "6444\n",
      "6445\n",
      "6446\n",
      "6447\n",
      "6448\n",
      "6449\n",
      "6450\n",
      "6451\n",
      "6452\n",
      "6453\n",
      "6454\n",
      "6455\n",
      "6456\n",
      "6457\n",
      "6458\n",
      "6459\n",
      "6460\n",
      "6461\n",
      "6462\n",
      "6463\n",
      "6464\n",
      "6465\n",
      "6466\n",
      "6467\n",
      "6468\n",
      "6469\n",
      "6470\n",
      "6471\n",
      "6472\n",
      "6473\n",
      "6474\n",
      "6475\n",
      "6476\n",
      "6477\n",
      "6478\n",
      "6479\n",
      "6480\n",
      "6481\n",
      "6482\n",
      "6483\n",
      "6484\n",
      "6485\n",
      "6486\n",
      "6487\n",
      "6488\n",
      "6489\n",
      "6490\n",
      "6491\n",
      "6492\n",
      "6493\n",
      "6494\n",
      "6495\n",
      "6496\n",
      "6497\n",
      "6498\n",
      "6499\n",
      "6500\n",
      "6501\n",
      "6502\n",
      "6503\n",
      "6504\n",
      "6505\n",
      "6506\n",
      "6507\n",
      "6508\n",
      "6509\n",
      "6510\n",
      "6511\n",
      "6512\n",
      "6513\n",
      "6514\n",
      "6515\n",
      "6516\n",
      "6517\n",
      "6518\n",
      "6519\n",
      "6520\n",
      "6521\n",
      "6522\n",
      "6523\n",
      "6524\n",
      "6525\n",
      "6526\n",
      "6527\n",
      "6528\n",
      "6529\n",
      "6530\n",
      "6531\n",
      "6532\n",
      "6533\n",
      "6534\n",
      "6535\n",
      "6536\n",
      "6537\n",
      "6538\n",
      "6539\n",
      "6540\n",
      "6541\n",
      "6542\n",
      "6543\n",
      "6544\n",
      "6545\n",
      "6546\n",
      "6547\n",
      "6548\n",
      "6549\n",
      "6550\n",
      "6551\n",
      "6552\n",
      "6553\n",
      "6554\n",
      "6555\n",
      "6556\n",
      "6557\n",
      "6558\n",
      "6559\n",
      "6560\n",
      "6561\n",
      "6562\n",
      "6563\n",
      "6564\n",
      "6565\n",
      "6566\n",
      "6567\n",
      "6568\n",
      "6569\n",
      "6570\n",
      "6571\n",
      "6572\n",
      "6573\n",
      "6574\n",
      "6575\n",
      "6576\n",
      "6577\n",
      "6578\n",
      "6579\n",
      "6580\n",
      "6581\n",
      "6582\n",
      "6583\n",
      "6584\n",
      "6585\n",
      "6586\n",
      "6587\n",
      "6588\n",
      "6589\n",
      "6590\n",
      "6591\n",
      "6592\n",
      "6593\n",
      "6594\n",
      "6595\n",
      "6596\n",
      "6597\n",
      "6598\n",
      "6599\n",
      "6600\n",
      "6601\n",
      "6602\n",
      "6603\n",
      "6604\n",
      "6605\n",
      "6606\n",
      "6607\n",
      "6608\n",
      "6609\n",
      "6610\n",
      "6611\n",
      "6612\n",
      "6613\n",
      "6614\n",
      "6615\n",
      "6616\n",
      "6617\n",
      "6618\n",
      "6619\n",
      "6620\n",
      "6621\n",
      "6622\n",
      "6623\n",
      "6624\n",
      "6625\n",
      "6626\n",
      "6627\n",
      "6628\n",
      "6629\n",
      "6630\n",
      "6631\n",
      "6632\n",
      "6633\n",
      "6634\n",
      "6635\n",
      "6636\n",
      "6637\n",
      "6638\n",
      "6639\n",
      "6640\n",
      "6641\n",
      "6642\n",
      "6643\n",
      "6644\n",
      "6645\n",
      "6646\n",
      "6647\n",
      "6648\n",
      "6649\n",
      "6650\n",
      "6651\n",
      "6652\n",
      "6653\n",
      "6654\n",
      "6655\n",
      "6656\n",
      "6657\n",
      "6658\n",
      "6659\n",
      "6660\n",
      "6661\n",
      "6662\n",
      "6663\n",
      "6664\n",
      "6665\n",
      "6666\n",
      "6667\n",
      "6668\n",
      "6669\n",
      "6670\n",
      "6671\n",
      "6672\n",
      "6673\n",
      "6674\n",
      "6675\n",
      "6676\n",
      "6677\n",
      "6678\n",
      "6679\n",
      "6680\n",
      "6681\n",
      "6682\n",
      "6683\n",
      "6684\n",
      "6685\n",
      "6686\n",
      "6687\n",
      "6688\n",
      "6689\n",
      "6690\n",
      "6691\n",
      "6692\n",
      "6693\n",
      "6694\n",
      "6695\n",
      "6696\n",
      "6697\n",
      "6698\n",
      "6699\n",
      "6700\n",
      "6701\n",
      "6702\n",
      "6703\n",
      "6704\n",
      "6705\n",
      "6706\n",
      "6707\n",
      "6708\n",
      "6709\n",
      "6710\n",
      "6711\n",
      "6712\n",
      "6713\n",
      "6714\n",
      "6715\n",
      "6716\n",
      "6717\n",
      "6718\n",
      "6719\n",
      "6720\n",
      "6721\n",
      "6722\n",
      "6723\n",
      "6724\n",
      "6725\n",
      "6726\n",
      "6727\n",
      "6728\n",
      "6729\n",
      "6730\n",
      "6731\n",
      "6732\n",
      "6733\n",
      "6734\n",
      "6735\n",
      "6736\n",
      "6737\n",
      "6738\n",
      "6739\n",
      "6740\n",
      "6741\n",
      "6742\n",
      "6743\n",
      "6744\n",
      "6745\n",
      "6746\n",
      "6747\n",
      "6748\n",
      "6749\n",
      "6750\n",
      "6751\n",
      "6752\n",
      "6753\n",
      "6754\n",
      "6755\n",
      "6756\n",
      "6757\n",
      "6758\n",
      "6759\n",
      "6760\n",
      "6761\n",
      "6762\n",
      "6763\n",
      "6764\n",
      "6765\n",
      "6766\n",
      "6767\n",
      "6768\n",
      "6769\n",
      "6770\n",
      "6771\n",
      "6772\n",
      "6773\n",
      "6774\n",
      "6775\n",
      "6776\n",
      "6777\n",
      "6778\n",
      "6779\n",
      "6780\n",
      "6781\n",
      "6782\n",
      "6783\n",
      "6784\n",
      "6785\n",
      "6786\n",
      "6787\n",
      "6788\n",
      "6789\n",
      "6790\n",
      "6791\n",
      "6792\n",
      "6793\n",
      "6794\n",
      "6795\n",
      "6796\n",
      "6797\n",
      "6798\n",
      "6799\n",
      "6800\n",
      "6801\n",
      "6802\n",
      "6803\n",
      "6804\n",
      "6805\n",
      "6806\n",
      "6807\n",
      "6808\n",
      "6809\n",
      "6810\n",
      "6811\n",
      "6812\n",
      "6813\n",
      "6814\n",
      "6815\n",
      "6816\n",
      "6817\n",
      "6818\n",
      "6819\n",
      "6820\n",
      "6821\n",
      "6822\n",
      "6823\n",
      "6824\n",
      "6825\n",
      "6826\n",
      "6827\n",
      "6828\n",
      "6829\n",
      "6830\n",
      "6831\n",
      "6832\n",
      "6833\n",
      "6834\n",
      "6835\n",
      "6836\n",
      "6837\n",
      "6838\n",
      "6839\n",
      "6840\n",
      "6841\n",
      "6842\n",
      "6843\n",
      "6844\n",
      "6845\n",
      "6846\n",
      "6847\n",
      "6848\n",
      "6849\n",
      "6850\n",
      "6851\n",
      "6852\n",
      "6853\n",
      "6854\n",
      "6855\n",
      "6856\n",
      "6857\n",
      "6858\n",
      "6859\n",
      "6860\n",
      "6861\n",
      "6862\n",
      "6863\n",
      "6864\n",
      "6865\n",
      "6866\n",
      "6867\n",
      "6868\n",
      "6869\n",
      "6870\n",
      "6871\n",
      "6872\n",
      "6873\n",
      "6874\n",
      "6875\n",
      "6876\n",
      "6877\n",
      "6878\n",
      "6879\n",
      "6880\n",
      "6881\n",
      "6882\n",
      "6883\n",
      "6884\n",
      "6885\n",
      "6886\n",
      "6887\n",
      "6888\n",
      "6889\n",
      "6890\n",
      "6891\n",
      "6892\n",
      "6893\n",
      "6894\n",
      "6895\n",
      "6896\n",
      "6897\n",
      "6898\n",
      "6899\n",
      "6900\n",
      "6901\n",
      "6902\n",
      "6903\n",
      "6904\n",
      "6905\n",
      "6906\n",
      "6907\n",
      "6908\n",
      "6909\n",
      "6910\n",
      "6911\n",
      "6912\n",
      "6913\n",
      "6914\n",
      "6915\n",
      "6916\n",
      "6917\n",
      "6918\n",
      "6919\n",
      "6920\n",
      "6921\n",
      "6922\n",
      "6923\n",
      "6924\n",
      "6925\n",
      "6926\n",
      "6927\n",
      "6928\n",
      "6929\n",
      "6930\n",
      "6931\n",
      "6932\n",
      "6933\n",
      "6934\n",
      "6935\n",
      "6936\n",
      "6937\n",
      "6938\n",
      "6939\n",
      "6940\n",
      "6941\n",
      "6942\n",
      "6943\n",
      "6944\n",
      "6945\n",
      "6946\n",
      "6947\n",
      "6948\n",
      "6949\n",
      "6950\n",
      "6951\n",
      "6952\n",
      "6953\n",
      "6954\n",
      "6955\n",
      "6956\n",
      "6957\n",
      "6958\n",
      "6959\n",
      "6960\n",
      "6961\n",
      "6962\n",
      "6963\n",
      "6964\n",
      "6965\n",
      "6966\n",
      "6967\n",
      "6968\n",
      "6969\n",
      "6970\n",
      "6971\n",
      "6972\n",
      "6973\n",
      "6974\n",
      "6975\n",
      "6976\n",
      "6977\n",
      "6978\n",
      "6979\n",
      "6980\n",
      "6981\n",
      "6982\n",
      "6983\n",
      "6984\n",
      "6985\n",
      "6986\n",
      "6987\n",
      "6988\n",
      "6989\n",
      "6990\n",
      "6991\n",
      "6992\n",
      "6993\n",
      "6994\n",
      "6995\n",
      "6996\n",
      "6997\n",
      "6998\n",
      "6999\n",
      "7000\n",
      "7001\n",
      "7002\n",
      "7003\n",
      "7004\n",
      "7005\n",
      "7006\n",
      "7007\n",
      "7008\n",
      "7009\n",
      "7010\n",
      "7011\n",
      "7012\n",
      "7013\n",
      "7014\n",
      "7015\n",
      "7016\n",
      "7017\n",
      "7018\n",
      "7019\n",
      "7020\n",
      "7021\n",
      "7022\n",
      "7023\n",
      "7024\n",
      "7025\n",
      "7026\n",
      "7027\n",
      "7028\n",
      "7029\n",
      "7030\n",
      "7031\n",
      "7032\n",
      "7033\n",
      "7034\n",
      "7035\n",
      "7036\n",
      "7037\n",
      "7038\n",
      "7039\n",
      "7040\n",
      "7041\n",
      "7042\n",
      "7043\n",
      "7044\n",
      "7045\n",
      "7046\n",
      "7047\n",
      "7048\n",
      "7049\n",
      "7050\n",
      "7051\n",
      "7052\n",
      "7053\n",
      "7054\n",
      "7055\n",
      "7056\n",
      "7057\n",
      "7058\n",
      "7059\n",
      "7060\n",
      "7061\n",
      "7062\n",
      "7063\n",
      "7064\n",
      "7065\n",
      "7066\n",
      "7067\n",
      "7068\n",
      "7069\n",
      "7070\n",
      "7071\n",
      "7072\n",
      "7073\n",
      "7074\n",
      "7075\n",
      "7076\n",
      "7077\n",
      "7078\n",
      "7079\n",
      "7080\n",
      "7081\n",
      "7082\n",
      "7083\n",
      "7084\n",
      "7085\n",
      "7086\n",
      "7087\n",
      "7088\n",
      "7089\n",
      "7090\n",
      "7091\n",
      "7092\n",
      "7093\n",
      "7094\n",
      "7095\n",
      "7096\n",
      "7097\n",
      "7098\n",
      "7099\n",
      "7100\n",
      "7101\n",
      "7102\n",
      "7103\n",
      "7104\n",
      "7105\n",
      "7106\n",
      "7107\n",
      "7108\n",
      "7109\n",
      "7110\n",
      "7111\n",
      "7112\n",
      "7113\n",
      "7114\n",
      "7115\n",
      "7116\n",
      "7117\n",
      "7118\n",
      "7119\n",
      "7120\n",
      "7121\n",
      "7122\n",
      "7123\n",
      "7124\n",
      "7125\n",
      "7126\n",
      "7127\n",
      "7128\n",
      "7129\n",
      "7130\n",
      "7131\n",
      "7132\n",
      "7133\n",
      "7134\n",
      "7135\n",
      "7136\n",
      "7137\n",
      "7138\n",
      "7139\n",
      "7140\n",
      "7141\n",
      "7142\n",
      "7143\n",
      "7144\n",
      "7145\n",
      "7146\n",
      "7147\n",
      "7148\n",
      "7149\n",
      "7150\n",
      "7151\n",
      "7152\n",
      "7153\n",
      "7154\n",
      "7155\n",
      "7156\n",
      "7157\n",
      "7158\n",
      "7159\n",
      "7160\n",
      "7161\n",
      "7162\n",
      "7163\n",
      "7164\n",
      "7165\n",
      "7166\n",
      "7167\n",
      "7168\n",
      "7169\n",
      "7170\n",
      "7171\n",
      "7172\n",
      "7173\n",
      "7174\n",
      "7175\n",
      "7176\n",
      "7177\n",
      "7178\n",
      "7179\n",
      "7180\n",
      "7181\n",
      "7182\n",
      "7183\n",
      "7184\n",
      "7185\n",
      "7186\n",
      "7187\n",
      "7188\n",
      "7189\n",
      "7190\n",
      "7191\n",
      "7192\n",
      "7193\n",
      "7194\n",
      "7195\n",
      "7196\n",
      "7197\n",
      "7198\n",
      "7199\n",
      "7200\n",
      "7201\n",
      "7202\n",
      "7203\n",
      "7204\n",
      "7205\n",
      "7206\n",
      "7207\n",
      "7208\n",
      "7209\n",
      "7210\n",
      "7211\n",
      "7212\n",
      "7213\n",
      "7214\n",
      "7215\n",
      "7216\n",
      "7217\n",
      "7218\n",
      "7219\n",
      "7220\n",
      "7221\n",
      "7222\n",
      "7223\n",
      "7224\n",
      "7225\n",
      "7226\n",
      "7227\n",
      "7228\n",
      "7229\n",
      "7230\n",
      "7231\n",
      "7232\n",
      "7233\n",
      "7234\n",
      "7235\n",
      "7236\n",
      "7237\n",
      "7238\n",
      "7239\n",
      "7240\n",
      "7241\n",
      "7242\n",
      "7243\n",
      "7244\n",
      "7245\n",
      "7246\n",
      "7247\n",
      "7248\n",
      "7249\n",
      "7250\n",
      "7251\n",
      "7252\n",
      "7253\n",
      "7254\n",
      "7255\n",
      "7256\n",
      "7257\n",
      "7258\n",
      "7259\n",
      "7260\n",
      "7261\n",
      "7262\n",
      "7263\n",
      "7264\n",
      "7265\n",
      "7266\n",
      "7267\n",
      "7268\n",
      "7269\n",
      "7270\n",
      "7271\n",
      "7272\n",
      "7273\n",
      "7274\n",
      "7275\n",
      "7276\n",
      "7277\n",
      "7278\n",
      "7279\n",
      "7280\n",
      "7281\n",
      "7282\n",
      "7283\n",
      "7284\n",
      "7285\n",
      "7286\n",
      "7287\n",
      "7288\n",
      "7289\n",
      "7290\n",
      "7291\n",
      "7292\n",
      "7293\n",
      "7294\n",
      "7295\n",
      "7296\n",
      "7297\n",
      "7298\n",
      "7299\n",
      "7300\n",
      "7301\n",
      "7302\n",
      "7303\n",
      "7304\n",
      "7305\n",
      "7306\n",
      "7307\n",
      "7308\n",
      "7309\n",
      "7310\n",
      "7311\n",
      "7312\n",
      "7313\n",
      "7314\n",
      "7315\n",
      "7316\n",
      "7317\n",
      "7318\n",
      "7319\n",
      "7320\n",
      "7321\n",
      "7322\n",
      "7323\n",
      "7324\n",
      "7325\n",
      "7326\n",
      "7327\n",
      "7328\n",
      "7329\n",
      "7330\n",
      "7331\n",
      "7332\n",
      "7333\n",
      "7334\n",
      "7335\n",
      "7336\n",
      "7337\n",
      "7338\n",
      "7339\n",
      "7340\n",
      "7341\n",
      "7342\n",
      "7343\n",
      "7344\n",
      "7345\n",
      "7346\n",
      "7347\n",
      "7348\n",
      "7349\n",
      "7350\n",
      "7351\n",
      "7352\n",
      "7353\n",
      "7354\n",
      "7355\n",
      "7356\n",
      "7357\n",
      "7358\n",
      "7359\n",
      "7360\n",
      "7361\n",
      "7362\n",
      "7363\n",
      "7364\n",
      "7365\n",
      "7366\n",
      "7367\n",
      "7368\n",
      "7369\n",
      "7370\n",
      "7371\n",
      "7372\n",
      "7373\n",
      "7374\n",
      "7375\n",
      "7376\n",
      "7377\n",
      "7378\n",
      "7379\n",
      "7380\n",
      "7381\n",
      "7382\n",
      "7383\n",
      "7384\n",
      "7385\n",
      "7386\n",
      "7387\n",
      "7388\n",
      "7389\n",
      "7390\n",
      "7391\n",
      "7392\n",
      "7393\n",
      "7394\n",
      "7395\n",
      "7396\n",
      "7397\n",
      "7398\n",
      "7399\n",
      "7400\n",
      "7401\n",
      "7402\n",
      "7403\n",
      "7404\n",
      "7405\n",
      "7406\n",
      "7407\n",
      "7408\n",
      "7409\n",
      "7410\n",
      "7411\n",
      "7412\n",
      "7413\n",
      "7414\n",
      "7415\n",
      "7416\n",
      "7417\n",
      "7418\n",
      "7419\n",
      "7420\n",
      "7421\n",
      "7422\n",
      "7423\n",
      "7424\n",
      "7425\n",
      "7426\n",
      "7427\n",
      "7428\n",
      "7429\n",
      "7430\n",
      "7431\n",
      "7432\n",
      "7433\n",
      "7434\n",
      "7435\n",
      "7436\n",
      "7437\n",
      "7438\n",
      "7439\n",
      "7440\n",
      "7441\n",
      "7442\n",
      "7443\n",
      "7444\n",
      "7445\n",
      "7446\n",
      "7447\n",
      "7448\n",
      "7449\n",
      "7450\n",
      "7451\n",
      "7452\n",
      "7453\n",
      "7454\n",
      "7455\n",
      "7456\n",
      "7457\n",
      "7458\n",
      "7459\n",
      "7460\n",
      "7461\n",
      "7462\n",
      "7463\n",
      "7464\n",
      "7465\n",
      "7466\n",
      "7467\n",
      "7468\n",
      "7469\n",
      "7470\n",
      "7471\n",
      "7472\n",
      "7473\n",
      "7474\n",
      "7475\n",
      "7476\n",
      "7477\n",
      "7478\n",
      "7479\n",
      "7480\n",
      "7481\n",
      "7482\n",
      "7483\n",
      "7484\n",
      "7485\n",
      "7486\n",
      "7487\n",
      "7488\n",
      "7489\n",
      "7490\n",
      "7491\n",
      "7492\n",
      "7493\n",
      "7494\n",
      "7495\n",
      "7496\n",
      "7497\n",
      "7498\n",
      "7499\n",
      "7500\n",
      "7501\n",
      "7502\n",
      "7503\n",
      "7504\n",
      "7505\n",
      "7506\n",
      "7507\n",
      "7508\n",
      "7509\n",
      "7510\n",
      "7511\n",
      "7512\n",
      "7513\n",
      "7514\n",
      "7515\n",
      "7516\n",
      "7517\n",
      "7518\n",
      "7519\n",
      "7520\n",
      "7521\n",
      "7522\n",
      "7523\n",
      "7524\n",
      "7525\n",
      "7526\n",
      "7527\n",
      "7528\n",
      "7529\n",
      "7530\n",
      "7531\n",
      "7532\n",
      "7533\n",
      "7534\n",
      "7535\n",
      "7536\n",
      "7537\n",
      "7538\n",
      "7539\n",
      "7540\n",
      "7541\n",
      "7542\n",
      "7543\n",
      "7544\n",
      "7545\n",
      "7546\n",
      "7547\n",
      "7548\n",
      "7549\n",
      "7550\n",
      "7551\n",
      "7552\n",
      "7553\n",
      "7554\n",
      "7555\n",
      "7556\n",
      "7557\n",
      "7558\n",
      "7559\n",
      "7560\n",
      "7561\n",
      "7562\n",
      "7563\n",
      "7564\n",
      "7565\n",
      "7566\n",
      "7567\n",
      "7568\n",
      "7569\n",
      "7570\n",
      "7571\n",
      "7572\n",
      "7573\n",
      "7574\n",
      "7575\n",
      "7576\n",
      "7577\n",
      "7578\n",
      "7579\n",
      "7580\n",
      "7581\n",
      "7582\n",
      "7583\n",
      "7584\n",
      "7585\n",
      "7586\n",
      "7587\n",
      "7588\n",
      "7589\n",
      "7590\n",
      "7591\n",
      "7592\n",
      "7593\n",
      "7594\n",
      "7595\n",
      "7596\n",
      "7597\n",
      "7598\n",
      "7599\n",
      "7600\n",
      "7601\n",
      "7602\n",
      "7603\n",
      "7604\n",
      "7605\n",
      "7606\n",
      "7607\n",
      "7608\n",
      "7609\n",
      "7610\n",
      "7611\n",
      "7612\n",
      "7613\n",
      "7614\n",
      "7615\n",
      "7616\n",
      "7617\n",
      "7618\n",
      "7619\n",
      "7620\n",
      "7621\n",
      "7622\n",
      "7623\n",
      "7624\n",
      "7625\n",
      "7626\n",
      "7627\n",
      "7628\n",
      "7629\n",
      "7630\n",
      "7631\n",
      "7632\n",
      "7633\n",
      "7634\n",
      "7635\n",
      "7636\n",
      "7637\n",
      "7638\n",
      "7639\n",
      "7640\n",
      "7641\n",
      "7642\n",
      "7643\n",
      "7644\n",
      "7645\n",
      "7646\n",
      "7647\n",
      "7648\n",
      "7649\n",
      "7650\n",
      "7651\n",
      "7652\n",
      "7653\n",
      "7654\n",
      "7655\n",
      "7656\n",
      "7657\n",
      "7658\n",
      "7659\n",
      "7660\n",
      "7661\n",
      "7662\n",
      "7663\n",
      "7664\n",
      "7665\n",
      "7666\n",
      "7667\n",
      "7668\n",
      "7669\n",
      "7670\n",
      "7671\n",
      "7672\n",
      "7673\n",
      "7674\n",
      "7675\n",
      "7676\n",
      "7677\n",
      "7678\n",
      "7679\n",
      "7680\n",
      "7681\n",
      "7682\n",
      "7683\n",
      "7684\n",
      "7685\n",
      "7686\n",
      "7687\n",
      "7688\n",
      "7689\n",
      "7690\n",
      "7691\n",
      "7692\n",
      "7693\n",
      "7694\n",
      "7695\n",
      "7696\n",
      "7697\n",
      "7698\n",
      "7699\n",
      "7700\n",
      "7701\n",
      "7702\n",
      "7703\n",
      "7704\n",
      "7705\n",
      "7706\n",
      "7707\n",
      "7708\n",
      "7709\n",
      "7710\n",
      "7711\n",
      "7712\n",
      "7713\n",
      "7714\n",
      "7715\n",
      "7716\n",
      "7717\n",
      "7718\n",
      "7719\n",
      "7720\n",
      "7721\n",
      "7722\n",
      "7723\n",
      "7724\n",
      "7725\n",
      "7726\n",
      "7727\n",
      "7728\n",
      "7729\n",
      "7730\n",
      "7731\n",
      "7732\n",
      "7733\n",
      "7734\n",
      "7735\n",
      "7736\n",
      "7737\n",
      "7738\n",
      "7739\n",
      "7740\n",
      "7741\n",
      "7742\n",
      "7743\n",
      "7744\n",
      "7745\n",
      "7746\n",
      "7747\n",
      "7748\n",
      "7749\n",
      "7750\n",
      "7751\n",
      "7752\n",
      "7753\n",
      "7754\n",
      "7755\n",
      "7756\n",
      "7757\n",
      "7758\n",
      "7759\n",
      "7760\n",
      "7761\n",
      "7762\n",
      "7763\n",
      "7764\n",
      "7765\n",
      "7766\n",
      "7767\n",
      "7768\n",
      "7769\n",
      "7770\n",
      "7771\n",
      "7772\n",
      "7773\n",
      "7774\n",
      "7775\n",
      "7776\n",
      "7777\n",
      "7778\n",
      "7779\n",
      "7780\n",
      "7781\n",
      "7782\n",
      "7783\n",
      "7784\n",
      "7785\n",
      "7786\n",
      "7787\n",
      "7788\n",
      "7789\n",
      "7790\n",
      "7791\n",
      "7792\n",
      "7793\n",
      "7794\n",
      "7795\n",
      "7796\n",
      "7797\n",
      "7798\n",
      "7799\n",
      "7800\n",
      "7801\n",
      "7802\n",
      "7803\n",
      "7804\n",
      "7805\n",
      "7806\n",
      "7807\n",
      "7808\n",
      "7809\n",
      "7810\n",
      "7811\n",
      "7812\n",
      "7813\n",
      "7814\n",
      "7815\n",
      "7816\n",
      "7817\n",
      "7818\n",
      "7819\n",
      "7820\n",
      "7821\n",
      "7822\n",
      "7823\n",
      "7824\n",
      "7825\n",
      "7826\n",
      "7827\n",
      "7828\n",
      "7829\n",
      "7830\n",
      "7831\n",
      "7832\n",
      "7833\n",
      "7834\n",
      "7835\n",
      "7836\n",
      "7837\n",
      "7838\n",
      "7839\n",
      "7840\n",
      "7841\n",
      "7842\n",
      "7843\n",
      "7844\n",
      "7845\n",
      "7846\n",
      "7847\n",
      "7848\n",
      "7849\n",
      "7850\n",
      "7851\n",
      "7852\n",
      "7853\n",
      "7854\n",
      "7855\n",
      "7856\n",
      "7857\n",
      "7858\n",
      "7859\n",
      "7860\n",
      "7861\n",
      "7862\n",
      "7863\n",
      "7864\n",
      "7865\n",
      "7866\n",
      "7867\n",
      "7868\n",
      "7869\n",
      "7870\n",
      "7871\n",
      "7872\n",
      "7873\n",
      "7874\n",
      "7875\n",
      "7876\n",
      "7877\n",
      "7878\n",
      "7879\n",
      "7880\n",
      "7881\n",
      "7882\n",
      "7883\n",
      "7884\n",
      "7885\n",
      "7886\n",
      "7887\n",
      "7888\n",
      "7889\n",
      "7890\n",
      "7891\n",
      "7892\n",
      "7893\n",
      "7894\n",
      "7895\n",
      "7896\n",
      "7897\n",
      "7898\n",
      "7899\n",
      "7900\n",
      "7901\n",
      "7902\n",
      "7903\n",
      "7904\n",
      "7905\n",
      "7906\n",
      "7907\n",
      "7908\n",
      "7909\n",
      "7910\n",
      "7911\n",
      "7912\n",
      "7913\n",
      "7914\n",
      "7915\n",
      "7916\n",
      "7917\n",
      "7918\n",
      "7919\n",
      "7920\n",
      "7921\n",
      "7922\n",
      "7923\n",
      "7924\n",
      "7925\n",
      "7926\n",
      "7927\n",
      "7928\n",
      "7929\n",
      "7930\n",
      "7931\n",
      "7932\n",
      "7933\n",
      "7934\n",
      "7935\n",
      "7936\n",
      "7937\n",
      "7938\n",
      "7939\n",
      "7940\n",
      "7941\n",
      "7942\n",
      "7943\n",
      "7944\n",
      "7945\n",
      "7946\n",
      "7947\n",
      "7948\n",
      "7949\n",
      "7950\n",
      "7951\n",
      "7952\n",
      "7953\n",
      "7954\n",
      "7955\n",
      "7956\n",
      "7957\n",
      "7958\n",
      "7959\n",
      "7960\n",
      "7961\n",
      "7962\n",
      "7963\n",
      "7964\n",
      "7965\n",
      "7966\n",
      "7967\n",
      "7968\n",
      "7969\n",
      "7970\n",
      "7971\n",
      "7972\n",
      "7973\n",
      "7974\n",
      "7975\n",
      "7976\n",
      "7977\n",
      "7978\n",
      "7979\n",
      "7980\n",
      "7981\n",
      "7982\n",
      "7983\n",
      "7984\n",
      "7985\n",
      "7986\n",
      "7987\n",
      "7988\n",
      "7989\n",
      "7990\n",
      "7991\n",
      "7992\n",
      "7993\n",
      "7994\n",
      "7995\n",
      "7996\n",
      "7997\n",
      "7998\n",
      "7999\n",
      "8000\n",
      "8001\n",
      "8002\n",
      "8003\n",
      "8004\n",
      "8005\n",
      "8006\n",
      "8007\n",
      "8008\n",
      "8009\n",
      "8010\n",
      "8011\n",
      "8012\n",
      "8013\n",
      "8014\n",
      "8015\n",
      "8016\n",
      "8017\n",
      "8018\n",
      "8019\n",
      "8020\n",
      "8021\n",
      "8022\n",
      "8023\n",
      "8024\n",
      "8025\n",
      "8026\n",
      "8027\n",
      "8028\n",
      "8029\n",
      "8030\n",
      "8031\n",
      "8032\n",
      "8033\n",
      "8034\n",
      "8035\n",
      "8036\n",
      "8037\n",
      "8038\n",
      "8039\n",
      "8040\n",
      "8041\n",
      "8042\n",
      "8043\n",
      "8044\n",
      "8045\n",
      "8046\n",
      "8047\n",
      "8048\n",
      "8049\n",
      "8050\n",
      "8051\n",
      "8052\n",
      "8053\n",
      "8054\n",
      "8055\n",
      "8056\n",
      "8057\n",
      "8058\n",
      "8059\n",
      "8060\n",
      "8061\n",
      "8062\n",
      "8063\n",
      "8064\n",
      "8065\n",
      "8066\n",
      "8067\n",
      "8068\n",
      "8069\n",
      "8070\n",
      "8071\n",
      "8072\n",
      "8073\n",
      "8074\n",
      "8075\n",
      "8076\n",
      "8077\n",
      "8078\n",
      "8079\n",
      "8080\n",
      "8081\n",
      "8082\n",
      "8083\n",
      "8084\n",
      "8085\n",
      "8086\n",
      "8087\n",
      "8088\n",
      "8089\n",
      "8090\n",
      "8091\n",
      "8092\n",
      "8093\n",
      "8094\n",
      "8095\n",
      "8096\n",
      "8097\n",
      "8098\n",
      "8099\n",
      "8100\n",
      "8101\n",
      "8102\n",
      "8103\n",
      "8104\n",
      "8105\n",
      "8106\n",
      "8107\n",
      "8108\n",
      "8109\n",
      "8110\n",
      "8111\n",
      "8112\n",
      "8113\n",
      "8114\n",
      "8115\n",
      "8116\n",
      "8117\n",
      "8118\n",
      "8119\n",
      "8120\n",
      "8121\n",
      "8122\n",
      "8123\n",
      "8124\n",
      "8125\n",
      "8126\n",
      "8127\n",
      "8128\n",
      "8129\n",
      "8130\n",
      "8131\n",
      "8132\n",
      "8133\n",
      "8134\n",
      "8135\n",
      "8136\n",
      "8137\n",
      "8138\n",
      "8139\n",
      "8140\n",
      "8141\n",
      "8142\n",
      "8143\n",
      "8144\n",
      "8145\n",
      "8146\n",
      "8147\n",
      "8148\n",
      "8149\n",
      "8150\n",
      "8151\n",
      "8152\n",
      "8153\n",
      "8154\n",
      "8155\n",
      "8156\n",
      "8157\n",
      "8158\n",
      "8159\n",
      "8160\n",
      "8161\n",
      "8162\n",
      "8163\n",
      "8164\n",
      "8165\n",
      "8166\n",
      "8167\n",
      "8168\n",
      "8169\n",
      "8170\n",
      "8171\n",
      "8172\n",
      "8173\n",
      "8174\n",
      "8175\n",
      "8176\n",
      "8177\n",
      "8178\n",
      "8179\n",
      "8180\n",
      "8181\n",
      "8182\n",
      "8183\n",
      "8184\n",
      "8185\n",
      "8186\n",
      "8187\n",
      "8188\n",
      "8189\n",
      "8190\n",
      "8191\n",
      "8192\n",
      "8193\n",
      "8194\n",
      "8195\n",
      "8196\n",
      "8197\n",
      "8198\n",
      "8199\n",
      "8200\n",
      "8201\n",
      "8202\n",
      "8203\n",
      "8204\n",
      "8205\n",
      "8206\n",
      "8207\n",
      "8208\n",
      "8209\n",
      "8210\n",
      "8211\n",
      "8212\n",
      "8213\n",
      "8214\n",
      "8215\n",
      "8216\n",
      "8217\n",
      "8218\n",
      "8219\n",
      "8220\n",
      "8221\n",
      "8222\n",
      "8223\n",
      "8224\n",
      "8225\n",
      "8226\n",
      "8227\n",
      "8228\n",
      "8229\n",
      "8230\n",
      "8231\n",
      "8232\n",
      "8233\n",
      "8234\n",
      "8235\n",
      "8236\n",
      "8237\n",
      "8238\n",
      "8239\n",
      "8240\n",
      "8241\n",
      "8242\n",
      "8243\n",
      "8244\n",
      "8245\n",
      "8246\n",
      "8247\n",
      "8248\n",
      "8249\n",
      "8250\n",
      "8251\n",
      "8252\n",
      "8253\n",
      "8254\n",
      "8255\n",
      "8256\n",
      "8257\n",
      "8258\n",
      "8259\n",
      "8260\n",
      "8261\n",
      "8262\n",
      "8263\n",
      "8264\n",
      "8265\n",
      "8266\n",
      "8267\n",
      "8268\n",
      "8269\n",
      "8270\n",
      "8271\n",
      "8272\n",
      "8273\n",
      "8274\n",
      "8275\n",
      "8276\n",
      "8277\n",
      "8278\n",
      "8279\n",
      "8280\n",
      "8281\n",
      "8282\n",
      "8283\n",
      "8284\n",
      "8285\n",
      "8286\n",
      "8287\n",
      "8288\n",
      "8289\n",
      "8290\n",
      "8291\n",
      "8292\n",
      "8293\n",
      "8294\n",
      "8295\n",
      "8296\n",
      "8297\n",
      "8298\n",
      "8299\n",
      "8300\n",
      "8301\n",
      "8302\n",
      "8303\n",
      "8304\n",
      "8305\n",
      "8306\n",
      "8307\n",
      "8308\n",
      "8309\n",
      "8310\n",
      "8311\n",
      "8312\n",
      "8313\n",
      "8314\n",
      "8315\n",
      "8316\n",
      "8317\n",
      "8318\n",
      "8319\n",
      "8320\n",
      "8321\n",
      "8322\n",
      "8323\n",
      "8324\n",
      "8325\n",
      "8326\n",
      "8327\n",
      "8328\n",
      "8329\n",
      "8330\n",
      "8331\n",
      "8332\n",
      "8333\n",
      "8334\n",
      "8335\n",
      "8336\n",
      "8337\n",
      "8338\n",
      "8339\n",
      "8340\n",
      "8341\n",
      "8342\n",
      "8343\n",
      "8344\n",
      "8345\n",
      "8346\n",
      "8347\n",
      "8348\n",
      "8349\n",
      "8350\n",
      "8351\n",
      "8352\n",
      "8353\n",
      "8354\n",
      "8355\n",
      "8356\n",
      "8357\n",
      "8358\n",
      "8359\n",
      "8360\n",
      "8361\n",
      "8362\n",
      "8363\n",
      "8364\n",
      "8365\n",
      "8366\n",
      "8367\n",
      "8368\n",
      "8369\n",
      "8370\n",
      "8371\n",
      "8372\n",
      "8373\n",
      "8374\n",
      "8375\n",
      "8376\n",
      "8377\n",
      "8378\n",
      "8379\n",
      "8380\n",
      "8381\n",
      "8382\n",
      "8383\n",
      "8384\n",
      "8385\n",
      "8386\n",
      "8387\n",
      "8388\n",
      "8389\n",
      "8390\n",
      "8391\n",
      "8392\n",
      "8393\n",
      "8394\n",
      "8395\n",
      "8396\n",
      "8397\n",
      "8398\n",
      "8399\n",
      "8400\n",
      "8401\n",
      "8402\n",
      "8403\n",
      "8404\n",
      "8405\n",
      "8406\n",
      "8407\n",
      "8408\n",
      "8409\n",
      "8410\n",
      "8411\n",
      "8412\n",
      "8413\n",
      "8414\n",
      "8415\n",
      "8416\n",
      "8417\n",
      "8418\n",
      "8419\n",
      "8420\n",
      "8421\n",
      "8422\n",
      "8423\n",
      "8424\n",
      "8425\n",
      "8426\n",
      "8427\n",
      "8428\n",
      "8429\n",
      "8430\n",
      "8431\n",
      "8432\n",
      "8433\n",
      "8434\n",
      "8435\n",
      "8436\n",
      "8437\n",
      "8438\n",
      "8439\n",
      "8440\n",
      "8441\n",
      "8442\n",
      "8443\n",
      "8444\n",
      "8445\n",
      "8446\n",
      "8447\n",
      "8448\n",
      "8449\n",
      "8450\n",
      "8451\n",
      "8452\n",
      "8453\n",
      "8454\n",
      "8455\n",
      "8456\n",
      "8457\n",
      "8458\n",
      "8459\n",
      "8460\n",
      "8461\n",
      "8462\n",
      "8463\n",
      "8464\n",
      "8465\n",
      "8466\n",
      "8467\n",
      "8468\n",
      "8469\n",
      "8470\n",
      "8471\n",
      "8472\n",
      "8473\n",
      "8474\n",
      "8475\n",
      "8476\n",
      "8477\n",
      "8478\n",
      "8479\n",
      "8480\n",
      "8481\n",
      "8482\n",
      "8483\n",
      "8484\n",
      "8485\n",
      "8486\n",
      "8487\n",
      "8488\n",
      "8489\n",
      "8490\n",
      "8491\n",
      "8492\n",
      "8493\n",
      "8494\n",
      "8495\n",
      "8496\n",
      "8497\n",
      "8498\n",
      "8499\n",
      "8500\n",
      "8501\n",
      "8502\n",
      "8503\n",
      "8504\n",
      "8505\n",
      "8506\n",
      "8507\n",
      "8508\n",
      "8509\n",
      "8510\n",
      "8511\n",
      "8512\n",
      "8513\n",
      "8514\n",
      "8515\n",
      "8516\n",
      "8517\n",
      "8518\n",
      "8519\n",
      "8520\n",
      "8521\n",
      "8522\n",
      "8523\n",
      "8524\n",
      "8525\n",
      "8526\n",
      "8527\n",
      "8528\n",
      "8529\n",
      "8530\n",
      "8531\n",
      "8532\n",
      "8533\n",
      "8534\n",
      "8535\n",
      "8536\n",
      "8537\n",
      "8538\n",
      "8539\n",
      "8540\n",
      "8541\n",
      "8542\n",
      "8543\n",
      "8544\n",
      "8545\n",
      "8546\n",
      "8547\n",
      "8548\n",
      "8549\n",
      "8550\n",
      "8551\n",
      "8552\n",
      "8553\n",
      "8554\n",
      "8555\n",
      "8556\n",
      "8557\n",
      "8558\n",
      "8559\n",
      "8560\n",
      "8561\n",
      "8562\n",
      "8563\n",
      "8564\n",
      "8565\n",
      "8566\n",
      "8567\n",
      "8568\n",
      "8569\n",
      "8570\n",
      "8571\n",
      "8572\n",
      "8573\n",
      "8574\n",
      "8575\n",
      "8576\n",
      "8577\n",
      "8578\n",
      "8579\n",
      "8580\n",
      "8581\n",
      "8582\n",
      "8583\n",
      "8584\n",
      "8585\n",
      "8586\n",
      "8587\n",
      "8588\n",
      "8589\n",
      "8590\n",
      "8591\n",
      "8592\n",
      "8593\n",
      "8594\n",
      "8595\n",
      "8596\n",
      "8597\n",
      "8598\n",
      "8599\n",
      "8600\n",
      "8601\n",
      "8602\n",
      "8603\n",
      "8604\n",
      "8605\n",
      "8606\n",
      "8607\n",
      "8608\n",
      "8609\n",
      "8610\n",
      "8611\n",
      "8612\n",
      "8613\n",
      "8614\n",
      "8615\n",
      "8616\n",
      "8617\n",
      "8618\n",
      "8619\n",
      "8620\n",
      "8621\n",
      "8622\n",
      "8623\n",
      "8624\n",
      "8625\n",
      "8626\n",
      "8627\n",
      "8628\n",
      "8629\n",
      "8630\n",
      "8631\n",
      "8632\n",
      "8633\n",
      "8634\n",
      "8635\n",
      "8636\n",
      "8637\n",
      "8638\n",
      "8639\n",
      "8640\n",
      "8641\n",
      "8642\n",
      "8643\n",
      "8644\n",
      "8645\n",
      "8646\n",
      "8647\n",
      "8648\n",
      "8649\n",
      "8650\n",
      "8651\n",
      "8652\n",
      "8653\n",
      "8654\n",
      "8655\n",
      "8656\n",
      "8657\n",
      "8658\n",
      "8659\n",
      "8660\n",
      "8661\n",
      "8662\n",
      "8663\n",
      "8664\n",
      "8665\n",
      "8666\n",
      "8667\n",
      "8668\n",
      "8669\n",
      "8670\n",
      "8671\n",
      "8672\n",
      "8673\n",
      "8674\n",
      "8675\n",
      "8676\n",
      "8677\n",
      "8678\n",
      "8679\n",
      "8680\n",
      "8681\n",
      "8682\n",
      "8683\n",
      "8684\n",
      "8685\n",
      "8686\n",
      "8687\n",
      "8688\n",
      "8689\n",
      "8690\n",
      "8691\n",
      "8692\n",
      "8693\n",
      "8694\n",
      "8695\n",
      "8696\n",
      "8697\n",
      "8698\n",
      "8699\n",
      "8700\n",
      "8701\n",
      "8702\n",
      "8703\n",
      "8704\n",
      "8705\n",
      "8706\n",
      "8707\n",
      "8708\n",
      "8709\n",
      "8710\n",
      "8711\n",
      "8712\n",
      "8713\n",
      "8714\n",
      "8715\n",
      "8716\n",
      "8717\n",
      "8718\n",
      "8719\n",
      "8720\n",
      "8721\n",
      "8722\n",
      "8723\n",
      "8724\n",
      "8725\n",
      "8726\n",
      "8727\n",
      "8728\n",
      "8729\n",
      "8730\n",
      "8731\n",
      "8732\n",
      "8733\n",
      "8734\n",
      "8735\n",
      "8736\n",
      "8737\n",
      "8738\n",
      "8739\n",
      "8740\n",
      "8741\n",
      "8742\n",
      "8743\n",
      "8744\n",
      "8745\n",
      "8746\n",
      "8747\n",
      "8748\n",
      "8749\n",
      "8750\n",
      "8751\n",
      "8752\n",
      "8753\n",
      "8754\n",
      "8755\n",
      "8756\n",
      "8757\n",
      "8758\n",
      "8759\n",
      "8760\n",
      "8761\n",
      "8762\n",
      "8763\n",
      "8764\n",
      "8765\n",
      "8766\n",
      "8767\n",
      "8768\n",
      "8769\n",
      "8770\n",
      "8771\n",
      "8772\n",
      "8773\n",
      "8774\n",
      "8775\n",
      "8776\n",
      "8777\n",
      "8778\n",
      "8779\n",
      "8780\n",
      "8781\n",
      "8782\n",
      "8783\n",
      "8784\n",
      "8785\n",
      "8786\n",
      "8787\n",
      "8788\n",
      "8789\n",
      "8790\n",
      "8791\n",
      "8792\n",
      "8793\n",
      "8794\n",
      "8795\n",
      "8796\n",
      "8797\n",
      "8798\n",
      "8799\n",
      "8800\n",
      "8801\n",
      "8802\n",
      "8803\n",
      "8804\n",
      "8805\n",
      "8806\n",
      "8807\n",
      "8808\n",
      "8809\n",
      "8810\n",
      "8811\n",
      "8812\n",
      "8813\n",
      "8814\n",
      "8815\n",
      "8816\n",
      "8817\n",
      "8818\n",
      "8819\n",
      "8820\n",
      "8821\n",
      "8822\n",
      "8823\n",
      "8824\n",
      "8825\n",
      "8826\n",
      "8827\n",
      "8828\n",
      "8829\n",
      "8830\n",
      "8831\n",
      "8832\n",
      "8833\n",
      "8834\n",
      "8835\n",
      "8836\n",
      "8837\n",
      "8838\n",
      "8839\n",
      "8840\n",
      "8841\n",
      "8842\n",
      "8843\n",
      "8844\n",
      "8845\n",
      "8846\n",
      "8847\n",
      "8848\n",
      "8849\n",
      "8850\n",
      "8851\n",
      "8852\n",
      "8853\n",
      "8854\n",
      "8855\n",
      "8856\n",
      "8857\n",
      "8858\n",
      "8859\n",
      "8860\n",
      "8861\n",
      "8862\n",
      "8863\n",
      "8864\n",
      "8865\n",
      "8866\n",
      "8867\n",
      "8868\n",
      "8869\n",
      "8870\n",
      "8871\n",
      "8872\n",
      "8873\n",
      "8874\n",
      "8875\n",
      "8876\n",
      "8877\n",
      "8878\n",
      "8879\n",
      "8880\n",
      "8881\n",
      "8882\n",
      "8883\n",
      "8884\n",
      "8885\n",
      "8886\n",
      "8887\n",
      "8888\n",
      "8889\n",
      "8890\n",
      "8891\n",
      "8892\n",
      "8893\n",
      "8894\n",
      "8895\n",
      "8896\n",
      "8897\n",
      "8898\n",
      "8899\n",
      "8900\n",
      "8901\n",
      "8902\n",
      "8903\n",
      "8904\n",
      "8905\n",
      "8906\n",
      "8907\n",
      "8908\n",
      "8909\n",
      "8910\n",
      "8911\n",
      "8912\n",
      "8913\n",
      "8914\n",
      "8915\n",
      "8916\n",
      "8917\n",
      "8918\n",
      "8919\n",
      "8920\n",
      "8921\n",
      "8922\n",
      "8923\n",
      "8924\n",
      "8925\n",
      "8926\n",
      "8927\n",
      "8928\n",
      "8929\n",
      "8930\n",
      "8931\n",
      "8932\n",
      "8933\n",
      "8934\n",
      "8935\n",
      "8936\n",
      "8937\n",
      "8938\n",
      "8939\n",
      "8940\n",
      "8941\n",
      "8942\n",
      "8943\n",
      "8944\n",
      "8945\n",
      "8946\n",
      "8947\n",
      "8948\n",
      "8949\n",
      "8950\n",
      "8951\n",
      "8952\n",
      "8953\n",
      "8954\n",
      "8955\n",
      "8956\n",
      "8957\n",
      "8958\n",
      "8959\n",
      "8960\n",
      "8961\n",
      "8962\n",
      "8963\n",
      "8964\n",
      "8965\n",
      "8966\n",
      "8967\n",
      "8968\n",
      "8969\n",
      "8970\n",
      "8971\n",
      "8972\n",
      "8973\n",
      "8974\n",
      "8975\n",
      "8976\n",
      "8977\n",
      "8978\n",
      "8979\n",
      "8980\n",
      "8981\n",
      "8982\n",
      "8983\n",
      "8984\n",
      "8985\n",
      "8986\n",
      "8987\n",
      "8988\n",
      "8989\n",
      "8990\n",
      "8991\n",
      "8992\n",
      "8993\n",
      "8994\n",
      "8995\n",
      "8996\n",
      "8997\n",
      "8998\n",
      "8999\n",
      "9000\n",
      "9001\n",
      "9002\n",
      "9003\n",
      "9004\n",
      "9005\n",
      "9006\n",
      "9007\n",
      "9008\n",
      "9009\n",
      "9010\n",
      "9011\n",
      "9012\n",
      "9013\n",
      "9014\n",
      "9015\n",
      "9016\n",
      "9017\n",
      "9018\n",
      "9019\n",
      "9020\n",
      "9021\n",
      "9022\n",
      "9023\n",
      "9024\n",
      "9025\n",
      "9026\n",
      "9027\n",
      "9028\n",
      "9029\n",
      "9030\n",
      "9031\n",
      "9032\n",
      "9033\n",
      "9034\n",
      "9035\n",
      "9036\n",
      "9037\n",
      "9038\n",
      "9039\n",
      "9040\n",
      "9041\n",
      "9042\n",
      "9043\n",
      "9044\n",
      "9045\n",
      "9046\n",
      "9047\n",
      "9048\n",
      "9049\n",
      "9050\n",
      "9051\n",
      "9052\n",
      "9053\n",
      "9054\n",
      "9055\n",
      "9056\n",
      "9057\n",
      "9058\n",
      "9059\n",
      "9060\n",
      "9061\n",
      "9062\n",
      "9063\n",
      "9064\n",
      "9065\n",
      "9066\n",
      "9067\n"
     ]
    }
   ],
   "source": [
    "dir_path = \"/content/gdrive/MyDrive/GDS/walks/noextrawalks\"\n",
    "\n",
    "import random\n",
    "\n",
    "num_neighbors =  15 # num_walks * (walk_length - 1)\n",
    "\n",
    "num_walks_primary = 40\n",
    "walk_length = 10 # 3\n",
    "\n",
    "min_neighbors_tolerance = 10\n",
    "max_num_secondary_rws = 8\n",
    "\n",
    "\n",
    "walk_length_list = [5, 10]\n",
    "walk_params_list = [[2, 2, 2, 2],\n",
    "                [5, 10, 1, 1],\n",
    "               [5, 10, 5, 10],\n",
    "               [10, 10, 1 ,1],\n",
    "               [1, 1, 1, 1]]\n",
    "\n",
    "for walk_length in walk_length_list:\n",
    "\n",
    "  for walk_params in walk_params_list:\n",
    "\n",
    "    name_extension = \"wl_\" + str(walk_length)\n",
    "    for x in walk_params:\n",
    "      name_extension += \"_\" + str(x)\n",
    "\n",
    "    print(name_extension)\n",
    "\n",
    "    random.seed(564)\n",
    "    np.random.seed(456)\n",
    "\n",
    "    pyg_graph, neigh_matrix, negative_array, node_list  = get_pyg_object(\"Graph_er_weighted_wfeatures\", \"Graph_er\", \"/content/gdrive/MyDrive/GDS/pickles/\", \n",
    "                                                                         walk_params[0], walk_params[1], walk_params[2], walk_params[3],\n",
    "                                                                          num_neighbors, walk_length, num_walks_primary, min_neighbors_tolerance, \n",
    "                                                                         max_num_secondary_rws , pages_to_remove)\n",
    "\n",
    "\n",
    "    dump_pickle_file(pyg_graph, \"pyg_graph_\" + name_extension, dir_path)\n",
    "    dump_pickle_file(neigh_matrix, \"neigh_matrix_\" + name_extension, dir_path)\n",
    "    dump_pickle_file(negative_array, \"negative_array_\" + name_extension, dir_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0GWAMy5ZTl2a"
   },
   "outputs": [],
   "source": [
    "G3 = load_pickle_file(\"Graph_er_weighted_wfeatures\", dir_path=\"/content/gdrive/MyDrive/GDS/pickles/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9QYAkbq9T-6K",
    "outputId": "c5b565be-7d8a-423f-b3c8-02ad69e42665"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10631"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yEzuMTMYTsEx",
    "outputId": "e12d5ec0-980b-4d22-8724-871c26f0672b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1555,)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G3.nodes[\"/\"]['node_feature'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "APRtlh75i1Tn",
    "outputId": "591ba0c8-374c-47e9-b7c9-095dd496c887"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wl_5_2_2_2_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:168: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "3861\n",
      "3862\n",
      "3863\n",
      "3864\n",
      "3865\n",
      "3866\n",
      "3867\n",
      "3868\n",
      "3869\n",
      "3870\n",
      "3871\n",
      "3872\n",
      "3873\n",
      "3874\n",
      "3875\n",
      "3876\n",
      "3877\n",
      "3878\n",
      "3879\n",
      "3880\n",
      "3881\n",
      "3882\n",
      "3883\n",
      "3884\n",
      "3885\n",
      "3886\n",
      "3887\n",
      "3888\n",
      "3889\n",
      "3890\n",
      "3891\n",
      "3892\n",
      "3893\n",
      "3894\n",
      "3895\n",
      "3896\n",
      "3897\n",
      "3898\n",
      "3899\n",
      "3900\n",
      "3901\n",
      "3902\n",
      "3903\n",
      "3904\n",
      "3905\n",
      "3906\n",
      "3907\n",
      "3908\n",
      "3909\n",
      "3910\n",
      "3911\n",
      "3912\n",
      "3913\n",
      "3914\n",
      "3915\n",
      "3916\n",
      "3917\n",
      "3918\n",
      "3919\n",
      "3920\n",
      "3921\n",
      "3922\n",
      "3923\n",
      "3924\n",
      "3925\n",
      "3926\n",
      "3927\n",
      "3928\n",
      "3929\n",
      "3930\n",
      "3931\n",
      "3932\n",
      "3933\n",
      "3934\n",
      "3935\n",
      "3936\n",
      "3937\n",
      "3938\n",
      "3939\n",
      "3940\n",
      "3941\n",
      "3942\n",
      "3943\n",
      "3944\n",
      "3945\n",
      "3946\n",
      "3947\n",
      "3948\n",
      "3949\n",
      "3950\n",
      "3951\n",
      "3952\n",
      "3953\n",
      "3954\n",
      "3955\n",
      "3956\n",
      "3957\n",
      "3958\n",
      "3959\n",
      "3960\n",
      "3961\n",
      "3962\n",
      "3963\n",
      "3964\n",
      "3965\n",
      "3966\n",
      "3967\n",
      "3968\n",
      "3969\n",
      "3970\n",
      "3971\n",
      "3972\n",
      "3973\n",
      "3974\n",
      "3975\n",
      "3976\n",
      "3977\n",
      "3978\n",
      "3979\n",
      "3980\n",
      "3981\n",
      "3982\n",
      "3983\n",
      "3984\n",
      "3985\n",
      "3986\n",
      "3987\n",
      "3988\n",
      "3989\n",
      "3990\n",
      "3991\n",
      "3992\n",
      "3993\n",
      "3994\n",
      "3995\n",
      "3996\n",
      "3997\n",
      "3998\n",
      "3999\n",
      "4000\n",
      "4001\n",
      "4002\n",
      "4003\n",
      "4004\n",
      "4005\n",
      "4006\n",
      "4007\n",
      "4008\n",
      "4009\n",
      "4010\n",
      "4011\n",
      "4012\n",
      "4013\n",
      "4014\n",
      "4015\n",
      "4016\n",
      "4017\n",
      "4018\n",
      "4019\n",
      "4020\n",
      "4021\n",
      "4022\n",
      "4023\n",
      "4024\n",
      "4025\n",
      "4026\n",
      "4027\n",
      "4028\n",
      "4029\n",
      "4030\n",
      "4031\n",
      "4032\n",
      "4033\n",
      "4034\n",
      "4035\n",
      "4036\n",
      "4037\n",
      "4038\n",
      "4039\n",
      "4040\n",
      "4041\n",
      "4042\n",
      "4043\n",
      "4044\n",
      "4045\n",
      "4046\n",
      "4047\n",
      "4048\n",
      "4049\n",
      "4050\n",
      "4051\n",
      "4052\n",
      "4053\n",
      "4054\n",
      "4055\n",
      "4056\n",
      "4057\n",
      "4058\n",
      "4059\n",
      "4060\n",
      "4061\n",
      "4062\n",
      "4063\n",
      "4064\n",
      "4065\n",
      "4066\n",
      "4067\n",
      "4068\n",
      "4069\n",
      "4070\n",
      "4071\n",
      "4072\n",
      "4073\n",
      "4074\n",
      "4075\n",
      "4076\n",
      "4077\n",
      "4078\n",
      "4079\n",
      "4080\n",
      "4081\n",
      "4082\n",
      "4083\n",
      "4084\n",
      "4085\n",
      "4086\n",
      "4087\n",
      "4088\n",
      "4089\n",
      "4090\n",
      "4091\n",
      "4092\n",
      "4093\n",
      "4094\n",
      "4095\n",
      "4096\n",
      "4097\n",
      "4098\n",
      "4099\n",
      "4100\n",
      "4101\n",
      "4102\n",
      "4103\n",
      "4104\n",
      "4105\n",
      "4106\n",
      "4107\n",
      "4108\n",
      "4109\n",
      "4110\n",
      "4111\n",
      "4112\n",
      "4113\n",
      "4114\n",
      "4115\n",
      "4116\n",
      "4117\n",
      "4118\n",
      "4119\n",
      "4120\n",
      "4121\n",
      "4122\n",
      "4123\n",
      "4124\n",
      "4125\n",
      "4126\n",
      "4127\n",
      "4128\n",
      "4129\n",
      "4130\n",
      "4131\n",
      "4132\n",
      "4133\n",
      "4134\n",
      "4135\n",
      "4136\n",
      "4137\n",
      "4138\n",
      "4139\n",
      "4140\n",
      "4141\n",
      "4142\n",
      "4143\n",
      "4144\n",
      "4145\n",
      "4146\n",
      "4147\n",
      "4148\n",
      "4149\n",
      "4150\n",
      "4151\n",
      "4152\n",
      "4153\n",
      "4154\n",
      "4155\n",
      "4156\n",
      "4157\n",
      "4158\n",
      "4159\n",
      "4160\n",
      "4161\n",
      "4162\n",
      "4163\n",
      "4164\n",
      "4165\n",
      "4166\n",
      "4167\n",
      "4168\n",
      "4169\n",
      "4170\n",
      "4171\n",
      "4172\n",
      "4173\n",
      "4174\n",
      "4175\n",
      "4176\n",
      "4177\n",
      "4178\n",
      "4179\n",
      "4180\n",
      "4181\n",
      "4182\n",
      "4183\n",
      "4184\n",
      "4185\n",
      "4186\n",
      "4187\n",
      "4188\n",
      "4189\n",
      "4190\n",
      "4191\n",
      "4192\n",
      "4193\n",
      "4194\n",
      "4195\n",
      "4196\n",
      "4197\n",
      "4198\n",
      "4199\n",
      "4200\n",
      "4201\n",
      "4202\n",
      "4203\n",
      "4204\n",
      "4205\n",
      "4206\n",
      "4207\n",
      "4208\n",
      "4209\n",
      "4210\n",
      "4211\n",
      "4212\n",
      "4213\n",
      "4214\n",
      "4215\n",
      "4216\n",
      "4217\n",
      "4218\n",
      "4219\n",
      "4220\n",
      "4221\n",
      "4222\n",
      "4223\n",
      "4224\n",
      "4225\n",
      "4226\n",
      "4227\n",
      "4228\n",
      "4229\n",
      "4230\n",
      "4231\n",
      "4232\n",
      "4233\n",
      "4234\n",
      "4235\n",
      "4236\n",
      "4237\n",
      "4238\n",
      "4239\n",
      "4240\n",
      "4241\n",
      "4242\n",
      "4243\n",
      "4244\n",
      "4245\n",
      "4246\n",
      "4247\n",
      "4248\n",
      "4249\n",
      "4250\n",
      "4251\n",
      "4252\n",
      "4253\n",
      "4254\n",
      "4255\n",
      "4256\n",
      "4257\n",
      "4258\n",
      "4259\n",
      "4260\n",
      "4261\n",
      "4262\n",
      "4263\n",
      "4264\n",
      "4265\n",
      "4266\n",
      "4267\n",
      "4268\n",
      "4269\n",
      "4270\n",
      "4271\n",
      "4272\n",
      "4273\n",
      "4274\n",
      "4275\n",
      "4276\n",
      "4277\n",
      "4278\n",
      "4279\n",
      "4280\n",
      "4281\n",
      "4282\n",
      "4283\n",
      "4284\n",
      "4285\n",
      "4286\n",
      "4287\n",
      "4288\n",
      "4289\n",
      "4290\n",
      "4291\n",
      "4292\n",
      "4293\n",
      "4294\n",
      "4295\n",
      "4296\n",
      "4297\n",
      "4298\n",
      "4299\n",
      "4300\n",
      "4301\n",
      "4302\n",
      "4303\n",
      "4304\n",
      "4305\n",
      "4306\n",
      "4307\n",
      "4308\n",
      "4309\n",
      "4310\n",
      "4311\n",
      "4312\n",
      "4313\n",
      "4314\n",
      "4315\n",
      "4316\n",
      "4317\n",
      "4318\n",
      "4319\n",
      "4320\n",
      "4321\n",
      "4322\n",
      "4323\n",
      "4324\n",
      "4325\n",
      "4326\n",
      "4327\n",
      "4328\n",
      "4329\n",
      "4330\n",
      "4331\n",
      "4332\n",
      "4333\n",
      "4334\n",
      "4335\n",
      "4336\n",
      "4337\n",
      "4338\n",
      "4339\n",
      "4340\n",
      "4341\n",
      "4342\n",
      "4343\n",
      "4344\n",
      "4345\n",
      "4346\n",
      "4347\n",
      "4348\n",
      "4349\n",
      "4350\n",
      "4351\n",
      "4352\n",
      "4353\n",
      "4354\n",
      "4355\n",
      "4356\n",
      "4357\n",
      "4358\n",
      "4359\n",
      "4360\n",
      "4361\n",
      "4362\n",
      "4363\n",
      "4364\n",
      "4365\n",
      "4366\n",
      "4367\n",
      "4368\n",
      "4369\n",
      "4370\n",
      "4371\n",
      "4372\n",
      "4373\n",
      "4374\n",
      "4375\n",
      "4376\n",
      "4377\n",
      "4378\n",
      "4379\n",
      "4380\n",
      "4381\n",
      "4382\n",
      "4383\n",
      "4384\n",
      "4385\n",
      "4386\n",
      "4387\n",
      "4388\n",
      "4389\n",
      "4390\n",
      "4391\n",
      "4392\n",
      "4393\n",
      "4394\n",
      "4395\n",
      "4396\n",
      "4397\n",
      "4398\n",
      "4399\n",
      "4400\n",
      "4401\n",
      "4402\n",
      "4403\n",
      "4404\n",
      "4405\n",
      "4406\n",
      "4407\n",
      "4408\n",
      "4409\n",
      "4410\n",
      "4411\n",
      "4412\n",
      "4413\n",
      "4414\n",
      "4415\n",
      "4416\n",
      "4417\n",
      "4418\n",
      "4419\n",
      "4420\n",
      "4421\n",
      "4422\n",
      "4423\n",
      "4424\n",
      "4425\n",
      "4426\n",
      "4427\n",
      "4428\n",
      "4429\n",
      "4430\n",
      "4431\n",
      "4432\n",
      "4433\n",
      "4434\n",
      "4435\n",
      "4436\n",
      "4437\n",
      "4438\n",
      "4439\n",
      "4440\n",
      "4441\n",
      "4442\n",
      "4443\n",
      "4444\n",
      "4445\n",
      "4446\n",
      "4447\n",
      "4448\n",
      "4449\n",
      "4450\n",
      "4451\n",
      "4452\n",
      "4453\n",
      "4454\n",
      "4455\n",
      "4456\n",
      "4457\n",
      "4458\n",
      "4459\n",
      "4460\n",
      "4461\n",
      "4462\n",
      "4463\n",
      "4464\n",
      "4465\n",
      "4466\n",
      "4467\n",
      "4468\n",
      "4469\n",
      "4470\n",
      "4471\n",
      "4472\n",
      "4473\n",
      "4474\n",
      "4475\n",
      "4476\n",
      "4477\n",
      "4478\n",
      "4479\n",
      "4480\n",
      "4481\n",
      "4482\n",
      "4483\n",
      "4484\n",
      "4485\n",
      "4486\n",
      "4487\n",
      "4488\n",
      "4489\n",
      "4490\n",
      "4491\n",
      "4492\n",
      "4493\n",
      "4494\n",
      "4495\n",
      "4496\n",
      "4497\n",
      "4498\n",
      "4499\n",
      "4500\n",
      "4501\n",
      "4502\n",
      "4503\n",
      "4504\n",
      "4505\n",
      "4506\n",
      "4507\n",
      "4508\n",
      "4509\n",
      "4510\n",
      "4511\n",
      "4512\n",
      "4513\n",
      "4514\n",
      "4515\n",
      "4516\n",
      "4517\n",
      "4518\n",
      "4519\n",
      "4520\n",
      "4521\n",
      "4522\n",
      "4523\n",
      "4524\n",
      "4525\n",
      "4526\n",
      "4527\n",
      "4528\n",
      "4529\n",
      "4530\n",
      "4531\n",
      "4532\n",
      "4533\n",
      "4534\n",
      "4535\n",
      "4536\n",
      "4537\n",
      "4538\n",
      "4539\n",
      "4540\n",
      "4541\n",
      "4542\n",
      "4543\n",
      "4544\n",
      "4545\n",
      "4546\n",
      "4547\n",
      "4548\n",
      "4549\n",
      "4550\n",
      "4551\n",
      "4552\n",
      "4553\n",
      "4554\n",
      "4555\n",
      "4556\n",
      "4557\n",
      "4558\n",
      "4559\n",
      "4560\n",
      "4561\n",
      "4562\n",
      "4563\n",
      "4564\n",
      "4565\n",
      "4566\n",
      "4567\n",
      "4568\n",
      "4569\n",
      "4570\n",
      "4571\n",
      "4572\n",
      "4573\n",
      "4574\n",
      "4575\n",
      "4576\n",
      "4577\n",
      "4578\n",
      "4579\n",
      "4580\n",
      "4581\n",
      "4582\n",
      "4583\n",
      "4584\n",
      "4585\n",
      "4586\n",
      "4587\n",
      "4588\n",
      "4589\n",
      "4590\n",
      "4591\n",
      "4592\n",
      "4593\n",
      "4594\n",
      "4595\n",
      "4596\n",
      "4597\n",
      "4598\n",
      "4599\n",
      "4600\n",
      "4601\n",
      "4602\n",
      "4603\n",
      "4604\n",
      "4605\n",
      "4606\n",
      "4607\n",
      "4608\n",
      "4609\n",
      "4610\n",
      "4611\n",
      "4612\n",
      "4613\n",
      "4614\n",
      "4615\n",
      "4616\n",
      "4617\n",
      "4618\n",
      "4619\n",
      "4620\n",
      "4621\n",
      "4622\n",
      "4623\n",
      "4624\n",
      "4625\n",
      "4626\n",
      "4627\n",
      "4628\n",
      "4629\n",
      "4630\n",
      "4631\n",
      "4632\n",
      "4633\n",
      "4634\n",
      "4635\n",
      "4636\n",
      "4637\n",
      "4638\n",
      "4639\n",
      "4640\n",
      "4641\n",
      "4642\n",
      "4643\n",
      "4644\n",
      "4645\n",
      "4646\n",
      "4647\n",
      "4648\n",
      "4649\n",
      "4650\n",
      "4651\n",
      "4652\n",
      "4653\n",
      "4654\n",
      "4655\n",
      "4656\n",
      "4657\n",
      "4658\n",
      "4659\n",
      "4660\n",
      "4661\n",
      "4662\n",
      "4663\n",
      "4664\n",
      "4665\n",
      "4666\n",
      "4667\n",
      "4668\n",
      "4669\n",
      "4670\n",
      "4671\n",
      "4672\n",
      "4673\n",
      "4674\n",
      "4675\n",
      "4676\n",
      "4677\n",
      "4678\n",
      "4679\n",
      "4680\n",
      "4681\n",
      "4682\n",
      "4683\n",
      "4684\n",
      "4685\n",
      "4686\n",
      "4687\n",
      "4688\n",
      "4689\n",
      "4690\n",
      "4691\n",
      "4692\n",
      "4693\n",
      "4694\n",
      "4695\n",
      "4696\n",
      "4697\n",
      "4698\n",
      "4699\n",
      "4700\n",
      "4701\n",
      "4702\n",
      "4703\n",
      "4704\n",
      "4705\n",
      "4706\n",
      "4707\n",
      "4708\n",
      "4709\n",
      "4710\n",
      "4711\n",
      "4712\n",
      "4713\n",
      "4714\n",
      "4715\n",
      "4716\n",
      "4717\n",
      "4718\n",
      "4719\n",
      "4720\n",
      "4721\n",
      "4722\n",
      "4723\n",
      "4724\n",
      "4725\n",
      "4726\n",
      "4727\n",
      "4728\n",
      "4729\n",
      "4730\n",
      "4731\n",
      "4732\n",
      "4733\n",
      "4734\n",
      "4735\n",
      "4736\n",
      "4737\n",
      "4738\n",
      "4739\n",
      "4740\n",
      "4741\n",
      "4742\n",
      "4743\n",
      "4744\n",
      "4745\n",
      "4746\n",
      "4747\n",
      "4748\n",
      "4749\n",
      "4750\n",
      "4751\n",
      "4752\n",
      "4753\n",
      "4754\n",
      "4755\n",
      "4756\n",
      "4757\n",
      "4758\n",
      "4759\n",
      "4760\n",
      "4761\n",
      "4762\n",
      "4763\n",
      "4764\n",
      "4765\n",
      "4766\n",
      "4767\n",
      "4768\n",
      "4769\n",
      "4770\n",
      "4771\n",
      "4772\n",
      "4773\n",
      "4774\n",
      "4775\n",
      "4776\n",
      "4777\n",
      "4778\n",
      "4779\n",
      "4780\n",
      "4781\n",
      "4782\n",
      "4783\n",
      "4784\n",
      "4785\n",
      "4786\n",
      "4787\n",
      "4788\n",
      "4789\n",
      "4790\n",
      "4791\n",
      "4792\n",
      "4793\n",
      "4794\n",
      "4795\n",
      "4796\n",
      "4797\n",
      "4798\n",
      "4799\n",
      "4800\n",
      "4801\n",
      "4802\n",
      "4803\n",
      "4804\n",
      "4805\n",
      "4806\n",
      "4807\n",
      "4808\n",
      "4809\n",
      "4810\n",
      "4811\n",
      "4812\n",
      "4813\n",
      "4814\n",
      "4815\n",
      "4816\n",
      "4817\n",
      "4818\n",
      "4819\n",
      "4820\n",
      "4821\n",
      "4822\n",
      "4823\n",
      "4824\n",
      "4825\n",
      "4826\n",
      "4827\n",
      "4828\n",
      "4829\n",
      "4830\n",
      "4831\n",
      "4832\n",
      "4833\n",
      "4834\n",
      "4835\n",
      "4836\n",
      "4837\n",
      "4838\n",
      "4839\n",
      "4840\n",
      "4841\n",
      "4842\n",
      "4843\n",
      "4844\n",
      "4845\n",
      "4846\n",
      "4847\n",
      "4848\n",
      "4849\n",
      "4850\n",
      "4851\n",
      "4852\n",
      "4853\n",
      "4854\n",
      "4855\n",
      "4856\n",
      "4857\n",
      "4858\n",
      "4859\n",
      "4860\n",
      "4861\n",
      "4862\n",
      "4863\n",
      "4864\n",
      "4865\n",
      "4866\n",
      "4867\n",
      "4868\n",
      "4869\n",
      "4870\n",
      "4871\n",
      "4872\n",
      "4873\n",
      "4874\n",
      "4875\n",
      "4876\n",
      "4877\n",
      "4878\n",
      "4879\n",
      "4880\n",
      "4881\n",
      "4882\n",
      "4883\n",
      "4884\n",
      "4885\n",
      "4886\n",
      "4887\n",
      "4888\n",
      "4889\n",
      "4890\n",
      "4891\n",
      "4892\n",
      "4893\n",
      "4894\n",
      "4895\n",
      "4896\n",
      "4897\n",
      "4898\n",
      "4899\n",
      "4900\n",
      "4901\n",
      "4902\n",
      "4903\n",
      "4904\n",
      "4905\n",
      "4906\n",
      "4907\n",
      "4908\n",
      "4909\n",
      "4910\n",
      "4911\n",
      "4912\n",
      "4913\n",
      "4914\n",
      "4915\n",
      "4916\n",
      "4917\n",
      "4918\n",
      "4919\n",
      "4920\n",
      "4921\n",
      "4922\n",
      "4923\n",
      "4924\n",
      "4925\n",
      "4926\n",
      "4927\n",
      "4928\n",
      "4929\n",
      "4930\n",
      "4931\n",
      "4932\n",
      "4933\n",
      "4934\n",
      "4935\n",
      "4936\n",
      "4937\n",
      "4938\n",
      "4939\n",
      "4940\n",
      "4941\n",
      "4942\n",
      "4943\n",
      "4944\n",
      "4945\n",
      "4946\n",
      "4947\n",
      "4948\n",
      "4949\n",
      "4950\n",
      "4951\n",
      "4952\n",
      "4953\n",
      "4954\n",
      "4955\n",
      "4956\n",
      "4957\n",
      "4958\n",
      "4959\n",
      "4960\n",
      "4961\n",
      "4962\n",
      "4963\n",
      "4964\n",
      "4965\n",
      "4966\n",
      "4967\n",
      "4968\n",
      "4969\n",
      "4970\n",
      "4971\n",
      "4972\n",
      "4973\n",
      "4974\n",
      "4975\n",
      "4976\n",
      "4977\n",
      "4978\n",
      "4979\n",
      "4980\n",
      "4981\n",
      "4982\n",
      "4983\n",
      "4984\n",
      "4985\n",
      "4986\n",
      "4987\n",
      "4988\n",
      "4989\n",
      "4990\n",
      "4991\n",
      "4992\n",
      "4993\n",
      "4994\n",
      "4995\n",
      "4996\n",
      "4997\n",
      "4998\n",
      "4999\n",
      "5000\n",
      "5001\n",
      "5002\n",
      "5003\n",
      "5004\n",
      "5005\n",
      "5006\n",
      "5007\n",
      "5008\n",
      "5009\n",
      "5010\n",
      "5011\n",
      "5012\n",
      "5013\n",
      "5014\n",
      "5015\n",
      "5016\n",
      "5017\n",
      "5018\n",
      "5019\n",
      "5020\n",
      "5021\n",
      "5022\n",
      "5023\n",
      "5024\n",
      "5025\n",
      "5026\n",
      "5027\n",
      "5028\n",
      "5029\n",
      "5030\n",
      "5031\n",
      "5032\n",
      "5033\n",
      "5034\n",
      "5035\n",
      "5036\n",
      "5037\n",
      "5038\n",
      "5039\n",
      "5040\n",
      "5041\n",
      "5042\n",
      "5043\n",
      "5044\n",
      "5045\n",
      "5046\n",
      "5047\n",
      "5048\n",
      "5049\n",
      "5050\n",
      "5051\n",
      "5052\n",
      "5053\n",
      "5054\n",
      "5055\n",
      "5056\n",
      "5057\n",
      "5058\n",
      "5059\n",
      "5060\n",
      "5061\n",
      "5062\n",
      "5063\n",
      "5064\n",
      "5065\n",
      "5066\n",
      "5067\n",
      "5068\n",
      "5069\n",
      "5070\n",
      "5071\n",
      "5072\n",
      "5073\n",
      "5074\n",
      "5075\n",
      "5076\n",
      "5077\n",
      "5078\n",
      "5079\n",
      "5080\n",
      "5081\n",
      "5082\n",
      "5083\n",
      "5084\n",
      "5085\n",
      "5086\n",
      "5087\n",
      "5088\n",
      "5089\n",
      "5090\n",
      "5091\n",
      "5092\n",
      "5093\n",
      "5094\n",
      "5095\n",
      "5096\n",
      "5097\n",
      "5098\n",
      "5099\n",
      "5100\n",
      "5101\n",
      "5102\n",
      "5103\n",
      "5104\n",
      "5105\n",
      "5106\n",
      "5107\n",
      "5108\n",
      "5109\n",
      "5110\n",
      "5111\n",
      "5112\n",
      "5113\n",
      "5114\n",
      "5115\n",
      "5116\n",
      "5117\n",
      "5118\n",
      "5119\n",
      "5120\n",
      "5121\n",
      "5122\n",
      "5123\n",
      "5124\n",
      "5125\n",
      "5126\n",
      "5127\n",
      "5128\n",
      "5129\n",
      "5130\n",
      "5131\n",
      "5132\n",
      "5133\n",
      "5134\n",
      "5135\n",
      "5136\n",
      "5137\n",
      "5138\n",
      "5139\n",
      "5140\n",
      "5141\n",
      "5142\n",
      "5143\n",
      "5144\n",
      "5145\n",
      "5146\n",
      "5147\n",
      "5148\n",
      "5149\n",
      "5150\n",
      "5151\n",
      "5152\n",
      "5153\n",
      "5154\n",
      "5155\n",
      "5156\n",
      "5157\n",
      "5158\n",
      "5159\n",
      "5160\n",
      "5161\n",
      "5162\n",
      "5163\n",
      "5164\n",
      "5165\n",
      "5166\n",
      "5167\n",
      "5168\n",
      "5169\n",
      "5170\n",
      "5171\n",
      "5172\n",
      "5173\n",
      "5174\n",
      "5175\n",
      "5176\n",
      "5177\n",
      "5178\n",
      "5179\n",
      "5180\n",
      "5181\n",
      "5182\n",
      "5183\n",
      "5184\n",
      "5185\n",
      "5186\n",
      "5187\n",
      "5188\n",
      "5189\n",
      "5190\n",
      "5191\n",
      "5192\n",
      "5193\n",
      "5194\n",
      "5195\n",
      "5196\n",
      "5197\n",
      "5198\n",
      "5199\n",
      "5200\n",
      "5201\n",
      "5202\n",
      "5203\n",
      "5204\n",
      "5205\n",
      "5206\n",
      "5207\n",
      "5208\n",
      "5209\n",
      "5210\n",
      "5211\n",
      "5212\n",
      "5213\n",
      "5214\n",
      "5215\n",
      "5216\n",
      "5217\n",
      "5218\n",
      "5219\n",
      "5220\n",
      "5221\n",
      "5222\n",
      "5223\n",
      "5224\n",
      "5225\n",
      "5226\n",
      "5227\n",
      "5228\n",
      "5229\n",
      "5230\n",
      "5231\n",
      "5232\n",
      "5233\n",
      "5234\n",
      "5235\n",
      "5236\n",
      "5237\n",
      "5238\n",
      "5239\n",
      "5240\n",
      "5241\n",
      "5242\n",
      "5243\n",
      "5244\n",
      "5245\n",
      "5246\n",
      "5247\n",
      "5248\n",
      "5249\n",
      "5250\n",
      "5251\n",
      "5252\n",
      "5253\n",
      "5254\n",
      "5255\n",
      "5256\n",
      "5257\n",
      "5258\n",
      "5259\n",
      "5260\n",
      "5261\n",
      "5262\n",
      "5263\n",
      "5264\n",
      "5265\n",
      "5266\n",
      "5267\n",
      "5268\n",
      "5269\n",
      "5270\n",
      "5271\n",
      "5272\n",
      "5273\n",
      "5274\n",
      "5275\n",
      "5276\n",
      "5277\n",
      "5278\n",
      "5279\n",
      "5280\n",
      "5281\n",
      "5282\n",
      "5283\n",
      "5284\n",
      "5285\n",
      "5286\n",
      "5287\n",
      "5288\n",
      "5289\n",
      "5290\n",
      "5291\n",
      "5292\n",
      "5293\n",
      "5294\n",
      "5295\n",
      "5296\n",
      "5297\n",
      "5298\n",
      "5299\n",
      "5300\n",
      "5301\n",
      "5302\n",
      "5303\n",
      "5304\n",
      "5305\n",
      "5306\n",
      "5307\n",
      "5308\n",
      "5309\n",
      "5310\n",
      "5311\n",
      "5312\n",
      "5313\n",
      "5314\n",
      "5315\n",
      "5316\n",
      "5317\n",
      "5318\n",
      "5319\n",
      "5320\n",
      "5321\n",
      "5322\n",
      "5323\n",
      "5324\n",
      "5325\n",
      "5326\n",
      "5327\n",
      "5328\n",
      "5329\n",
      "5330\n",
      "5331\n",
      "5332\n",
      "5333\n",
      "5334\n",
      "5335\n",
      "5336\n",
      "5337\n",
      "5338\n",
      "5339\n",
      "5340\n",
      "5341\n",
      "5342\n",
      "5343\n",
      "5344\n",
      "5345\n",
      "5346\n",
      "5347\n",
      "5348\n",
      "5349\n",
      "5350\n",
      "5351\n",
      "5352\n",
      "5353\n",
      "5354\n",
      "5355\n",
      "5356\n",
      "5357\n",
      "5358\n",
      "5359\n",
      "5360\n",
      "5361\n",
      "5362\n",
      "5363\n",
      "5364\n",
      "5365\n",
      "5366\n",
      "5367\n",
      "5368\n",
      "5369\n",
      "5370\n",
      "5371\n",
      "5372\n",
      "5373\n",
      "5374\n",
      "5375\n",
      "5376\n",
      "5377\n",
      "5378\n",
      "5379\n",
      "5380\n",
      "5381\n",
      "5382\n",
      "5383\n",
      "5384\n",
      "5385\n",
      "5386\n",
      "5387\n",
      "5388\n",
      "5389\n",
      "5390\n",
      "5391\n",
      "5392\n",
      "5393\n",
      "5394\n",
      "5395\n",
      "5396\n",
      "5397\n",
      "5398\n",
      "5399\n",
      "5400\n",
      "5401\n",
      "5402\n",
      "5403\n",
      "5404\n",
      "5405\n",
      "5406\n",
      "5407\n",
      "5408\n",
      "5409\n",
      "5410\n",
      "5411\n",
      "5412\n",
      "5413\n",
      "5414\n",
      "5415\n",
      "5416\n",
      "5417\n",
      "5418\n",
      "5419\n",
      "5420\n",
      "5421\n",
      "5422\n",
      "5423\n",
      "5424\n",
      "5425\n",
      "5426\n",
      "5427\n",
      "5428\n",
      "5429\n",
      "5430\n",
      "5431\n",
      "5432\n",
      "5433\n",
      "5434\n",
      "5435\n",
      "5436\n",
      "5437\n",
      "5438\n",
      "5439\n",
      "5440\n",
      "5441\n",
      "5442\n",
      "5443\n",
      "5444\n",
      "5445\n",
      "5446\n",
      "5447\n",
      "5448\n",
      "5449\n",
      "5450\n",
      "5451\n",
      "5452\n",
      "5453\n",
      "5454\n",
      "5455\n",
      "5456\n",
      "5457\n",
      "5458\n",
      "5459\n",
      "5460\n",
      "5461\n",
      "5462\n",
      "5463\n",
      "5464\n",
      "5465\n",
      "5466\n",
      "5467\n",
      "5468\n",
      "5469\n",
      "5470\n",
      "5471\n",
      "5472\n",
      "5473\n",
      "5474\n",
      "5475\n",
      "5476\n",
      "5477\n",
      "5478\n",
      "5479\n",
      "5480\n",
      "5481\n",
      "5482\n",
      "5483\n",
      "5484\n",
      "5485\n",
      "5486\n",
      "5487\n",
      "5488\n",
      "5489\n",
      "5490\n",
      "5491\n",
      "5492\n",
      "5493\n",
      "5494\n",
      "5495\n",
      "5496\n",
      "5497\n",
      "5498\n",
      "5499\n",
      "5500\n",
      "5501\n",
      "5502\n",
      "5503\n",
      "5504\n",
      "5505\n",
      "5506\n",
      "5507\n",
      "5508\n",
      "5509\n",
      "5510\n",
      "5511\n",
      "5512\n",
      "5513\n",
      "5514\n",
      "5515\n",
      "5516\n",
      "5517\n",
      "5518\n",
      "5519\n",
      "5520\n",
      "5521\n",
      "5522\n",
      "5523\n",
      "5524\n",
      "5525\n",
      "5526\n",
      "5527\n",
      "5528\n",
      "5529\n",
      "5530\n",
      "5531\n",
      "5532\n",
      "5533\n",
      "5534\n",
      "5535\n",
      "5536\n",
      "5537\n",
      "5538\n",
      "5539\n",
      "5540\n",
      "5541\n",
      "5542\n",
      "5543\n",
      "5544\n",
      "5545\n",
      "5546\n",
      "5547\n",
      "5548\n",
      "5549\n",
      "5550\n",
      "5551\n",
      "5552\n",
      "5553\n",
      "5554\n",
      "5555\n",
      "5556\n",
      "5557\n",
      "5558\n",
      "5559\n",
      "5560\n",
      "5561\n",
      "5562\n",
      "5563\n",
      "5564\n",
      "5565\n",
      "5566\n",
      "5567\n",
      "5568\n",
      "5569\n",
      "5570\n",
      "5571\n",
      "5572\n",
      "5573\n",
      "5574\n",
      "5575\n",
      "5576\n",
      "5577\n",
      "5578\n",
      "5579\n",
      "5580\n",
      "5581\n",
      "5582\n",
      "5583\n",
      "5584\n",
      "5585\n",
      "5586\n",
      "5587\n",
      "5588\n",
      "5589\n",
      "5590\n",
      "5591\n",
      "5592\n",
      "5593\n",
      "5594\n",
      "5595\n",
      "5596\n",
      "5597\n",
      "5598\n",
      "5599\n",
      "5600\n",
      "5601\n",
      "5602\n",
      "5603\n",
      "5604\n",
      "5605\n",
      "5606\n",
      "5607\n",
      "5608\n",
      "5609\n",
      "5610\n",
      "5611\n",
      "5612\n",
      "5613\n",
      "5614\n",
      "5615\n",
      "5616\n",
      "5617\n",
      "5618\n",
      "5619\n",
      "5620\n",
      "5621\n",
      "5622\n",
      "5623\n",
      "5624\n",
      "5625\n",
      "5626\n",
      "5627\n",
      "5628\n",
      "5629\n",
      "5630\n",
      "5631\n",
      "5632\n",
      "5633\n",
      "5634\n",
      "5635\n",
      "5636\n",
      "5637\n",
      "5638\n",
      "5639\n",
      "5640\n",
      "5641\n",
      "5642\n",
      "5643\n",
      "5644\n",
      "5645\n",
      "5646\n",
      "5647\n",
      "5648\n",
      "5649\n",
      "5650\n",
      "5651\n",
      "5652\n",
      "5653\n",
      "5654\n",
      "5655\n",
      "5656\n",
      "5657\n",
      "5658\n",
      "5659\n",
      "5660\n",
      "5661\n",
      "5662\n",
      "5663\n",
      "5664\n",
      "5665\n",
      "5666\n",
      "5667\n",
      "5668\n",
      "5669\n",
      "5670\n",
      "5671\n",
      "5672\n",
      "5673\n",
      "5674\n",
      "5675\n",
      "5676\n",
      "5677\n",
      "5678\n",
      "5679\n",
      "5680\n",
      "5681\n",
      "5682\n",
      "5683\n",
      "5684\n",
      "5685\n",
      "5686\n",
      "5687\n",
      "5688\n",
      "5689\n",
      "5690\n",
      "5691\n",
      "5692\n",
      "5693\n",
      "5694\n",
      "5695\n",
      "5696\n",
      "5697\n",
      "5698\n",
      "5699\n",
      "5700\n",
      "5701\n",
      "5702\n",
      "5703\n",
      "5704\n",
      "5705\n",
      "5706\n",
      "5707\n",
      "5708\n",
      "5709\n",
      "5710\n",
      "5711\n",
      "5712\n",
      "5713\n",
      "5714\n",
      "5715\n",
      "5716\n",
      "5717\n",
      "5718\n",
      "5719\n",
      "5720\n",
      "5721\n",
      "5722\n",
      "5723\n",
      "5724\n",
      "5725\n",
      "5726\n",
      "5727\n",
      "5728\n",
      "5729\n",
      "5730\n",
      "5731\n",
      "5732\n",
      "5733\n",
      "5734\n",
      "5735\n",
      "5736\n",
      "5737\n",
      "5738\n",
      "5739\n",
      "5740\n",
      "5741\n",
      "5742\n",
      "5743\n",
      "5744\n",
      "5745\n",
      "5746\n",
      "5747\n",
      "5748\n",
      "5749\n",
      "5750\n",
      "5751\n",
      "5752\n",
      "5753\n",
      "5754\n",
      "5755\n",
      "5756\n",
      "5757\n",
      "5758\n",
      "5759\n",
      "5760\n",
      "5761\n",
      "5762\n",
      "5763\n",
      "5764\n",
      "5765\n",
      "5766\n",
      "5767\n",
      "5768\n",
      "5769\n",
      "5770\n",
      "5771\n",
      "5772\n",
      "5773\n",
      "5774\n",
      "5775\n",
      "5776\n",
      "5777\n",
      "5778\n",
      "5779\n",
      "5780\n",
      "5781\n",
      "5782\n",
      "5783\n",
      "5784\n",
      "5785\n",
      "5786\n",
      "5787\n",
      "5788\n",
      "5789\n",
      "5790\n",
      "5791\n",
      "5792\n",
      "5793\n",
      "5794\n",
      "5795\n",
      "5796\n",
      "5797\n",
      "5798\n",
      "5799\n",
      "5800\n",
      "5801\n",
      "5802\n",
      "5803\n",
      "5804\n",
      "5805\n",
      "5806\n",
      "5807\n",
      "5808\n",
      "5809\n",
      "5810\n",
      "5811\n",
      "5812\n",
      "5813\n",
      "5814\n",
      "5815\n",
      "5816\n",
      "5817\n",
      "5818\n",
      "5819\n",
      "5820\n",
      "5821\n",
      "5822\n",
      "5823\n",
      "5824\n",
      "5825\n",
      "5826\n",
      "5827\n",
      "5828\n",
      "5829\n",
      "5830\n",
      "5831\n",
      "5832\n",
      "5833\n",
      "5834\n",
      "5835\n",
      "5836\n",
      "5837\n",
      "5838\n",
      "5839\n",
      "5840\n",
      "5841\n",
      "5842\n",
      "5843\n",
      "5844\n",
      "5845\n",
      "5846\n",
      "5847\n",
      "5848\n",
      "5849\n",
      "5850\n",
      "5851\n",
      "5852\n",
      "5853\n",
      "5854\n",
      "5855\n",
      "5856\n",
      "5857\n",
      "5858\n",
      "5859\n",
      "5860\n",
      "5861\n",
      "5862\n",
      "5863\n",
      "5864\n",
      "5865\n",
      "5866\n",
      "5867\n",
      "5868\n",
      "5869\n",
      "5870\n",
      "5871\n",
      "5872\n",
      "5873\n",
      "5874\n",
      "5875\n",
      "5876\n",
      "5877\n",
      "5878\n",
      "5879\n",
      "5880\n",
      "5881\n",
      "5882\n",
      "5883\n",
      "5884\n",
      "5885\n",
      "5886\n",
      "5887\n",
      "5888\n",
      "5889\n",
      "5890\n",
      "5891\n",
      "5892\n",
      "5893\n",
      "5894\n",
      "5895\n",
      "5896\n",
      "5897\n",
      "5898\n",
      "5899\n",
      "5900\n",
      "5901\n",
      "5902\n",
      "5903\n",
      "5904\n",
      "5905\n",
      "5906\n",
      "5907\n",
      "5908\n",
      "5909\n",
      "5910\n",
      "5911\n",
      "5912\n",
      "5913\n",
      "5914\n",
      "5915\n",
      "5916\n",
      "5917\n",
      "5918\n",
      "5919\n",
      "5920\n",
      "5921\n",
      "5922\n",
      "5923\n",
      "5924\n",
      "5925\n",
      "5926\n",
      "5927\n",
      "5928\n",
      "5929\n",
      "5930\n",
      "5931\n",
      "5932\n",
      "5933\n",
      "5934\n",
      "5935\n",
      "5936\n",
      "5937\n",
      "5938\n",
      "5939\n",
      "5940\n",
      "5941\n",
      "5942\n",
      "5943\n",
      "5944\n",
      "5945\n",
      "5946\n",
      "5947\n",
      "5948\n",
      "5949\n",
      "5950\n",
      "5951\n",
      "5952\n",
      "5953\n",
      "5954\n",
      "5955\n",
      "5956\n",
      "5957\n",
      "5958\n",
      "5959\n",
      "5960\n",
      "5961\n",
      "5962\n",
      "5963\n",
      "5964\n",
      "5965\n",
      "5966\n",
      "5967\n",
      "5968\n",
      "5969\n",
      "5970\n",
      "5971\n",
      "5972\n",
      "5973\n",
      "5974\n",
      "5975\n",
      "5976\n",
      "5977\n",
      "5978\n",
      "5979\n",
      "5980\n",
      "5981\n",
      "5982\n",
      "5983\n",
      "5984\n",
      "5985\n",
      "5986\n",
      "5987\n",
      "5988\n",
      "5989\n",
      "5990\n",
      "5991\n",
      "5992\n",
      "5993\n",
      "5994\n",
      "5995\n",
      "5996\n",
      "5997\n",
      "5998\n",
      "5999\n",
      "6000\n",
      "6001\n",
      "6002\n",
      "6003\n",
      "6004\n",
      "6005\n",
      "6006\n",
      "6007\n",
      "6008\n",
      "6009\n",
      "6010\n",
      "6011\n",
      "6012\n",
      "6013\n",
      "6014\n",
      "6015\n",
      "6016\n",
      "6017\n",
      "6018\n",
      "6019\n",
      "6020\n",
      "6021\n",
      "6022\n",
      "6023\n",
      "6024\n",
      "6025\n",
      "6026\n",
      "6027\n",
      "6028\n",
      "6029\n",
      "6030\n",
      "6031\n",
      "6032\n",
      "6033\n",
      "6034\n",
      "6035\n",
      "6036\n",
      "6037\n",
      "6038\n",
      "6039\n",
      "6040\n",
      "6041\n",
      "6042\n",
      "6043\n",
      "6044\n",
      "6045\n",
      "6046\n",
      "6047\n",
      "6048\n",
      "6049\n",
      "6050\n",
      "6051\n",
      "6052\n",
      "6053\n",
      "6054\n",
      "6055\n",
      "6056\n",
      "6057\n",
      "6058\n",
      "6059\n",
      "6060\n",
      "6061\n",
      "6062\n",
      "6063\n",
      "6064\n",
      "6065\n",
      "6066\n",
      "6067\n",
      "6068\n",
      "6069\n",
      "6070\n",
      "6071\n",
      "6072\n",
      "6073\n",
      "6074\n",
      "6075\n",
      "6076\n",
      "6077\n",
      "6078\n",
      "6079\n",
      "6080\n",
      "6081\n",
      "6082\n",
      "6083\n",
      "6084\n",
      "6085\n",
      "6086\n",
      "6087\n",
      "6088\n",
      "6089\n",
      "6090\n",
      "6091\n",
      "6092\n",
      "6093\n",
      "6094\n",
      "6095\n",
      "6096\n",
      "6097\n",
      "6098\n",
      "6099\n",
      "6100\n",
      "6101\n",
      "6102\n",
      "6103\n",
      "6104\n",
      "6105\n",
      "6106\n",
      "6107\n",
      "6108\n",
      "6109\n",
      "6110\n",
      "6111\n",
      "6112\n",
      "6113\n",
      "6114\n",
      "6115\n",
      "6116\n",
      "6117\n",
      "6118\n",
      "6119\n",
      "6120\n",
      "6121\n",
      "6122\n",
      "6123\n",
      "6124\n",
      "6125\n",
      "6126\n",
      "6127\n",
      "6128\n",
      "6129\n",
      "6130\n",
      "6131\n",
      "6132\n",
      "6133\n",
      "6134\n",
      "6135\n",
      "6136\n",
      "6137\n",
      "6138\n",
      "6139\n",
      "6140\n",
      "6141\n",
      "6142\n",
      "6143\n",
      "6144\n",
      "6145\n",
      "6146\n",
      "6147\n",
      "6148\n",
      "6149\n",
      "6150\n",
      "6151\n",
      "6152\n",
      "6153\n",
      "6154\n",
      "6155\n",
      "6156\n",
      "6157\n",
      "6158\n",
      "6159\n",
      "6160\n",
      "6161\n",
      "6162\n",
      "6163\n",
      "6164\n",
      "6165\n",
      "6166\n",
      "6167\n",
      "6168\n",
      "6169\n",
      "6170\n",
      "6171\n",
      "6172\n",
      "6173\n",
      "6174\n",
      "6175\n",
      "6176\n",
      "6177\n",
      "6178\n",
      "6179\n",
      "6180\n",
      "6181\n",
      "6182\n",
      "6183\n",
      "6184\n",
      "6185\n",
      "6186\n",
      "6187\n",
      "6188\n",
      "6189\n",
      "6190\n",
      "6191\n",
      "6192\n",
      "6193\n",
      "6194\n",
      "6195\n",
      "6196\n",
      "6197\n",
      "6198\n",
      "6199\n",
      "6200\n",
      "6201\n",
      "6202\n",
      "6203\n",
      "6204\n",
      "6205\n",
      "6206\n",
      "6207\n",
      "6208\n",
      "6209\n",
      "6210\n",
      "6211\n",
      "6212\n",
      "6213\n",
      "6214\n",
      "6215\n",
      "6216\n",
      "6217\n",
      "6218\n",
      "6219\n",
      "6220\n",
      "6221\n",
      "6222\n",
      "6223\n",
      "6224\n",
      "6225\n",
      "6226\n",
      "6227\n",
      "6228\n",
      "6229\n",
      "6230\n",
      "6231\n",
      "6232\n",
      "6233\n",
      "6234\n",
      "6235\n",
      "6236\n",
      "6237\n",
      "6238\n",
      "6239\n",
      "6240\n",
      "6241\n",
      "6242\n",
      "6243\n",
      "6244\n",
      "6245\n",
      "6246\n",
      "6247\n",
      "6248\n",
      "6249\n",
      "6250\n",
      "6251\n",
      "6252\n",
      "6253\n",
      "6254\n",
      "6255\n",
      "6256\n",
      "6257\n",
      "6258\n",
      "6259\n",
      "6260\n",
      "6261\n",
      "6262\n",
      "6263\n",
      "6264\n",
      "6265\n",
      "6266\n",
      "6267\n",
      "6268\n",
      "6269\n",
      "6270\n",
      "6271\n",
      "6272\n",
      "6273\n",
      "6274\n",
      "6275\n",
      "6276\n",
      "6277\n",
      "6278\n",
      "6279\n",
      "6280\n",
      "6281\n",
      "6282\n",
      "6283\n",
      "6284\n",
      "6285\n",
      "6286\n",
      "6287\n",
      "6288\n",
      "6289\n",
      "6290\n",
      "6291\n",
      "6292\n",
      "6293\n",
      "6294\n",
      "6295\n",
      "6296\n",
      "6297\n",
      "6298\n",
      "6299\n",
      "6300\n",
      "6301\n",
      "6302\n",
      "6303\n",
      "6304\n",
      "6305\n",
      "6306\n",
      "6307\n",
      "6308\n",
      "6309\n",
      "6310\n",
      "6311\n",
      "6312\n",
      "6313\n",
      "6314\n",
      "6315\n",
      "6316\n",
      "6317\n",
      "6318\n",
      "6319\n",
      "6320\n",
      "6321\n",
      "6322\n",
      "6323\n",
      "6324\n",
      "6325\n",
      "6326\n",
      "6327\n",
      "6328\n",
      "6329\n",
      "6330\n",
      "6331\n",
      "6332\n",
      "6333\n",
      "6334\n",
      "6335\n",
      "6336\n",
      "6337\n",
      "6338\n",
      "6339\n",
      "6340\n",
      "6341\n",
      "6342\n",
      "6343\n",
      "6344\n",
      "6345\n",
      "6346\n",
      "6347\n",
      "6348\n",
      "6349\n",
      "6350\n",
      "6351\n",
      "6352\n",
      "6353\n",
      "6354\n",
      "6355\n",
      "6356\n",
      "6357\n",
      "6358\n",
      "6359\n",
      "6360\n",
      "6361\n",
      "6362\n",
      "6363\n",
      "6364\n",
      "6365\n",
      "6366\n",
      "6367\n",
      "6368\n",
      "6369\n",
      "6370\n",
      "6371\n",
      "6372\n",
      "6373\n",
      "6374\n",
      "6375\n",
      "6376\n",
      "6377\n",
      "6378\n",
      "6379\n",
      "6380\n",
      "6381\n",
      "6382\n",
      "6383\n",
      "6384\n",
      "6385\n",
      "6386\n",
      "6387\n",
      "6388\n",
      "6389\n",
      "6390\n",
      "6391\n",
      "6392\n",
      "6393\n",
      "6394\n",
      "6395\n",
      "6396\n",
      "6397\n",
      "6398\n",
      "6399\n",
      "6400\n",
      "6401\n",
      "6402\n",
      "6403\n",
      "6404\n",
      "6405\n",
      "6406\n",
      "6407\n",
      "6408\n",
      "6409\n",
      "6410\n",
      "6411\n",
      "6412\n",
      "6413\n",
      "6414\n",
      "6415\n",
      "6416\n",
      "6417\n",
      "6418\n",
      "6419\n",
      "6420\n",
      "6421\n",
      "6422\n",
      "6423\n",
      "6424\n",
      "6425\n",
      "6426\n",
      "6427\n",
      "6428\n",
      "6429\n",
      "6430\n",
      "6431\n",
      "6432\n",
      "6433\n",
      "6434\n",
      "6435\n",
      "6436\n",
      "6437\n",
      "6438\n",
      "6439\n",
      "6440\n",
      "6441\n",
      "6442\n",
      "6443\n",
      "6444\n",
      "6445\n",
      "6446\n",
      "6447\n",
      "6448\n",
      "6449\n",
      "6450\n",
      "6451\n",
      "6452\n",
      "6453\n",
      "6454\n",
      "6455\n",
      "6456\n",
      "6457\n",
      "6458\n",
      "6459\n",
      "6460\n",
      "6461\n",
      "6462\n",
      "6463\n",
      "6464\n",
      "6465\n",
      "6466\n",
      "6467\n",
      "6468\n",
      "6469\n",
      "6470\n",
      "6471\n",
      "6472\n",
      "6473\n",
      "6474\n",
      "6475\n",
      "6476\n",
      "6477\n",
      "6478\n",
      "6479\n",
      "6480\n",
      "6481\n",
      "6482\n",
      "6483\n",
      "6484\n",
      "6485\n",
      "6486\n",
      "6487\n",
      "6488\n",
      "6489\n",
      "6490\n",
      "6491\n",
      "6492\n",
      "6493\n",
      "6494\n",
      "6495\n",
      "6496\n",
      "6497\n",
      "6498\n",
      "6499\n",
      "6500\n",
      "6501\n",
      "6502\n",
      "6503\n",
      "6504\n",
      "6505\n",
      "6506\n",
      "6507\n",
      "6508\n",
      "6509\n",
      "6510\n",
      "6511\n",
      "6512\n",
      "6513\n",
      "6514\n",
      "6515\n",
      "6516\n",
      "6517\n",
      "6518\n",
      "6519\n",
      "6520\n",
      "6521\n",
      "6522\n",
      "6523\n",
      "6524\n",
      "6525\n",
      "6526\n",
      "6527\n",
      "6528\n",
      "6529\n",
      "6530\n",
      "6531\n",
      "6532\n",
      "6533\n",
      "6534\n",
      "6535\n",
      "6536\n",
      "6537\n",
      "6538\n",
      "6539\n",
      "6540\n",
      "6541\n",
      "6542\n",
      "6543\n",
      "6544\n",
      "6545\n",
      "6546\n",
      "6547\n",
      "6548\n",
      "6549\n",
      "6550\n",
      "6551\n",
      "6552\n",
      "6553\n",
      "6554\n",
      "6555\n",
      "6556\n",
      "6557\n",
      "6558\n",
      "6559\n",
      "6560\n",
      "6561\n",
      "6562\n",
      "6563\n",
      "6564\n",
      "6565\n",
      "6566\n",
      "6567\n",
      "6568\n",
      "6569\n",
      "6570\n",
      "6571\n",
      "6572\n",
      "6573\n",
      "6574\n",
      "6575\n",
      "6576\n",
      "6577\n",
      "6578\n",
      "6579\n",
      "6580\n",
      "6581\n",
      "6582\n",
      "6583\n",
      "6584\n",
      "6585\n",
      "6586\n",
      "6587\n",
      "6588\n",
      "6589\n",
      "6590\n",
      "6591\n",
      "6592\n",
      "6593\n",
      "6594\n",
      "6595\n",
      "6596\n",
      "6597\n",
      "6598\n",
      "6599\n",
      "6600\n",
      "6601\n",
      "6602\n",
      "6603\n",
      "6604\n",
      "6605\n",
      "6606\n",
      "6607\n",
      "6608\n",
      "6609\n",
      "6610\n",
      "6611\n",
      "6612\n",
      "6613\n",
      "6614\n",
      "6615\n",
      "6616\n",
      "6617\n",
      "6618\n",
      "6619\n",
      "6620\n",
      "6621\n",
      "6622\n",
      "6623\n",
      "6624\n",
      "6625\n",
      "6626\n",
      "6627\n",
      "6628\n",
      "6629\n",
      "6630\n",
      "6631\n",
      "6632\n",
      "6633\n",
      "6634\n",
      "6635\n",
      "6636\n",
      "6637\n",
      "6638\n",
      "6639\n",
      "6640\n",
      "6641\n",
      "6642\n",
      "6643\n",
      "6644\n",
      "6645\n",
      "6646\n",
      "6647\n",
      "6648\n",
      "6649\n",
      "6650\n",
      "6651\n",
      "6652\n",
      "6653\n",
      "6654\n",
      "6655\n",
      "6656\n",
      "6657\n",
      "6658\n",
      "6659\n",
      "6660\n",
      "6661\n",
      "6662\n",
      "6663\n",
      "6664\n",
      "6665\n",
      "6666\n",
      "6667\n",
      "6668\n",
      "6669\n",
      "6670\n",
      "6671\n",
      "6672\n",
      "6673\n",
      "6674\n",
      "6675\n",
      "6676\n",
      "6677\n",
      "6678\n",
      "6679\n",
      "6680\n",
      "6681\n",
      "6682\n",
      "6683\n",
      "6684\n",
      "6685\n",
      "6686\n",
      "6687\n",
      "6688\n",
      "6689\n",
      "6690\n",
      "6691\n",
      "6692\n",
      "6693\n",
      "6694\n",
      "6695\n",
      "6696\n",
      "6697\n",
      "6698\n",
      "6699\n",
      "6700\n",
      "6701\n",
      "6702\n",
      "6703\n",
      "6704\n",
      "6705\n",
      "6706\n",
      "6707\n",
      "6708\n",
      "6709\n",
      "6710\n",
      "6711\n",
      "6712\n",
      "6713\n",
      "6714\n",
      "6715\n",
      "6716\n",
      "6717\n",
      "6718\n",
      "6719\n",
      "6720\n",
      "6721\n",
      "6722\n",
      "6723\n",
      "6724\n",
      "6725\n",
      "6726\n",
      "6727\n",
      "6728\n",
      "6729\n",
      "6730\n",
      "6731\n",
      "6732\n",
      "6733\n",
      "6734\n",
      "6735\n",
      "6736\n",
      "6737\n",
      "6738\n",
      "6739\n",
      "6740\n",
      "6741\n",
      "6742\n",
      "6743\n",
      "6744\n",
      "6745\n",
      "6746\n",
      "6747\n",
      "6748\n",
      "6749\n",
      "6750\n",
      "6751\n",
      "6752\n",
      "6753\n",
      "6754\n",
      "6755\n",
      "6756\n",
      "6757\n",
      "6758\n",
      "6759\n",
      "6760\n",
      "6761\n",
      "6762\n",
      "6763\n",
      "6764\n",
      "6765\n",
      "6766\n",
      "6767\n",
      "6768\n",
      "6769\n",
      "6770\n",
      "6771\n",
      "6772\n",
      "6773\n",
      "6774\n",
      "6775\n",
      "6776\n",
      "6777\n",
      "6778\n",
      "6779\n",
      "6780\n",
      "6781\n",
      "6782\n",
      "6783\n",
      "6784\n",
      "6785\n",
      "6786\n",
      "6787\n",
      "6788\n",
      "6789\n",
      "6790\n",
      "6791\n",
      "6792\n",
      "6793\n",
      "6794\n",
      "6795\n",
      "6796\n",
      "6797\n",
      "6798\n",
      "6799\n",
      "6800\n",
      "6801\n",
      "6802\n",
      "6803\n",
      "6804\n",
      "6805\n",
      "6806\n",
      "6807\n",
      "6808\n",
      "6809\n",
      "6810\n",
      "6811\n",
      "6812\n",
      "6813\n",
      "6814\n",
      "6815\n",
      "6816\n",
      "6817\n",
      "6818\n",
      "6819\n",
      "6820\n",
      "6821\n",
      "6822\n",
      "6823\n",
      "6824\n",
      "6825\n",
      "6826\n",
      "6827\n",
      "6828\n",
      "6829\n",
      "6830\n",
      "6831\n",
      "6832\n",
      "6833\n",
      "6834\n",
      "6835\n",
      "6836\n",
      "6837\n",
      "6838\n",
      "6839\n",
      "6840\n",
      "6841\n",
      "6842\n",
      "6843\n",
      "6844\n",
      "6845\n",
      "6846\n",
      "6847\n",
      "6848\n",
      "6849\n",
      "6850\n",
      "6851\n",
      "6852\n",
      "6853\n",
      "6854\n",
      "6855\n",
      "6856\n",
      "6857\n",
      "6858\n",
      "6859\n",
      "6860\n",
      "6861\n",
      "6862\n",
      "6863\n",
      "6864\n",
      "6865\n",
      "6866\n",
      "6867\n",
      "6868\n",
      "6869\n",
      "6870\n",
      "6871\n",
      "6872\n",
      "6873\n",
      "6874\n",
      "6875\n",
      "6876\n",
      "6877\n",
      "6878\n",
      "6879\n",
      "6880\n",
      "6881\n",
      "6882\n",
      "6883\n",
      "6884\n",
      "6885\n",
      "6886\n",
      "6887\n",
      "6888\n",
      "6889\n",
      "6890\n",
      "6891\n",
      "6892\n",
      "6893\n",
      "6894\n",
      "6895\n",
      "6896\n",
      "6897\n",
      "6898\n",
      "6899\n",
      "6900\n",
      "6901\n",
      "6902\n",
      "6903\n",
      "6904\n",
      "6905\n",
      "6906\n",
      "6907\n",
      "6908\n",
      "6909\n",
      "6910\n",
      "6911\n",
      "6912\n",
      "6913\n",
      "6914\n",
      "6915\n",
      "6916\n",
      "6917\n",
      "6918\n",
      "6919\n",
      "6920\n",
      "6921\n",
      "6922\n",
      "6923\n",
      "6924\n",
      "6925\n",
      "6926\n",
      "6927\n",
      "6928\n",
      "6929\n",
      "6930\n",
      "6931\n",
      "6932\n",
      "6933\n",
      "6934\n",
      "6935\n",
      "6936\n",
      "6937\n",
      "6938\n",
      "6939\n",
      "6940\n",
      "6941\n",
      "6942\n",
      "6943\n",
      "6944\n",
      "6945\n",
      "6946\n",
      "6947\n",
      "6948\n",
      "6949\n",
      "6950\n",
      "6951\n",
      "6952\n",
      "6953\n",
      "6954\n",
      "6955\n",
      "6956\n",
      "6957\n",
      "6958\n",
      "6959\n",
      "6960\n",
      "6961\n",
      "6962\n",
      "6963\n",
      "6964\n",
      "6965\n",
      "6966\n",
      "6967\n",
      "6968\n",
      "6969\n",
      "6970\n",
      "6971\n",
      "6972\n",
      "6973\n",
      "6974\n",
      "6975\n",
      "6976\n",
      "6977\n",
      "6978\n",
      "6979\n",
      "6980\n",
      "6981\n",
      "6982\n",
      "6983\n",
      "6984\n",
      "6985\n",
      "6986\n",
      "6987\n",
      "6988\n",
      "6989\n",
      "6990\n",
      "6991\n",
      "6992\n",
      "6993\n",
      "6994\n",
      "6995\n",
      "6996\n",
      "6997\n",
      "6998\n",
      "6999\n",
      "7000\n",
      "7001\n",
      "7002\n",
      "7003\n",
      "7004\n",
      "7005\n",
      "7006\n",
      "7007\n",
      "7008\n",
      "7009\n",
      "7010\n",
      "7011\n",
      "7012\n",
      "7013\n",
      "7014\n",
      "7015\n",
      "7016\n",
      "7017\n",
      "7018\n",
      "7019\n",
      "7020\n",
      "7021\n",
      "7022\n",
      "7023\n",
      "7024\n",
      "7025\n",
      "7026\n",
      "7027\n",
      "7028\n",
      "7029\n",
      "7030\n",
      "7031\n",
      "7032\n",
      "7033\n",
      "7034\n",
      "7035\n",
      "7036\n",
      "7037\n",
      "7038\n",
      "7039\n",
      "7040\n",
      "7041\n",
      "7042\n",
      "7043\n",
      "7044\n",
      "7045\n",
      "7046\n",
      "7047\n",
      "7048\n",
      "7049\n",
      "7050\n",
      "7051\n",
      "7052\n",
      "7053\n",
      "7054\n",
      "7055\n",
      "7056\n",
      "7057\n",
      "7058\n",
      "7059\n",
      "7060\n",
      "7061\n",
      "7062\n",
      "7063\n",
      "7064\n",
      "7065\n",
      "7066\n",
      "7067\n",
      "7068\n",
      "7069\n",
      "7070\n",
      "7071\n",
      "7072\n",
      "7073\n",
      "7074\n",
      "7075\n",
      "7076\n",
      "7077\n",
      "7078\n",
      "7079\n",
      "7080\n",
      "7081\n",
      "7082\n",
      "7083\n",
      "7084\n",
      "7085\n",
      "7086\n",
      "7087\n",
      "7088\n",
      "7089\n",
      "7090\n",
      "7091\n",
      "7092\n",
      "7093\n",
      "7094\n",
      "7095\n",
      "7096\n",
      "7097\n",
      "7098\n",
      "7099\n",
      "7100\n",
      "7101\n",
      "7102\n",
      "7103\n",
      "7104\n",
      "7105\n",
      "7106\n",
      "7107\n",
      "7108\n",
      "7109\n",
      "7110\n",
      "7111\n",
      "7112\n",
      "7113\n",
      "7114\n",
      "7115\n",
      "7116\n",
      "7117\n",
      "7118\n",
      "7119\n",
      "7120\n",
      "7121\n",
      "7122\n",
      "7123\n",
      "7124\n",
      "7125\n",
      "7126\n",
      "7127\n",
      "7128\n",
      "7129\n",
      "7130\n",
      "7131\n",
      "7132\n",
      "7133\n",
      "7134\n",
      "7135\n",
      "7136\n",
      "7137\n",
      "7138\n",
      "7139\n",
      "7140\n",
      "7141\n",
      "7142\n",
      "7143\n",
      "7144\n",
      "7145\n",
      "7146\n",
      "7147\n",
      "7148\n",
      "7149\n",
      "7150\n",
      "7151\n",
      "7152\n",
      "7153\n",
      "7154\n",
      "7155\n",
      "7156\n",
      "7157\n",
      "7158\n",
      "7159\n",
      "7160\n",
      "7161\n",
      "7162\n",
      "7163\n",
      "7164\n",
      "7165\n",
      "7166\n",
      "7167\n",
      "7168\n",
      "7169\n",
      "7170\n",
      "7171\n",
      "7172\n",
      "7173\n",
      "7174\n",
      "7175\n",
      "7176\n",
      "7177\n",
      "7178\n",
      "7179\n",
      "7180\n",
      "7181\n",
      "7182\n",
      "7183\n",
      "7184\n",
      "7185\n",
      "7186\n",
      "7187\n",
      "7188\n",
      "7189\n",
      "7190\n",
      "7191\n",
      "7192\n",
      "7193\n",
      "7194\n",
      "7195\n",
      "7196\n",
      "7197\n",
      "7198\n",
      "7199\n",
      "7200\n",
      "7201\n",
      "7202\n",
      "7203\n",
      "7204\n",
      "7205\n",
      "7206\n",
      "7207\n",
      "7208\n",
      "7209\n",
      "7210\n",
      "7211\n",
      "7212\n",
      "7213\n",
      "7214\n",
      "7215\n",
      "7216\n",
      "7217\n",
      "7218\n",
      "7219\n",
      "7220\n",
      "7221\n",
      "7222\n",
      "7223\n",
      "7224\n",
      "7225\n",
      "7226\n",
      "7227\n",
      "7228\n",
      "7229\n",
      "7230\n",
      "7231\n",
      "7232\n",
      "7233\n",
      "7234\n",
      "7235\n",
      "7236\n",
      "7237\n",
      "7238\n",
      "7239\n",
      "7240\n",
      "7241\n",
      "7242\n",
      "7243\n",
      "7244\n",
      "7245\n",
      "7246\n",
      "7247\n",
      "7248\n",
      "7249\n",
      "7250\n",
      "7251\n",
      "7252\n",
      "7253\n",
      "7254\n",
      "7255\n",
      "7256\n",
      "7257\n",
      "7258\n",
      "7259\n",
      "7260\n",
      "7261\n",
      "7262\n",
      "7263\n",
      "7264\n",
      "7265\n",
      "7266\n",
      "7267\n",
      "7268\n",
      "7269\n",
      "7270\n",
      "7271\n",
      "7272\n",
      "7273\n",
      "7274\n",
      "7275\n",
      "7276\n",
      "7277\n",
      "7278\n",
      "7279\n",
      "7280\n",
      "7281\n",
      "7282\n",
      "7283\n",
      "7284\n",
      "7285\n",
      "7286\n",
      "7287\n",
      "7288\n",
      "7289\n",
      "7290\n",
      "7291\n",
      "7292\n",
      "7293\n",
      "7294\n",
      "7295\n",
      "7296\n",
      "7297\n",
      "7298\n",
      "7299\n",
      "7300\n",
      "7301\n",
      "7302\n",
      "7303\n",
      "7304\n",
      "7305\n",
      "7306\n",
      "7307\n",
      "7308\n",
      "7309\n",
      "7310\n",
      "7311\n",
      "7312\n",
      "7313\n",
      "7314\n",
      "7315\n",
      "7316\n",
      "7317\n",
      "7318\n",
      "7319\n",
      "7320\n",
      "7321\n",
      "7322\n",
      "7323\n",
      "7324\n",
      "7325\n",
      "7326\n",
      "7327\n",
      "7328\n",
      "7329\n",
      "7330\n",
      "7331\n",
      "7332\n",
      "7333\n",
      "7334\n",
      "7335\n",
      "7336\n",
      "7337\n",
      "7338\n",
      "7339\n",
      "7340\n",
      "7341\n",
      "7342\n",
      "7343\n",
      "7344\n",
      "7345\n",
      "7346\n",
      "7347\n",
      "7348\n",
      "7349\n",
      "7350\n",
      "7351\n",
      "7352\n",
      "7353\n",
      "7354\n",
      "7355\n",
      "7356\n",
      "7357\n",
      "7358\n",
      "7359\n",
      "7360\n",
      "7361\n",
      "7362\n",
      "7363\n",
      "7364\n",
      "7365\n",
      "7366\n",
      "7367\n",
      "7368\n",
      "7369\n",
      "7370\n",
      "7371\n",
      "7372\n",
      "7373\n",
      "7374\n",
      "7375\n",
      "7376\n",
      "7377\n",
      "7378\n",
      "7379\n",
      "7380\n",
      "7381\n",
      "7382\n",
      "7383\n",
      "7384\n",
      "7385\n",
      "7386\n",
      "7387\n",
      "7388\n",
      "7389\n",
      "7390\n",
      "7391\n",
      "7392\n",
      "7393\n",
      "7394\n",
      "7395\n",
      "7396\n",
      "7397\n",
      "7398\n",
      "7399\n",
      "7400\n",
      "7401\n",
      "7402\n",
      "7403\n",
      "7404\n",
      "7405\n",
      "7406\n",
      "7407\n",
      "7408\n",
      "7409\n",
      "7410\n",
      "7411\n",
      "7412\n",
      "7413\n",
      "7414\n",
      "7415\n",
      "7416\n",
      "7417\n",
      "7418\n",
      "7419\n",
      "7420\n",
      "7421\n",
      "7422\n",
      "7423\n",
      "7424\n",
      "7425\n",
      "7426\n",
      "7427\n",
      "7428\n",
      "7429\n",
      "7430\n",
      "7431\n",
      "7432\n",
      "7433\n",
      "7434\n",
      "7435\n",
      "7436\n",
      "7437\n",
      "7438\n",
      "7439\n",
      "7440\n",
      "7441\n",
      "7442\n",
      "7443\n",
      "7444\n",
      "7445\n",
      "7446\n",
      "7447\n",
      "7448\n",
      "7449\n",
      "7450\n",
      "7451\n",
      "7452\n",
      "7453\n",
      "7454\n",
      "7455\n",
      "7456\n",
      "7457\n",
      "7458\n",
      "7459\n",
      "7460\n",
      "7461\n",
      "7462\n",
      "7463\n",
      "7464\n",
      "7465\n",
      "7466\n",
      "7467\n",
      "7468\n",
      "7469\n",
      "7470\n",
      "7471\n",
      "7472\n",
      "7473\n",
      "7474\n",
      "7475\n",
      "7476\n",
      "7477\n",
      "7478\n",
      "7479\n",
      "7480\n",
      "7481\n",
      "7482\n",
      "7483\n",
      "7484\n",
      "7485\n",
      "7486\n",
      "7487\n",
      "7488\n",
      "7489\n",
      "7490\n",
      "7491\n",
      "7492\n",
      "7493\n",
      "7494\n",
      "7495\n",
      "7496\n",
      "7497\n",
      "7498\n",
      "7499\n",
      "7500\n",
      "7501\n",
      "7502\n",
      "7503\n",
      "7504\n",
      "7505\n",
      "7506\n",
      "7507\n",
      "7508\n",
      "7509\n",
      "7510\n",
      "7511\n",
      "7512\n",
      "7513\n",
      "7514\n",
      "7515\n",
      "7516\n",
      "7517\n",
      "7518\n",
      "7519\n",
      "7520\n",
      "7521\n",
      "7522\n",
      "7523\n",
      "7524\n",
      "7525\n",
      "7526\n",
      "7527\n",
      "7528\n",
      "7529\n",
      "7530\n",
      "7531\n",
      "7532\n",
      "7533\n",
      "7534\n",
      "7535\n",
      "7536\n",
      "7537\n",
      "7538\n",
      "7539\n",
      "7540\n",
      "7541\n",
      "7542\n",
      "7543\n",
      "7544\n",
      "7545\n",
      "7546\n",
      "7547\n",
      "7548\n",
      "7549\n",
      "7550\n",
      "7551\n",
      "7552\n",
      "7553\n",
      "7554\n",
      "7555\n",
      "7556\n",
      "7557\n",
      "7558\n",
      "7559\n",
      "7560\n",
      "7561\n",
      "7562\n",
      "7563\n",
      "7564\n",
      "7565\n",
      "7566\n",
      "7567\n",
      "7568\n",
      "7569\n",
      "7570\n",
      "7571\n",
      "7572\n",
      "7573\n",
      "7574\n",
      "7575\n",
      "7576\n",
      "7577\n",
      "7578\n",
      "7579\n",
      "7580\n",
      "7581\n",
      "7582\n",
      "7583\n",
      "7584\n",
      "7585\n",
      "7586\n",
      "7587\n",
      "7588\n",
      "7589\n",
      "7590\n",
      "7591\n",
      "7592\n",
      "7593\n",
      "7594\n",
      "7595\n",
      "7596\n",
      "7597\n",
      "7598\n",
      "7599\n",
      "7600\n",
      "7601\n",
      "7602\n",
      "7603\n",
      "7604\n",
      "7605\n",
      "7606\n",
      "7607\n",
      "7608\n",
      "7609\n",
      "7610\n",
      "7611\n",
      "7612\n",
      "7613\n",
      "7614\n",
      "7615\n",
      "7616\n",
      "7617\n",
      "7618\n",
      "7619\n",
      "7620\n",
      "7621\n",
      "7622\n",
      "7623\n",
      "7624\n",
      "7625\n",
      "7626\n",
      "7627\n",
      "7628\n",
      "7629\n",
      "7630\n",
      "7631\n",
      "7632\n",
      "7633\n",
      "7634\n",
      "7635\n",
      "7636\n",
      "7637\n",
      "7638\n",
      "7639\n",
      "7640\n",
      "7641\n",
      "7642\n",
      "7643\n",
      "7644\n",
      "7645\n",
      "7646\n",
      "7647\n",
      "7648\n",
      "7649\n",
      "7650\n",
      "7651\n",
      "7652\n",
      "7653\n",
      "7654\n",
      "7655\n",
      "7656\n",
      "7657\n",
      "7658\n",
      "7659\n",
      "7660\n",
      "7661\n",
      "7662\n",
      "7663\n",
      "7664\n",
      "7665\n",
      "7666\n",
      "7667\n",
      "7668\n",
      "7669\n",
      "7670\n",
      "7671\n",
      "7672\n",
      "7673\n",
      "7674\n",
      "7675\n",
      "7676\n",
      "7677\n",
      "7678\n",
      "7679\n",
      "7680\n",
      "7681\n",
      "7682\n",
      "7683\n",
      "7684\n",
      "7685\n",
      "7686\n",
      "7687\n",
      "7688\n",
      "7689\n",
      "7690\n",
      "7691\n",
      "7692\n",
      "7693\n",
      "7694\n",
      "7695\n",
      "7696\n",
      "7697\n",
      "7698\n",
      "7699\n",
      "7700\n",
      "7701\n",
      "7702\n",
      "7703\n",
      "7704\n",
      "7705\n",
      "7706\n",
      "7707\n",
      "7708\n",
      "7709\n",
      "7710\n",
      "7711\n",
      "7712\n",
      "7713\n",
      "7714\n",
      "7715\n",
      "7716\n",
      "7717\n",
      "7718\n",
      "7719\n",
      "7720\n",
      "7721\n",
      "7722\n",
      "7723\n",
      "7724\n",
      "7725\n",
      "7726\n",
      "7727\n",
      "7728\n",
      "7729\n",
      "7730\n",
      "7731\n",
      "7732\n",
      "7733\n",
      "7734\n",
      "7735\n",
      "7736\n",
      "7737\n",
      "7738\n",
      "7739\n",
      "7740\n",
      "7741\n",
      "7742\n",
      "7743\n",
      "7744\n",
      "7745\n",
      "7746\n",
      "7747\n",
      "7748\n",
      "7749\n",
      "7750\n",
      "7751\n",
      "7752\n",
      "7753\n",
      "7754\n",
      "7755\n",
      "7756\n",
      "7757\n",
      "7758\n",
      "7759\n",
      "7760\n",
      "7761\n",
      "7762\n",
      "7763\n",
      "7764\n",
      "7765\n",
      "7766\n",
      "7767\n",
      "7768\n",
      "7769\n",
      "7770\n",
      "7771\n",
      "7772\n",
      "7773\n",
      "7774\n",
      "7775\n",
      "7776\n",
      "7777\n",
      "7778\n",
      "7779\n",
      "7780\n",
      "7781\n",
      "7782\n",
      "7783\n",
      "7784\n",
      "7785\n",
      "7786\n",
      "7787\n",
      "7788\n",
      "7789\n",
      "7790\n",
      "7791\n",
      "7792\n",
      "7793\n",
      "7794\n",
      "7795\n",
      "7796\n",
      "7797\n",
      "7798\n",
      "7799\n",
      "7800\n",
      "7801\n",
      "7802\n",
      "7803\n",
      "7804\n",
      "7805\n",
      "7806\n",
      "7807\n",
      "7808\n",
      "7809\n",
      "7810\n",
      "7811\n",
      "7812\n",
      "7813\n",
      "7814\n",
      "7815\n",
      "7816\n",
      "7817\n",
      "7818\n",
      "7819\n",
      "7820\n",
      "7821\n",
      "7822\n",
      "7823\n",
      "7824\n",
      "7825\n",
      "7826\n",
      "7827\n",
      "7828\n",
      "7829\n",
      "7830\n",
      "7831\n",
      "7832\n",
      "7833\n",
      "7834\n",
      "7835\n",
      "7836\n",
      "7837\n",
      "7838\n",
      "7839\n",
      "7840\n",
      "7841\n",
      "7842\n",
      "7843\n",
      "7844\n",
      "7845\n",
      "7846\n",
      "7847\n",
      "7848\n",
      "7849\n",
      "7850\n",
      "7851\n",
      "7852\n",
      "7853\n",
      "7854\n",
      "7855\n",
      "7856\n",
      "7857\n",
      "7858\n",
      "7859\n",
      "7860\n",
      "7861\n",
      "7862\n",
      "7863\n",
      "7864\n",
      "7865\n",
      "7866\n",
      "7867\n",
      "7868\n",
      "7869\n",
      "7870\n",
      "7871\n",
      "7872\n",
      "7873\n",
      "7874\n",
      "7875\n",
      "7876\n",
      "7877\n",
      "7878\n",
      "7879\n",
      "7880\n",
      "7881\n",
      "7882\n",
      "7883\n",
      "7884\n",
      "7885\n",
      "7886\n",
      "7887\n",
      "7888\n",
      "7889\n",
      "7890\n",
      "7891\n",
      "7892\n",
      "7893\n",
      "7894\n",
      "7895\n",
      "7896\n",
      "7897\n",
      "7898\n",
      "7899\n",
      "7900\n",
      "7901\n",
      "7902\n",
      "7903\n",
      "7904\n",
      "7905\n",
      "7906\n",
      "7907\n",
      "7908\n",
      "7909\n",
      "7910\n",
      "7911\n",
      "7912\n",
      "7913\n",
      "7914\n",
      "7915\n",
      "7916\n",
      "7917\n",
      "7918\n",
      "7919\n",
      "7920\n",
      "7921\n",
      "7922\n",
      "7923\n",
      "7924\n",
      "7925\n",
      "7926\n",
      "7927\n",
      "7928\n",
      "7929\n",
      "7930\n",
      "7931\n",
      "7932\n",
      "7933\n",
      "7934\n",
      "7935\n",
      "7936\n",
      "7937\n",
      "7938\n",
      "7939\n",
      "7940\n",
      "7941\n",
      "7942\n",
      "7943\n",
      "7944\n",
      "7945\n",
      "7946\n",
      "7947\n",
      "7948\n",
      "7949\n",
      "7950\n",
      "7951\n",
      "7952\n",
      "7953\n",
      "7954\n",
      "7955\n",
      "7956\n",
      "7957\n",
      "7958\n",
      "7959\n",
      "7960\n",
      "7961\n",
      "7962\n",
      "7963\n",
      "7964\n",
      "7965\n",
      "7966\n",
      "7967\n",
      "7968\n",
      "7969\n",
      "7970\n",
      "7971\n",
      "7972\n",
      "7973\n",
      "7974\n",
      "7975\n",
      "7976\n",
      "7977\n",
      "7978\n",
      "7979\n",
      "7980\n",
      "7981\n",
      "7982\n",
      "7983\n",
      "7984\n",
      "7985\n",
      "7986\n",
      "7987\n",
      "7988\n",
      "7989\n",
      "7990\n",
      "7991\n",
      "7992\n",
      "7993\n",
      "7994\n",
      "7995\n",
      "7996\n",
      "7997\n",
      "7998\n",
      "7999\n",
      "8000\n",
      "8001\n",
      "8002\n",
      "8003\n",
      "8004\n",
      "8005\n",
      "8006\n",
      "8007\n",
      "8008\n",
      "8009\n",
      "8010\n",
      "8011\n",
      "8012\n",
      "8013\n",
      "8014\n",
      "8015\n",
      "8016\n",
      "8017\n",
      "8018\n",
      "8019\n",
      "8020\n",
      "8021\n",
      "8022\n",
      "8023\n",
      "8024\n",
      "8025\n",
      "8026\n",
      "8027\n",
      "8028\n",
      "8029\n",
      "8030\n",
      "8031\n",
      "8032\n",
      "8033\n",
      "8034\n",
      "8035\n",
      "8036\n",
      "8037\n",
      "8038\n",
      "8039\n",
      "8040\n",
      "8041\n",
      "8042\n",
      "8043\n",
      "8044\n",
      "8045\n",
      "8046\n",
      "8047\n",
      "8048\n",
      "8049\n",
      "8050\n",
      "8051\n",
      "8052\n",
      "8053\n",
      "8054\n",
      "8055\n",
      "8056\n",
      "8057\n",
      "8058\n",
      "8059\n",
      "8060\n",
      "8061\n",
      "8062\n",
      "8063\n",
      "8064\n",
      "8065\n",
      "8066\n",
      "8067\n",
      "8068\n",
      "8069\n",
      "8070\n",
      "8071\n",
      "8072\n",
      "8073\n",
      "8074\n",
      "8075\n",
      "8076\n",
      "8077\n",
      "8078\n",
      "8079\n",
      "8080\n",
      "8081\n",
      "8082\n",
      "8083\n",
      "8084\n",
      "8085\n",
      "8086\n",
      "8087\n",
      "8088\n",
      "8089\n",
      "8090\n",
      "8091\n",
      "8092\n",
      "8093\n",
      "8094\n",
      "8095\n",
      "8096\n",
      "8097\n",
      "8098\n",
      "8099\n",
      "8100\n",
      "8101\n",
      "8102\n",
      "8103\n",
      "8104\n",
      "8105\n",
      "8106\n",
      "8107\n",
      "8108\n",
      "8109\n",
      "8110\n",
      "8111\n",
      "8112\n",
      "8113\n",
      "8114\n",
      "8115\n",
      "8116\n",
      "8117\n",
      "8118\n",
      "8119\n",
      "8120\n",
      "8121\n",
      "8122\n",
      "8123\n",
      "8124\n",
      "8125\n",
      "8126\n",
      "8127\n",
      "8128\n",
      "8129\n",
      "8130\n",
      "8131\n",
      "8132\n",
      "8133\n",
      "8134\n",
      "8135\n",
      "8136\n",
      "8137\n",
      "8138\n",
      "8139\n",
      "8140\n",
      "8141\n",
      "8142\n",
      "8143\n",
      "8144\n",
      "8145\n",
      "8146\n",
      "8147\n",
      "8148\n",
      "8149\n",
      "8150\n",
      "8151\n",
      "8152\n",
      "8153\n",
      "8154\n",
      "8155\n",
      "8156\n",
      "8157\n",
      "8158\n",
      "8159\n",
      "8160\n",
      "8161\n",
      "8162\n",
      "8163\n",
      "8164\n",
      "8165\n",
      "8166\n",
      "8167\n",
      "8168\n",
      "8169\n",
      "8170\n",
      "8171\n",
      "8172\n",
      "8173\n",
      "8174\n",
      "8175\n",
      "8176\n",
      "8177\n",
      "8178\n",
      "8179\n",
      "8180\n",
      "8181\n",
      "8182\n",
      "8183\n",
      "8184\n",
      "8185\n",
      "8186\n",
      "8187\n",
      "8188\n",
      "8189\n",
      "8190\n",
      "8191\n",
      "8192\n",
      "8193\n",
      "8194\n",
      "8195\n",
      "8196\n",
      "8197\n",
      "8198\n",
      "8199\n",
      "8200\n",
      "8201\n",
      "8202\n",
      "8203\n",
      "8204\n",
      "8205\n",
      "8206\n",
      "8207\n",
      "8208\n",
      "8209\n",
      "8210\n",
      "8211\n",
      "8212\n",
      "8213\n",
      "8214\n",
      "8215\n",
      "8216\n",
      "8217\n",
      "8218\n",
      "8219\n",
      "8220\n",
      "8221\n",
      "8222\n",
      "8223\n",
      "8224\n",
      "8225\n",
      "8226\n",
      "8227\n",
      "8228\n",
      "8229\n",
      "8230\n",
      "8231\n",
      "8232\n",
      "8233\n",
      "8234\n",
      "8235\n",
      "8236\n",
      "8237\n",
      "8238\n",
      "8239\n",
      "8240\n",
      "8241\n",
      "8242\n",
      "8243\n",
      "8244\n",
      "8245\n",
      "8246\n",
      "8247\n",
      "8248\n",
      "8249\n",
      "8250\n",
      "8251\n",
      "8252\n",
      "8253\n",
      "8254\n",
      "8255\n",
      "8256\n",
      "8257\n",
      "8258\n",
      "8259\n",
      "8260\n",
      "8261\n",
      "8262\n",
      "8263\n",
      "8264\n",
      "8265\n",
      "8266\n",
      "8267\n",
      "8268\n",
      "8269\n",
      "8270\n",
      "8271\n",
      "8272\n",
      "8273\n",
      "8274\n",
      "8275\n",
      "8276\n",
      "8277\n",
      "8278\n",
      "8279\n",
      "8280\n",
      "8281\n",
      "8282\n",
      "8283\n",
      "8284\n",
      "8285\n",
      "8286\n",
      "8287\n",
      "8288\n",
      "8289\n",
      "8290\n",
      "8291\n",
      "8292\n",
      "8293\n",
      "8294\n",
      "8295\n",
      "8296\n",
      "8297\n",
      "8298\n",
      "8299\n",
      "8300\n",
      "8301\n",
      "8302\n",
      "8303\n",
      "8304\n",
      "8305\n",
      "8306\n",
      "8307\n",
      "8308\n",
      "8309\n",
      "8310\n",
      "8311\n",
      "8312\n",
      "8313\n",
      "8314\n",
      "8315\n",
      "8316\n",
      "8317\n",
      "8318\n",
      "8319\n",
      "8320\n",
      "8321\n",
      "8322\n",
      "8323\n",
      "8324\n",
      "8325\n",
      "8326\n",
      "8327\n",
      "8328\n",
      "8329\n",
      "8330\n",
      "8331\n",
      "8332\n",
      "8333\n",
      "8334\n",
      "8335\n",
      "8336\n",
      "8337\n",
      "8338\n",
      "8339\n",
      "8340\n",
      "8341\n",
      "8342\n",
      "8343\n",
      "8344\n",
      "8345\n",
      "8346\n",
      "8347\n",
      "8348\n",
      "8349\n",
      "8350\n",
      "8351\n",
      "8352\n",
      "8353\n",
      "8354\n",
      "8355\n",
      "8356\n",
      "8357\n",
      "8358\n",
      "8359\n",
      "8360\n",
      "8361\n",
      "8362\n",
      "8363\n",
      "8364\n",
      "8365\n",
      "8366\n",
      "8367\n",
      "8368\n",
      "8369\n",
      "8370\n",
      "8371\n",
      "8372\n",
      "8373\n",
      "8374\n",
      "8375\n",
      "8376\n",
      "8377\n",
      "8378\n",
      "8379\n",
      "8380\n",
      "8381\n",
      "8382\n",
      "8383\n",
      "8384\n",
      "8385\n",
      "8386\n",
      "8387\n",
      "8388\n",
      "8389\n",
      "8390\n",
      "8391\n",
      "8392\n",
      "8393\n",
      "8394\n",
      "8395\n",
      "8396\n",
      "8397\n",
      "8398\n",
      "8399\n",
      "8400\n",
      "8401\n",
      "8402\n",
      "8403\n",
      "8404\n",
      "8405\n",
      "8406\n",
      "8407\n",
      "8408\n",
      "8409\n",
      "8410\n",
      "8411\n",
      "8412\n",
      "8413\n",
      "8414\n",
      "8415\n",
      "8416\n",
      "8417\n",
      "8418\n",
      "8419\n",
      "8420\n",
      "8421\n",
      "8422\n",
      "8423\n",
      "8424\n",
      "8425\n",
      "8426\n",
      "8427\n",
      "8428\n",
      "8429\n",
      "8430\n",
      "8431\n",
      "8432\n",
      "8433\n",
      "8434\n",
      "8435\n",
      "8436\n",
      "8437\n",
      "8438\n",
      "8439\n",
      "8440\n",
      "8441\n",
      "8442\n",
      "8443\n",
      "8444\n",
      "8445\n",
      "8446\n",
      "8447\n",
      "8448\n",
      "8449\n",
      "8450\n",
      "8451\n",
      "8452\n",
      "8453\n",
      "8454\n",
      "8455\n",
      "8456\n",
      "8457\n",
      "8458\n",
      "8459\n",
      "8460\n",
      "8461\n",
      "8462\n",
      "8463\n",
      "8464\n",
      "8465\n",
      "8466\n",
      "8467\n",
      "8468\n",
      "8469\n",
      "8470\n",
      "8471\n",
      "8472\n",
      "8473\n",
      "8474\n",
      "8475\n",
      "8476\n",
      "8477\n",
      "8478\n",
      "8479\n",
      "8480\n",
      "8481\n",
      "8482\n",
      "8483\n",
      "8484\n",
      "8485\n",
      "8486\n",
      "8487\n",
      "8488\n",
      "8489\n",
      "8490\n",
      "8491\n",
      "8492\n",
      "8493\n",
      "8494\n",
      "8495\n",
      "8496\n",
      "8497\n",
      "8498\n",
      "8499\n",
      "8500\n",
      "8501\n",
      "8502\n",
      "8503\n",
      "8504\n",
      "8505\n",
      "8506\n",
      "8507\n",
      "8508\n",
      "8509\n",
      "8510\n",
      "8511\n",
      "8512\n",
      "8513\n",
      "8514\n",
      "8515\n",
      "8516\n",
      "8517\n",
      "8518\n",
      "8519\n",
      "8520\n",
      "8521\n",
      "8522\n",
      "8523\n",
      "8524\n",
      "8525\n",
      "8526\n",
      "8527\n",
      "8528\n",
      "8529\n",
      "8530\n",
      "8531\n",
      "8532\n",
      "8533\n",
      "8534\n",
      "8535\n",
      "8536\n",
      "8537\n",
      "8538\n",
      "8539\n",
      "8540\n",
      "8541\n",
      "8542\n",
      "8543\n",
      "8544\n",
      "8545\n",
      "8546\n",
      "8547\n",
      "8548\n",
      "8549\n",
      "8550\n",
      "8551\n",
      "8552\n",
      "8553\n",
      "8554\n",
      "8555\n",
      "8556\n",
      "8557\n",
      "8558\n",
      "8559\n",
      "8560\n",
      "8561\n",
      "8562\n",
      "8563\n",
      "8564\n",
      "8565\n",
      "8566\n",
      "8567\n",
      "8568\n",
      "8569\n",
      "8570\n",
      "8571\n",
      "8572\n",
      "8573\n",
      "8574\n",
      "8575\n",
      "8576\n",
      "8577\n",
      "8578\n",
      "8579\n",
      "8580\n",
      "8581\n",
      "8582\n",
      "8583\n",
      "8584\n",
      "8585\n",
      "8586\n",
      "8587\n",
      "8588\n",
      "8589\n",
      "8590\n",
      "8591\n",
      "8592\n",
      "8593\n",
      "8594\n",
      "8595\n",
      "8596\n",
      "8597\n",
      "8598\n",
      "8599\n",
      "8600\n",
      "8601\n",
      "8602\n",
      "8603\n",
      "8604\n",
      "8605\n",
      "8606\n",
      "8607\n",
      "8608\n",
      "8609\n",
      "8610\n",
      "8611\n",
      "8612\n",
      "8613\n",
      "8614\n",
      "8615\n",
      "8616\n",
      "8617\n",
      "8618\n",
      "8619\n",
      "8620\n",
      "8621\n",
      "8622\n",
      "8623\n",
      "8624\n",
      "8625\n",
      "8626\n",
      "8627\n",
      "8628\n",
      "8629\n",
      "8630\n",
      "8631\n",
      "8632\n",
      "8633\n",
      "8634\n",
      "8635\n",
      "8636\n",
      "8637\n",
      "8638\n",
      "8639\n",
      "8640\n",
      "8641\n",
      "8642\n",
      "8643\n",
      "8644\n",
      "8645\n",
      "8646\n",
      "8647\n",
      "8648\n",
      "8649\n",
      "8650\n",
      "8651\n",
      "8652\n",
      "8653\n",
      "8654\n",
      "8655\n",
      "8656\n",
      "8657\n",
      "8658\n",
      "8659\n",
      "8660\n",
      "8661\n",
      "8662\n",
      "8663\n",
      "8664\n",
      "8665\n",
      "8666\n",
      "8667\n",
      "8668\n",
      "8669\n",
      "8670\n",
      "8671\n",
      "8672\n",
      "8673\n",
      "8674\n",
      "8675\n",
      "8676\n",
      "8677\n",
      "8678\n",
      "8679\n",
      "8680\n",
      "8681\n",
      "8682\n",
      "8683\n",
      "8684\n",
      "8685\n",
      "8686\n",
      "8687\n",
      "8688\n",
      "8689\n",
      "8690\n",
      "8691\n",
      "8692\n",
      "8693\n",
      "8694\n",
      "8695\n",
      "8696\n",
      "8697\n",
      "8698\n",
      "8699\n",
      "8700\n",
      "8701\n",
      "8702\n",
      "8703\n",
      "8704\n",
      "8705\n",
      "8706\n",
      "8707\n",
      "8708\n",
      "8709\n",
      "8710\n",
      "8711\n",
      "8712\n",
      "8713\n",
      "8714\n",
      "8715\n",
      "8716\n",
      "8717\n",
      "8718\n",
      "8719\n",
      "8720\n",
      "8721\n",
      "8722\n",
      "8723\n",
      "8724\n",
      "8725\n",
      "8726\n",
      "8727\n",
      "8728\n",
      "8729\n",
      "8730\n",
      "8731\n",
      "8732\n",
      "8733\n",
      "8734\n",
      "8735\n",
      "8736\n",
      "8737\n",
      "8738\n",
      "8739\n",
      "8740\n",
      "8741\n",
      "8742\n",
      "8743\n",
      "8744\n",
      "8745\n",
      "8746\n",
      "8747\n",
      "8748\n",
      "8749\n",
      "8750\n",
      "8751\n",
      "8752\n",
      "8753\n",
      "8754\n",
      "8755\n",
      "8756\n",
      "8757\n",
      "8758\n",
      "8759\n",
      "8760\n",
      "8761\n",
      "8762\n",
      "8763\n",
      "8764\n",
      "8765\n",
      "8766\n",
      "8767\n",
      "8768\n",
      "8769\n",
      "8770\n",
      "8771\n",
      "8772\n",
      "8773\n",
      "8774\n",
      "8775\n",
      "8776\n",
      "8777\n",
      "8778\n",
      "8779\n",
      "8780\n",
      "8781\n",
      "8782\n",
      "8783\n",
      "8784\n",
      "8785\n",
      "8786\n",
      "8787\n",
      "8788\n",
      "8789\n",
      "8790\n",
      "8791\n",
      "8792\n",
      "8793\n",
      "8794\n",
      "8795\n",
      "8796\n",
      "8797\n",
      "8798\n",
      "8799\n",
      "8800\n",
      "8801\n",
      "8802\n",
      "8803\n",
      "8804\n",
      "8805\n",
      "8806\n",
      "8807\n",
      "8808\n",
      "8809\n",
      "8810\n",
      "8811\n",
      "8812\n",
      "8813\n",
      "8814\n",
      "8815\n",
      "8816\n",
      "8817\n",
      "8818\n",
      "8819\n",
      "8820\n",
      "8821\n",
      "8822\n",
      "8823\n",
      "8824\n",
      "8825\n",
      "8826\n",
      "8827\n",
      "8828\n",
      "8829\n",
      "8830\n",
      "8831\n",
      "8832\n",
      "8833\n",
      "8834\n",
      "8835\n",
      "8836\n",
      "8837\n",
      "8838\n",
      "8839\n",
      "8840\n",
      "8841\n",
      "8842\n",
      "8843\n",
      "8844\n",
      "8845\n",
      "8846\n",
      "8847\n",
      "8848\n",
      "8849\n",
      "8850\n",
      "8851\n",
      "8852\n",
      "8853\n",
      "8854\n",
      "8855\n",
      "8856\n",
      "8857\n",
      "8858\n",
      "8859\n",
      "8860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch_geometric/utils/convert.py:192: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  data[key] = torch.tensor(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "4068\n",
      "4069\n",
      "4070\n",
      "4071\n",
      "4072\n",
      "4073\n",
      "4074\n",
      "4075\n",
      "4076\n",
      "4077\n",
      "4078\n",
      "4079\n",
      "4080\n",
      "4081\n",
      "4082\n",
      "4083\n",
      "4084\n",
      "4085\n",
      "4086\n",
      "4087\n",
      "4088\n",
      "4089\n",
      "4090\n",
      "4091\n",
      "4092\n",
      "4093\n",
      "4094\n",
      "4095\n",
      "4096\n",
      "4097\n",
      "4098\n",
      "4099\n",
      "4100\n",
      "4101\n",
      "4102\n",
      "4103\n",
      "4104\n",
      "4105\n",
      "4106\n",
      "4107\n",
      "4108\n",
      "4109\n",
      "4110\n",
      "4111\n",
      "4112\n",
      "4113\n",
      "4114\n",
      "4115\n",
      "4116\n",
      "4117\n",
      "4118\n",
      "4119\n",
      "4120\n",
      "4121\n",
      "4122\n",
      "4123\n",
      "4124\n",
      "4125\n",
      "4126\n",
      "4127\n",
      "4128\n",
      "4129\n",
      "4130\n",
      "4131\n",
      "4132\n",
      "4133\n",
      "4134\n",
      "4135\n",
      "4136\n",
      "4137\n",
      "4138\n",
      "4139\n",
      "4140\n",
      "4141\n",
      "4142\n",
      "4143\n",
      "4144\n",
      "4145\n",
      "4146\n",
      "4147\n",
      "4148\n",
      "4149\n",
      "4150\n",
      "4151\n",
      "4152\n",
      "4153\n",
      "4154\n",
      "4155\n",
      "4156\n",
      "4157\n",
      "4158\n",
      "4159\n",
      "4160\n",
      "4161\n",
      "4162\n",
      "4163\n",
      "4164\n",
      "4165\n",
      "4166\n",
      "4167\n",
      "4168\n",
      "4169\n",
      "4170\n",
      "4171\n",
      "4172\n",
      "4173\n",
      "4174\n",
      "4175\n",
      "4176\n",
      "4177\n",
      "4178\n",
      "4179\n",
      "4180\n",
      "4181\n",
      "4182\n",
      "4183\n",
      "4184\n",
      "4185\n",
      "4186\n",
      "4187\n",
      "4188\n",
      "4189\n",
      "4190\n",
      "4191\n",
      "4192\n",
      "4193\n",
      "4194\n",
      "4195\n",
      "4196\n",
      "4197\n",
      "4198\n",
      "4199\n",
      "4200\n",
      "4201\n",
      "4202\n",
      "4203\n",
      "4204\n",
      "4205\n",
      "4206\n",
      "4207\n",
      "4208\n",
      "4209\n",
      "4210\n",
      "4211\n",
      "4212\n",
      "4213\n",
      "4214\n",
      "4215\n",
      "4216\n",
      "4217\n",
      "4218\n",
      "4219\n",
      "4220\n",
      "4221\n",
      "4222\n",
      "4223\n",
      "4224\n",
      "4225\n",
      "4226\n",
      "4227\n",
      "4228\n",
      "4229\n",
      "4230\n",
      "4231\n",
      "4232\n",
      "4233\n",
      "4234\n",
      "4235\n",
      "4236\n",
      "4237\n",
      "4238\n",
      "4239\n",
      "4240\n",
      "4241\n",
      "4242\n",
      "4243\n",
      "4244\n",
      "4245\n",
      "4246\n",
      "4247\n",
      "4248\n",
      "4249\n",
      "4250\n",
      "4251\n",
      "4252\n",
      "4253\n",
      "4254\n",
      "4255\n",
      "4256\n",
      "4257\n",
      "4258\n",
      "4259\n",
      "4260\n",
      "4261\n",
      "4262\n",
      "4263\n",
      "4264\n",
      "4265\n",
      "4266\n",
      "4267\n",
      "4268\n",
      "4269\n",
      "4270\n",
      "4271\n",
      "4272\n",
      "4273\n",
      "4274\n",
      "4275\n",
      "4276\n",
      "4277\n",
      "4278\n",
      "4279\n",
      "4280\n",
      "4281\n",
      "4282\n",
      "4283\n",
      "4284\n",
      "4285\n",
      "4286\n",
      "4287\n",
      "4288\n",
      "4289\n",
      "4290\n",
      "4291\n",
      "4292\n",
      "4293\n",
      "4294\n",
      "4295\n",
      "4296\n",
      "4297\n",
      "4298\n",
      "4299\n",
      "4300\n",
      "4301\n",
      "4302\n",
      "4303\n",
      "4304\n",
      "4305\n",
      "4306\n",
      "4307\n",
      "4308\n",
      "4309\n",
      "4310\n",
      "4311\n",
      "4312\n",
      "4313\n",
      "4314\n",
      "4315\n",
      "4316\n",
      "4317\n",
      "4318\n",
      "4319\n",
      "4320\n",
      "4321\n",
      "4322\n",
      "4323\n",
      "4324\n",
      "4325\n",
      "4326\n",
      "4327\n",
      "4328\n",
      "4329\n",
      "4330\n",
      "4331\n",
      "4332\n",
      "4333\n",
      "4334\n",
      "4335\n",
      "4336\n",
      "4337\n",
      "4338\n",
      "4339\n",
      "4340\n",
      "4341\n",
      "4342\n",
      "4343\n",
      "4344\n",
      "4345\n",
      "4346\n",
      "4347\n",
      "4348\n",
      "4349\n",
      "4350\n",
      "4351\n",
      "4352\n",
      "4353\n",
      "4354\n",
      "4355\n",
      "4356\n",
      "4357\n",
      "4358\n",
      "4359\n",
      "4360\n",
      "4361\n",
      "4362\n",
      "4363\n",
      "4364\n",
      "4365\n",
      "4366\n",
      "4367\n",
      "4368\n",
      "4369\n",
      "4370\n",
      "4371\n",
      "4372\n",
      "4373\n",
      "4374\n",
      "4375\n",
      "4376\n",
      "4377\n",
      "4378\n",
      "4379\n",
      "4380\n",
      "4381\n",
      "4382\n",
      "4383\n",
      "4384\n",
      "4385\n",
      "4386\n",
      "4387\n",
      "4388\n",
      "4389\n",
      "4390\n",
      "4391\n",
      "4392\n",
      "4393\n",
      "4394\n",
      "4395\n",
      "4396\n",
      "4397\n",
      "4398\n",
      "4399\n",
      "4400\n",
      "4401\n",
      "4402\n",
      "4403\n",
      "4404\n",
      "4405\n",
      "4406\n",
      "4407\n",
      "4408\n",
      "4409\n",
      "4410\n",
      "4411\n",
      "4412\n",
      "4413\n",
      "4414\n",
      "4415\n",
      "4416\n",
      "4417\n",
      "4418\n",
      "4419\n",
      "4420\n",
      "4421\n",
      "4422\n",
      "4423\n",
      "4424\n",
      "4425\n",
      "4426\n",
      "4427\n",
      "4428\n",
      "4429\n",
      "4430\n",
      "4431\n",
      "4432\n",
      "4433\n",
      "4434\n",
      "4435\n",
      "4436\n",
      "4437\n",
      "4438\n",
      "4439\n",
      "4440\n",
      "4441\n",
      "4442\n",
      "4443\n",
      "4444\n",
      "4445\n",
      "4446\n",
      "4447\n",
      "4448\n",
      "4449\n",
      "4450\n",
      "4451\n",
      "4452\n",
      "4453\n",
      "4454\n",
      "4455\n",
      "4456\n",
      "4457\n",
      "4458\n",
      "4459\n",
      "4460\n",
      "4461\n",
      "4462\n",
      "4463\n",
      "4464\n",
      "4465\n",
      "4466\n",
      "4467\n",
      "4468\n",
      "4469\n",
      "4470\n",
      "4471\n",
      "4472\n",
      "4473\n",
      "4474\n",
      "4475\n",
      "4476\n",
      "4477\n",
      "4478\n",
      "4479\n",
      "4480\n",
      "4481\n",
      "4482\n",
      "4483\n",
      "4484\n",
      "4485\n",
      "4486\n",
      "4487\n",
      "4488\n",
      "4489\n",
      "4490\n",
      "4491\n",
      "4492\n",
      "4493\n",
      "4494\n",
      "4495\n",
      "4496\n",
      "4497\n",
      "4498\n",
      "4499\n",
      "4500\n",
      "4501\n",
      "4502\n",
      "4503\n",
      "4504\n",
      "4505\n",
      "4506\n",
      "4507\n",
      "4508\n",
      "4509\n",
      "4510\n",
      "4511\n",
      "4512\n",
      "4513\n",
      "4514\n",
      "4515\n",
      "4516\n",
      "4517\n",
      "4518\n",
      "4519\n",
      "4520\n",
      "4521\n",
      "4522\n",
      "4523\n",
      "4524\n",
      "4525\n",
      "4526\n",
      "4527\n",
      "4528\n",
      "4529\n",
      "4530\n",
      "4531\n",
      "4532\n",
      "4533\n",
      "4534\n",
      "4535\n",
      "4536\n",
      "4537\n",
      "4538\n",
      "4539\n",
      "4540\n",
      "4541\n",
      "4542\n",
      "4543\n",
      "4544\n",
      "4545\n",
      "4546\n",
      "4547\n",
      "4548\n",
      "4549\n",
      "4550\n",
      "4551\n",
      "4552\n",
      "4553\n",
      "4554\n",
      "4555\n",
      "4556\n",
      "4557\n",
      "4558\n",
      "4559\n",
      "4560\n",
      "4561\n",
      "4562\n",
      "4563\n",
      "4564\n",
      "4565\n",
      "4566\n",
      "4567\n",
      "4568\n",
      "4569\n",
      "4570\n",
      "4571\n",
      "4572\n",
      "4573\n",
      "4574\n",
      "4575\n",
      "4576\n",
      "4577\n",
      "4578\n",
      "4579\n",
      "4580\n",
      "4581\n",
      "4582\n",
      "4583\n",
      "4584\n",
      "4585\n",
      "4586\n",
      "4587\n",
      "4588\n",
      "4589\n",
      "4590\n",
      "4591\n",
      "4592\n",
      "4593\n",
      "4594\n",
      "4595\n",
      "4596\n",
      "4597\n",
      "4598\n",
      "4599\n",
      "4600\n",
      "4601\n",
      "4602\n",
      "4603\n",
      "4604\n",
      "4605\n",
      "4606\n",
      "4607\n",
      "4608\n",
      "4609\n",
      "4610\n",
      "4611\n",
      "4612\n",
      "4613\n",
      "4614\n",
      "4615\n",
      "4616\n",
      "4617\n",
      "4618\n",
      "4619\n",
      "4620\n",
      "4621\n",
      "4622\n",
      "4623\n",
      "4624\n",
      "4625\n",
      "4626\n",
      "4627\n",
      "4628\n",
      "4629\n",
      "4630\n",
      "4631\n",
      "4632\n",
      "4633\n",
      "4634\n",
      "4635\n",
      "4636\n",
      "4637\n",
      "4638\n",
      "4639\n",
      "4640\n",
      "4641\n",
      "4642\n",
      "4643\n",
      "4644\n",
      "4645\n",
      "4646\n",
      "4647\n",
      "4648\n",
      "4649\n",
      "4650\n",
      "4651\n",
      "4652\n",
      "4653\n",
      "4654\n",
      "4655\n",
      "4656\n",
      "4657\n",
      "4658\n",
      "4659\n",
      "4660\n",
      "4661\n",
      "4662\n",
      "4663\n",
      "4664\n",
      "4665\n",
      "4666\n",
      "4667\n",
      "4668\n",
      "4669\n",
      "4670\n",
      "4671\n",
      "4672\n",
      "4673\n",
      "4674\n",
      "4675\n",
      "4676\n",
      "4677\n",
      "4678\n",
      "4679\n",
      "4680\n",
      "4681\n",
      "4682\n",
      "4683\n",
      "4684\n",
      "4685\n",
      "4686\n",
      "4687\n",
      "4688\n",
      "4689\n",
      "4690\n",
      "4691\n",
      "4692\n",
      "4693\n",
      "4694\n",
      "4695\n",
      "4696\n",
      "4697\n",
      "4698\n",
      "4699\n",
      "4700\n",
      "4701\n",
      "4702\n",
      "4703\n",
      "4704\n",
      "4705\n",
      "4706\n",
      "4707\n",
      "4708\n",
      "4709\n",
      "4710\n",
      "4711\n",
      "4712\n",
      "4713\n",
      "4714\n",
      "4715\n",
      "4716\n",
      "4717\n",
      "4718\n",
      "4719\n",
      "4720\n",
      "4721\n",
      "4722\n",
      "4723\n",
      "4724\n",
      "4725\n",
      "4726\n",
      "4727\n",
      "4728\n",
      "4729\n",
      "4730\n",
      "4731\n",
      "4732\n",
      "4733\n",
      "4734\n",
      "4735\n",
      "4736\n",
      "4737\n",
      "4738\n",
      "4739\n",
      "4740\n",
      "4741\n",
      "4742\n",
      "4743\n",
      "4744\n",
      "4745\n",
      "4746\n",
      "4747\n",
      "4748\n",
      "4749\n",
      "4750\n",
      "4751\n",
      "4752\n",
      "4753\n",
      "4754\n",
      "4755\n",
      "4756\n",
      "4757\n",
      "4758\n",
      "4759\n",
      "4760\n",
      "4761\n",
      "4762\n",
      "4763\n",
      "4764\n",
      "4765\n",
      "4766\n",
      "4767\n",
      "4768\n",
      "4769\n",
      "4770\n",
      "4771\n",
      "4772\n",
      "4773\n",
      "4774\n",
      "4775\n",
      "4776\n",
      "4777\n",
      "4778\n",
      "4779\n",
      "4780\n",
      "4781\n",
      "4782\n",
      "4783\n",
      "4784\n",
      "4785\n",
      "4786\n",
      "4787\n",
      "4788\n",
      "4789\n",
      "4790\n",
      "4791\n",
      "4792\n",
      "4793\n",
      "4794\n",
      "4795\n",
      "4796\n",
      "4797\n",
      "4798\n",
      "4799\n",
      "4800\n",
      "4801\n",
      "4802\n",
      "4803\n",
      "4804\n",
      "4805\n",
      "4806\n",
      "4807\n",
      "4808\n",
      "4809\n",
      "4810\n",
      "4811\n",
      "4812\n",
      "4813\n",
      "4814\n",
      "4815\n",
      "4816\n",
      "4817\n",
      "4818\n",
      "4819\n",
      "4820\n",
      "4821\n",
      "4822\n",
      "4823\n",
      "4824\n",
      "4825\n",
      "4826\n",
      "4827\n",
      "4828\n",
      "4829\n",
      "4830\n",
      "4831\n",
      "4832\n",
      "4833\n",
      "4834\n",
      "4835\n",
      "4836\n",
      "4837\n",
      "4838\n",
      "4839\n",
      "4840\n",
      "4841\n",
      "4842\n",
      "4843\n",
      "4844\n",
      "4845\n",
      "4846\n",
      "4847\n",
      "4848\n",
      "4849\n",
      "4850\n",
      "4851\n",
      "4852\n",
      "4853\n",
      "4854\n",
      "4855\n",
      "4856\n",
      "4857\n",
      "4858\n",
      "4859\n",
      "4860\n",
      "4861\n",
      "4862\n",
      "4863\n",
      "4864\n",
      "4865\n",
      "4866\n",
      "4867\n",
      "4868\n",
      "4869\n",
      "4870\n",
      "4871\n",
      "4872\n",
      "4873\n",
      "4874\n",
      "4875\n",
      "4876\n",
      "4877\n",
      "4878\n",
      "4879\n",
      "4880\n",
      "4881\n",
      "4882\n",
      "4883\n",
      "4884\n",
      "4885\n",
      "4886\n",
      "4887\n",
      "4888\n",
      "4889\n",
      "4890\n",
      "4891\n",
      "4892\n",
      "4893\n",
      "4894\n",
      "4895\n",
      "4896\n",
      "4897\n",
      "4898\n",
      "4899\n",
      "4900\n",
      "4901\n",
      "4902\n",
      "4903\n",
      "4904\n",
      "4905\n",
      "4906\n",
      "4907\n",
      "4908\n",
      "4909\n",
      "4910\n",
      "4911\n",
      "4912\n",
      "4913\n",
      "4914\n",
      "4915\n",
      "4916\n",
      "4917\n",
      "4918\n",
      "4919\n",
      "4920\n",
      "4921\n",
      "4922\n",
      "4923\n",
      "4924\n",
      "4925\n",
      "4926\n",
      "4927\n",
      "4928\n",
      "4929\n",
      "4930\n",
      "4931\n",
      "4932\n",
      "4933\n",
      "4934\n",
      "4935\n",
      "4936\n",
      "4937\n",
      "4938\n",
      "4939\n",
      "4940\n",
      "4941\n",
      "4942\n",
      "4943\n",
      "4944\n",
      "4945\n",
      "4946\n",
      "4947\n",
      "4948\n",
      "4949\n",
      "4950\n",
      "4951\n",
      "4952\n",
      "4953\n",
      "4954\n",
      "4955\n",
      "4956\n",
      "4957\n",
      "4958\n",
      "4959\n",
      "4960\n",
      "4961\n",
      "4962\n",
      "4963\n",
      "4964\n",
      "4965\n",
      "4966\n",
      "4967\n",
      "4968\n",
      "4969\n",
      "4970\n",
      "4971\n",
      "4972\n",
      "4973\n",
      "4974\n",
      "4975\n",
      "4976\n",
      "4977\n",
      "4978\n",
      "4979\n",
      "4980\n",
      "4981\n",
      "4982\n",
      "4983\n",
      "4984\n",
      "4985\n",
      "4986\n",
      "4987\n",
      "4988\n",
      "4989\n",
      "4990\n",
      "4991\n",
      "4992\n",
      "4993\n",
      "4994\n",
      "4995\n",
      "4996\n",
      "4997\n",
      "4998\n",
      "4999\n",
      "5000\n",
      "5001\n",
      "5002\n",
      "5003\n",
      "5004\n",
      "5005\n",
      "5006\n",
      "5007\n",
      "5008\n",
      "5009\n",
      "5010\n",
      "5011\n",
      "5012\n",
      "5013\n",
      "5014\n",
      "5015\n",
      "5016\n",
      "5017\n",
      "5018\n",
      "5019\n",
      "5020\n",
      "5021\n",
      "5022\n",
      "5023\n",
      "5024\n",
      "5025\n",
      "5026\n",
      "5027\n",
      "5028\n",
      "5029\n",
      "5030\n",
      "5031\n",
      "5032\n",
      "5033\n",
      "5034\n",
      "5035\n",
      "5036\n",
      "5037\n",
      "5038\n",
      "5039\n",
      "5040\n",
      "5041\n",
      "5042\n",
      "5043\n",
      "5044\n",
      "5045\n",
      "5046\n",
      "5047\n",
      "5048\n",
      "5049\n",
      "5050\n",
      "5051\n",
      "5052\n",
      "5053\n",
      "5054\n",
      "5055\n",
      "5056\n",
      "5057\n",
      "5058\n",
      "5059\n",
      "5060\n",
      "5061\n",
      "5062\n",
      "5063\n",
      "5064\n",
      "5065\n",
      "5066\n",
      "5067\n",
      "5068\n",
      "5069\n",
      "5070\n",
      "5071\n",
      "5072\n",
      "5073\n",
      "5074\n",
      "5075\n",
      "5076\n",
      "5077\n",
      "5078\n",
      "5079\n",
      "5080\n",
      "5081\n",
      "5082\n",
      "5083\n",
      "5084\n",
      "5085\n",
      "5086\n",
      "5087\n",
      "5088\n",
      "5089\n",
      "5090\n",
      "5091\n",
      "5092\n",
      "5093\n",
      "5094\n",
      "5095\n",
      "5096\n",
      "5097\n",
      "5098\n",
      "5099\n",
      "5100\n",
      "5101\n",
      "5102\n",
      "5103\n",
      "5104\n",
      "5105\n",
      "5106\n",
      "5107\n",
      "5108\n",
      "5109\n",
      "5110\n",
      "5111\n",
      "5112\n",
      "5113\n",
      "5114\n",
      "5115\n",
      "5116\n",
      "5117\n",
      "5118\n",
      "5119\n",
      "5120\n",
      "5121\n",
      "5122\n",
      "5123\n",
      "5124\n",
      "5125\n",
      "5126\n",
      "5127\n",
      "5128\n",
      "5129\n",
      "5130\n",
      "5131\n",
      "5132\n",
      "5133\n",
      "5134\n",
      "5135\n",
      "5136\n",
      "5137\n",
      "5138\n",
      "5139\n",
      "5140\n",
      "5141\n",
      "5142\n",
      "5143\n",
      "5144\n",
      "5145\n",
      "5146\n",
      "5147\n",
      "5148\n",
      "5149\n",
      "5150\n",
      "5151\n",
      "5152\n",
      "5153\n",
      "5154\n",
      "5155\n",
      "5156\n",
      "5157\n",
      "5158\n",
      "5159\n",
      "5160\n",
      "5161\n",
      "5162\n",
      "5163\n",
      "5164\n",
      "5165\n",
      "5166\n",
      "5167\n",
      "5168\n",
      "5169\n",
      "5170\n",
      "5171\n",
      "5172\n",
      "5173\n",
      "5174\n",
      "5175\n",
      "5176\n",
      "5177\n",
      "5178\n",
      "5179\n",
      "5180\n",
      "5181\n",
      "5182\n",
      "5183\n",
      "5184\n",
      "5185\n",
      "5186\n",
      "5187\n",
      "5188\n",
      "5189\n",
      "5190\n",
      "5191\n",
      "5192\n",
      "5193\n",
      "5194\n",
      "5195\n",
      "5196\n",
      "5197\n",
      "5198\n",
      "5199\n",
      "5200\n",
      "5201\n",
      "5202\n",
      "5203\n",
      "5204\n",
      "5205\n",
      "5206\n",
      "5207\n",
      "5208\n",
      "5209\n",
      "5210\n",
      "5211\n",
      "5212\n",
      "5213\n",
      "5214\n",
      "5215\n",
      "5216\n",
      "5217\n",
      "5218\n",
      "5219\n",
      "5220\n",
      "5221\n",
      "5222\n",
      "5223\n",
      "5224\n",
      "5225\n",
      "5226\n",
      "5227\n",
      "5228\n",
      "5229\n",
      "5230\n",
      "5231\n",
      "5232\n",
      "5233\n",
      "5234\n",
      "5235\n",
      "5236\n",
      "5237\n",
      "5238\n",
      "5239\n",
      "5240\n",
      "5241\n",
      "5242\n",
      "5243\n",
      "5244\n",
      "5245\n",
      "5246\n",
      "5247\n",
      "5248\n",
      "5249\n",
      "5250\n",
      "5251\n",
      "5252\n",
      "5253\n",
      "5254\n",
      "5255\n",
      "5256\n",
      "5257\n",
      "5258\n",
      "5259\n",
      "5260\n",
      "5261\n",
      "5262\n",
      "5263\n",
      "5264\n",
      "5265\n",
      "5266\n",
      "5267\n",
      "5268\n",
      "5269\n",
      "5270\n",
      "5271\n",
      "5272\n",
      "5273\n",
      "5274\n",
      "5275\n",
      "5276\n",
      "5277\n",
      "5278\n",
      "5279\n",
      "5280\n",
      "5281\n",
      "5282\n",
      "5283\n",
      "5284\n",
      "5285\n",
      "5286\n",
      "5287\n",
      "5288\n",
      "5289\n",
      "5290\n",
      "5291\n",
      "5292\n",
      "5293\n",
      "5294\n",
      "5295\n",
      "5296\n",
      "5297\n",
      "5298\n",
      "5299\n",
      "5300\n",
      "5301\n",
      "5302\n",
      "5303\n",
      "5304\n",
      "5305\n",
      "5306\n",
      "5307\n",
      "5308\n",
      "5309\n",
      "5310\n",
      "5311\n",
      "5312\n",
      "5313\n",
      "5314\n",
      "5315\n",
      "5316\n",
      "5317\n",
      "5318\n",
      "5319\n",
      "5320\n",
      "5321\n",
      "5322\n",
      "5323\n",
      "5324\n",
      "5325\n",
      "5326\n",
      "5327\n",
      "5328\n",
      "5329\n",
      "5330\n",
      "5331\n",
      "5332\n",
      "5333\n",
      "5334\n",
      "5335\n",
      "5336\n",
      "5337\n",
      "5338\n",
      "5339\n",
      "5340\n",
      "5341\n",
      "5342\n",
      "5343\n",
      "5344\n",
      "5345\n",
      "5346\n",
      "5347\n",
      "5348\n",
      "5349\n",
      "5350\n",
      "5351\n",
      "5352\n",
      "5353\n",
      "5354\n",
      "5355\n",
      "5356\n",
      "5357\n",
      "5358\n",
      "5359\n",
      "5360\n",
      "5361\n",
      "5362\n",
      "5363\n",
      "5364\n",
      "5365\n",
      "5366\n",
      "5367\n",
      "5368\n",
      "5369\n",
      "5370\n",
      "5371\n",
      "5372\n",
      "5373\n",
      "5374\n",
      "5375\n",
      "5376\n",
      "5377\n",
      "5378\n",
      "5379\n",
      "5380\n",
      "5381\n",
      "5382\n",
      "5383\n",
      "5384\n",
      "5385\n",
      "5386\n",
      "5387\n",
      "5388\n",
      "5389\n",
      "5390\n",
      "5391\n",
      "5392\n",
      "5393\n",
      "5394\n",
      "5395\n",
      "5396\n",
      "5397\n",
      "5398\n",
      "5399\n",
      "5400\n",
      "5401\n",
      "5402\n",
      "5403\n",
      "5404\n",
      "5405\n",
      "5406\n",
      "5407\n",
      "5408\n",
      "5409\n",
      "5410\n",
      "5411\n",
      "5412\n",
      "5413\n",
      "5414\n",
      "5415\n",
      "5416\n",
      "5417\n",
      "5418\n",
      "5419\n",
      "5420\n",
      "5421\n",
      "5422\n",
      "5423\n",
      "5424\n",
      "5425\n",
      "5426\n",
      "5427\n",
      "5428\n",
      "5429\n",
      "5430\n",
      "5431\n",
      "5432\n",
      "5433\n",
      "5434\n",
      "5435\n",
      "5436\n",
      "5437\n",
      "5438\n",
      "5439\n",
      "5440\n",
      "5441\n",
      "5442\n",
      "5443\n",
      "5444\n",
      "5445\n",
      "5446\n",
      "5447\n",
      "5448\n",
      "5449\n",
      "5450\n",
      "5451\n",
      "5452\n",
      "5453\n",
      "5454\n",
      "5455\n",
      "5456\n",
      "5457\n",
      "5458\n",
      "5459\n",
      "5460\n",
      "5461\n",
      "5462\n",
      "5463\n",
      "5464\n",
      "5465\n",
      "5466\n",
      "5467\n",
      "5468\n",
      "5469\n",
      "5470\n",
      "5471\n",
      "5472\n",
      "5473\n",
      "5474\n",
      "5475\n",
      "5476\n",
      "5477\n",
      "5478\n",
      "5479\n",
      "5480\n",
      "5481\n",
      "5482\n",
      "5483\n",
      "5484\n",
      "5485\n",
      "5486\n",
      "5487\n",
      "5488\n",
      "5489\n",
      "5490\n",
      "5491\n",
      "5492\n",
      "5493\n",
      "5494\n",
      "5495\n",
      "5496\n",
      "5497\n",
      "5498\n",
      "5499\n",
      "5500\n",
      "5501\n",
      "5502\n",
      "5503\n",
      "5504\n",
      "5505\n",
      "5506\n",
      "5507\n",
      "5508\n",
      "5509\n",
      "5510\n",
      "5511\n",
      "5512\n",
      "5513\n",
      "5514\n",
      "5515\n",
      "5516\n",
      "5517\n",
      "5518\n",
      "5519\n",
      "5520\n",
      "5521\n",
      "5522\n",
      "5523\n",
      "5524\n",
      "5525\n",
      "5526\n",
      "5527\n",
      "5528\n",
      "5529\n",
      "5530\n",
      "5531\n",
      "5532\n",
      "5533\n",
      "5534\n",
      "5535\n",
      "5536\n",
      "5537\n",
      "5538\n",
      "5539\n",
      "5540\n",
      "5541\n",
      "5542\n",
      "5543\n",
      "5544\n",
      "5545\n",
      "5546\n",
      "5547\n",
      "5548\n",
      "5549\n",
      "5550\n",
      "5551\n",
      "5552\n",
      "5553\n",
      "5554\n",
      "5555\n",
      "5556\n",
      "5557\n",
      "5558\n",
      "5559\n",
      "5560\n",
      "5561\n",
      "5562\n",
      "5563\n",
      "5564\n",
      "5565\n",
      "5566\n",
      "5567\n",
      "5568\n",
      "5569\n",
      "5570\n",
      "5571\n",
      "5572\n",
      "5573\n",
      "5574\n",
      "5575\n",
      "5576\n",
      "5577\n",
      "5578\n",
      "5579\n",
      "5580\n",
      "5581\n",
      "5582\n",
      "5583\n",
      "5584\n",
      "5585\n",
      "5586\n",
      "5587\n",
      "5588\n",
      "5589\n",
      "5590\n",
      "5591\n",
      "5592\n",
      "5593\n",
      "5594\n",
      "5595\n",
      "5596\n",
      "5597\n",
      "5598\n",
      "5599\n",
      "5600\n",
      "5601\n",
      "5602\n",
      "5603\n",
      "5604\n",
      "5605\n",
      "5606\n",
      "5607\n",
      "5608\n",
      "5609\n",
      "5610\n",
      "5611\n",
      "5612\n",
      "5613\n",
      "5614\n",
      "5615\n",
      "5616\n",
      "5617\n",
      "5618\n",
      "5619\n",
      "5620\n",
      "5621\n",
      "5622\n",
      "5623\n",
      "5624\n",
      "5625\n",
      "5626\n",
      "5627\n",
      "5628\n",
      "5629\n",
      "5630\n",
      "5631\n",
      "5632\n",
      "5633\n",
      "5634\n",
      "5635\n",
      "5636\n",
      "5637\n",
      "5638\n",
      "5639\n",
      "5640\n",
      "5641\n",
      "5642\n",
      "5643\n",
      "5644\n",
      "5645\n",
      "5646\n",
      "5647\n",
      "5648\n",
      "5649\n",
      "5650\n",
      "5651\n",
      "5652\n",
      "5653\n",
      "5654\n",
      "5655\n",
      "5656\n",
      "5657\n",
      "5658\n",
      "5659\n",
      "5660\n",
      "5661\n",
      "5662\n",
      "5663\n",
      "5664\n",
      "5665\n",
      "5666\n",
      "5667\n",
      "5668\n",
      "5669\n",
      "5670\n",
      "5671\n",
      "5672\n",
      "5673\n",
      "5674\n",
      "5675\n",
      "5676\n",
      "5677\n",
      "5678\n",
      "5679\n",
      "5680\n",
      "5681\n",
      "5682\n",
      "5683\n",
      "5684\n",
      "5685\n",
      "5686\n",
      "5687\n",
      "5688\n",
      "5689\n",
      "5690\n",
      "5691\n",
      "5692\n",
      "5693\n",
      "5694\n",
      "5695\n",
      "5696\n",
      "5697\n",
      "5698\n",
      "5699\n",
      "5700\n",
      "5701\n",
      "5702\n",
      "5703\n",
      "5704\n",
      "5705\n",
      "5706\n",
      "5707\n",
      "5708\n",
      "5709\n",
      "5710\n",
      "5711\n",
      "5712\n",
      "5713\n",
      "5714\n",
      "5715\n",
      "5716\n",
      "5717\n",
      "5718\n",
      "5719\n",
      "5720\n",
      "5721\n",
      "5722\n",
      "5723\n",
      "5724\n",
      "5725\n",
      "5726\n",
      "5727\n",
      "5728\n",
      "5729\n",
      "5730\n",
      "5731\n",
      "5732\n",
      "5733\n",
      "5734\n",
      "5735\n",
      "5736\n",
      "5737\n",
      "5738\n",
      "5739\n",
      "5740\n",
      "5741\n",
      "5742\n",
      "5743\n",
      "5744\n",
      "5745\n",
      "5746\n",
      "5747\n",
      "5748\n",
      "5749\n",
      "5750\n",
      "5751\n",
      "5752\n",
      "5753\n",
      "5754\n",
      "5755\n",
      "5756\n",
      "5757\n",
      "5758\n",
      "5759\n",
      "5760\n",
      "5761\n",
      "5762\n",
      "5763\n",
      "5764\n",
      "5765\n",
      "5766\n",
      "5767\n",
      "5768\n",
      "5769\n",
      "5770\n",
      "5771\n",
      "5772\n",
      "5773\n",
      "5774\n",
      "5775\n",
      "5776\n",
      "5777\n",
      "5778\n",
      "5779\n",
      "5780\n",
      "5781\n",
      "5782\n",
      "5783\n",
      "5784\n",
      "5785\n",
      "5786\n",
      "5787\n",
      "5788\n",
      "5789\n",
      "5790\n",
      "5791\n",
      "5792\n",
      "5793\n",
      "5794\n",
      "5795\n",
      "5796\n",
      "5797\n",
      "5798\n",
      "5799\n",
      "5800\n",
      "5801\n",
      "5802\n",
      "5803\n",
      "5804\n",
      "5805\n",
      "5806\n",
      "5807\n",
      "5808\n",
      "5809\n",
      "5810\n",
      "5811\n",
      "5812\n",
      "5813\n",
      "5814\n",
      "5815\n",
      "5816\n",
      "5817\n",
      "5818\n",
      "5819\n",
      "5820\n",
      "5821\n",
      "5822\n",
      "5823\n",
      "5824\n",
      "5825\n",
      "5826\n",
      "5827\n",
      "5828\n",
      "5829\n",
      "5830\n",
      "5831\n",
      "5832\n",
      "5833\n",
      "5834\n",
      "5835\n",
      "5836\n",
      "5837\n",
      "5838\n",
      "5839\n",
      "5840\n",
      "5841\n",
      "5842\n",
      "5843\n",
      "5844\n",
      "5845\n",
      "5846\n",
      "5847\n",
      "5848\n",
      "5849\n",
      "5850\n",
      "5851\n",
      "5852\n",
      "5853\n",
      "5854\n",
      "5855\n",
      "5856\n",
      "5857\n",
      "5858\n",
      "5859\n",
      "5860\n",
      "5861\n",
      "5862\n",
      "5863\n",
      "5864\n",
      "5865\n",
      "5866\n",
      "5867\n",
      "5868\n",
      "5869\n",
      "5870\n",
      "5871\n",
      "5872\n",
      "5873\n",
      "5874\n",
      "5875\n",
      "5876\n",
      "5877\n",
      "5878\n",
      "5879\n",
      "5880\n",
      "5881\n",
      "5882\n",
      "5883\n",
      "5884\n",
      "5885\n",
      "5886\n",
      "5887\n",
      "5888\n",
      "5889\n",
      "5890\n",
      "5891\n",
      "5892\n",
      "5893\n",
      "5894\n",
      "5895\n",
      "5896\n",
      "5897\n",
      "5898\n",
      "5899\n",
      "5900\n",
      "5901\n",
      "5902\n",
      "5903\n",
      "5904\n",
      "5905\n",
      "5906\n",
      "5907\n",
      "5908\n",
      "5909\n",
      "5910\n",
      "5911\n",
      "5912\n",
      "5913\n",
      "5914\n",
      "5915\n",
      "5916\n",
      "5917\n",
      "5918\n",
      "5919\n",
      "5920\n",
      "5921\n",
      "5922\n",
      "5923\n",
      "5924\n",
      "5925\n",
      "5926\n",
      "5927\n",
      "5928\n",
      "5929\n",
      "5930\n",
      "5931\n",
      "5932\n",
      "5933\n",
      "5934\n",
      "5935\n",
      "5936\n",
      "5937\n",
      "5938\n",
      "5939\n",
      "5940\n",
      "5941\n",
      "5942\n",
      "5943\n",
      "5944\n",
      "5945\n",
      "5946\n",
      "5947\n",
      "5948\n",
      "5949\n",
      "5950\n",
      "5951\n",
      "5952\n",
      "5953\n",
      "5954\n",
      "5955\n",
      "5956\n",
      "5957\n",
      "5958\n",
      "5959\n",
      "5960\n",
      "5961\n",
      "5962\n",
      "5963\n",
      "5964\n",
      "5965\n",
      "5966\n",
      "5967\n",
      "5968\n",
      "5969\n",
      "5970\n",
      "5971\n",
      "5972\n",
      "5973\n",
      "5974\n",
      "5975\n",
      "5976\n",
      "5977\n",
      "5978\n",
      "5979\n",
      "5980\n",
      "5981\n",
      "5982\n",
      "5983\n",
      "5984\n",
      "5985\n",
      "5986\n",
      "5987\n",
      "5988\n",
      "5989\n",
      "5990\n",
      "5991\n",
      "5992\n",
      "5993\n",
      "5994\n",
      "5995\n",
      "5996\n",
      "5997\n",
      "5998\n",
      "5999\n",
      "6000\n",
      "6001\n",
      "6002\n",
      "6003\n",
      "6004\n",
      "6005\n",
      "6006\n",
      "6007\n",
      "6008\n",
      "6009\n",
      "6010\n",
      "6011\n",
      "6012\n",
      "6013\n",
      "6014\n",
      "6015\n",
      "6016\n",
      "6017\n",
      "6018\n",
      "6019\n",
      "6020\n",
      "6021\n",
      "6022\n",
      "6023\n",
      "6024\n",
      "6025\n",
      "6026\n",
      "6027\n",
      "6028\n",
      "6029\n",
      "6030\n",
      "6031\n",
      "6032\n",
      "6033\n",
      "6034\n",
      "6035\n",
      "6036\n",
      "6037\n",
      "6038\n",
      "6039\n",
      "6040\n",
      "6041\n",
      "6042\n",
      "6043\n",
      "6044\n",
      "6045\n",
      "6046\n",
      "6047\n",
      "6048\n",
      "6049\n",
      "6050\n",
      "6051\n",
      "6052\n",
      "6053\n",
      "6054\n",
      "6055\n",
      "6056\n",
      "6057\n",
      "6058\n",
      "6059\n",
      "6060\n",
      "6061\n",
      "6062\n",
      "6063\n",
      "6064\n",
      "6065\n",
      "6066\n",
      "6067\n",
      "6068\n",
      "6069\n",
      "6070\n",
      "6071\n",
      "6072\n",
      "6073\n",
      "6074\n",
      "6075\n",
      "6076\n",
      "6077\n",
      "6078\n",
      "6079\n",
      "6080\n",
      "6081\n",
      "6082\n",
      "6083\n",
      "6084\n",
      "6085\n",
      "6086\n",
      "6087\n",
      "6088\n",
      "6089\n",
      "6090\n",
      "6091\n",
      "6092\n",
      "6093\n",
      "6094\n",
      "6095\n",
      "6096\n",
      "6097\n",
      "6098\n",
      "6099\n",
      "6100\n",
      "6101\n",
      "6102\n",
      "6103\n",
      "6104\n",
      "6105\n",
      "6106\n",
      "6107\n",
      "6108\n",
      "6109\n",
      "6110\n",
      "6111\n",
      "6112\n",
      "6113\n",
      "6114\n",
      "6115\n",
      "6116\n",
      "6117\n",
      "6118\n",
      "6119\n",
      "6120\n",
      "6121\n",
      "6122\n",
      "6123\n",
      "6124\n",
      "6125\n",
      "6126\n",
      "6127\n",
      "6128\n",
      "6129\n",
      "6130\n",
      "6131\n",
      "6132\n",
      "6133\n",
      "6134\n",
      "6135\n",
      "6136\n",
      "6137\n",
      "6138\n",
      "6139\n",
      "6140\n",
      "6141\n",
      "6142\n",
      "6143\n",
      "6144\n",
      "6145\n",
      "6146\n",
      "6147\n",
      "6148\n",
      "6149\n",
      "6150\n",
      "6151\n",
      "6152\n",
      "6153\n",
      "6154\n",
      "6155\n",
      "6156\n",
      "6157\n",
      "6158\n",
      "6159\n",
      "6160\n",
      "6161\n",
      "6162\n",
      "6163\n",
      "6164\n",
      "6165\n",
      "6166\n",
      "6167\n",
      "6168\n",
      "6169\n",
      "6170\n",
      "6171\n",
      "6172\n",
      "6173\n",
      "6174\n",
      "6175\n",
      "6176\n",
      "6177\n",
      "6178\n",
      "6179\n",
      "6180\n",
      "6181\n",
      "6182\n",
      "6183\n",
      "6184\n",
      "6185\n",
      "6186\n",
      "6187\n",
      "6188\n",
      "6189\n",
      "6190\n",
      "6191\n",
      "6192\n",
      "6193\n",
      "6194\n",
      "6195\n",
      "6196\n",
      "6197\n",
      "6198\n",
      "6199\n",
      "6200\n",
      "6201\n",
      "6202\n",
      "6203\n",
      "6204\n",
      "6205\n",
      "6206\n",
      "6207\n",
      "6208\n",
      "6209\n",
      "6210\n",
      "6211\n",
      "6212\n",
      "6213\n",
      "6214\n",
      "6215\n",
      "6216\n",
      "6217\n",
      "6218\n",
      "6219\n",
      "6220\n",
      "6221\n",
      "6222\n",
      "6223\n",
      "6224\n",
      "6225\n",
      "6226\n",
      "6227\n",
      "6228\n",
      "6229\n",
      "6230\n",
      "6231\n",
      "6232\n",
      "6233\n",
      "6234\n",
      "6235\n",
      "6236\n",
      "6237\n",
      "6238\n",
      "6239\n",
      "6240\n",
      "6241\n",
      "6242\n",
      "6243\n",
      "6244\n",
      "6245\n",
      "6246\n",
      "6247\n",
      "6248\n",
      "6249\n",
      "6250\n",
      "6251\n",
      "6252\n",
      "6253\n",
      "6254\n",
      "6255\n",
      "6256\n",
      "6257\n",
      "6258\n",
      "6259\n",
      "6260\n",
      "6261\n",
      "6262\n",
      "6263\n",
      "6264\n",
      "6265\n",
      "6266\n",
      "6267\n",
      "6268\n",
      "6269\n",
      "6270\n",
      "6271\n",
      "6272\n",
      "6273\n",
      "6274\n",
      "6275\n",
      "6276\n",
      "6277\n",
      "6278\n",
      "6279\n",
      "6280\n",
      "6281\n",
      "6282\n",
      "6283\n",
      "6284\n",
      "6285\n",
      "6286\n",
      "6287\n",
      "6288\n",
      "6289\n",
      "6290\n",
      "6291\n",
      "6292\n",
      "6293\n",
      "6294\n",
      "6295\n",
      "6296\n",
      "6297\n",
      "6298\n",
      "6299\n",
      "6300\n",
      "6301\n",
      "6302\n",
      "6303\n",
      "6304\n",
      "6305\n",
      "6306\n",
      "6307\n",
      "6308\n",
      "6309\n",
      "6310\n",
      "6311\n",
      "6312\n",
      "6313\n",
      "6314\n",
      "6315\n",
      "6316\n",
      "6317\n",
      "6318\n",
      "6319\n",
      "6320\n",
      "6321\n",
      "6322\n",
      "6323\n",
      "6324\n",
      "6325\n",
      "6326\n",
      "6327\n",
      "6328\n",
      "6329\n",
      "6330\n",
      "6331\n",
      "6332\n",
      "6333\n",
      "6334\n",
      "6335\n",
      "6336\n",
      "6337\n",
      "6338\n",
      "6339\n",
      "6340\n",
      "6341\n",
      "6342\n",
      "6343\n",
      "6344\n",
      "6345\n",
      "6346\n",
      "6347\n",
      "6348\n",
      "6349\n",
      "6350\n",
      "6351\n",
      "6352\n",
      "6353\n",
      "6354\n",
      "6355\n",
      "6356\n",
      "6357\n",
      "6358\n",
      "6359\n",
      "6360\n",
      "6361\n",
      "6362\n",
      "6363\n",
      "6364\n",
      "6365\n",
      "6366\n",
      "6367\n",
      "6368\n",
      "6369\n",
      "6370\n",
      "6371\n",
      "6372\n",
      "6373\n",
      "6374\n",
      "6375\n",
      "6376\n",
      "6377\n",
      "6378\n",
      "6379\n",
      "6380\n",
      "6381\n",
      "6382\n",
      "6383\n",
      "6384\n",
      "6385\n",
      "6386\n",
      "6387\n",
      "6388\n",
      "6389\n",
      "6390\n",
      "6391\n",
      "6392\n",
      "6393\n",
      "6394\n",
      "6395\n",
      "6396\n",
      "6397\n",
      "6398\n",
      "6399\n",
      "6400\n",
      "6401\n",
      "6402\n",
      "6403\n",
      "6404\n",
      "6405\n",
      "6406\n",
      "6407\n",
      "6408\n",
      "6409\n",
      "6410\n",
      "6411\n",
      "6412\n",
      "6413\n",
      "6414\n",
      "6415\n",
      "6416\n",
      "6417\n",
      "6418\n",
      "6419\n",
      "6420\n",
      "6421\n",
      "6422\n",
      "6423\n",
      "6424\n",
      "6425\n",
      "6426\n",
      "6427\n",
      "6428\n",
      "6429\n",
      "6430\n",
      "6431\n",
      "6432\n",
      "6433\n",
      "6434\n",
      "6435\n",
      "6436\n",
      "6437\n",
      "6438\n",
      "6439\n",
      "6440\n",
      "6441\n",
      "6442\n",
      "6443\n",
      "6444\n",
      "6445\n",
      "6446\n",
      "6447\n",
      "6448\n",
      "6449\n",
      "6450\n",
      "6451\n",
      "6452\n",
      "6453\n",
      "6454\n",
      "6455\n",
      "6456\n",
      "6457\n",
      "6458\n",
      "6459\n",
      "6460\n",
      "6461\n",
      "6462\n",
      "6463\n",
      "6464\n",
      "6465\n",
      "6466\n",
      "6467\n",
      "6468\n",
      "6469\n",
      "6470\n",
      "6471\n",
      "6472\n",
      "6473\n",
      "6474\n",
      "6475\n",
      "6476\n",
      "6477\n",
      "6478\n",
      "6479\n",
      "6480\n",
      "6481\n",
      "6482\n",
      "6483\n",
      "6484\n",
      "6485\n",
      "6486\n",
      "6487\n",
      "6488\n",
      "6489\n",
      "6490\n",
      "6491\n",
      "6492\n",
      "6493\n",
      "6494\n",
      "6495\n",
      "6496\n",
      "6497\n",
      "6498\n",
      "6499\n",
      "6500\n",
      "6501\n",
      "6502\n",
      "6503\n",
      "6504\n",
      "6505\n",
      "6506\n",
      "6507\n",
      "6508\n",
      "6509\n",
      "6510\n",
      "6511\n",
      "6512\n",
      "6513\n",
      "6514\n",
      "6515\n",
      "6516\n",
      "6517\n",
      "6518\n",
      "6519\n",
      "6520\n",
      "6521\n",
      "6522\n",
      "6523\n",
      "6524\n",
      "6525\n",
      "6526\n",
      "6527\n",
      "6528\n",
      "6529\n",
      "6530\n",
      "6531\n",
      "6532\n",
      "6533\n",
      "6534\n",
      "6535\n",
      "6536\n",
      "6537\n",
      "6538\n",
      "6539\n",
      "6540\n",
      "6541\n",
      "6542\n",
      "6543\n",
      "6544\n",
      "6545\n",
      "6546\n",
      "6547\n",
      "6548\n",
      "6549\n",
      "6550\n",
      "6551\n",
      "6552\n",
      "6553\n",
      "6554\n",
      "6555\n",
      "6556\n",
      "6557\n",
      "6558\n",
      "6559\n",
      "6560\n",
      "6561\n",
      "6562\n",
      "6563\n",
      "6564\n",
      "6565\n",
      "6566\n",
      "6567\n",
      "6568\n",
      "6569\n",
      "6570\n",
      "6571\n",
      "6572\n",
      "6573\n",
      "6574\n",
      "6575\n",
      "6576\n",
      "6577\n",
      "6578\n",
      "6579\n",
      "6580\n",
      "6581\n",
      "6582\n",
      "6583\n",
      "6584\n",
      "6585\n",
      "6586\n",
      "6587\n",
      "6588\n",
      "6589\n",
      "6590\n",
      "6591\n",
      "6592\n",
      "6593\n",
      "6594\n",
      "6595\n",
      "6596\n",
      "6597\n",
      "6598\n",
      "6599\n",
      "6600\n",
      "6601\n",
      "6602\n",
      "6603\n",
      "6604\n",
      "6605\n",
      "6606\n",
      "6607\n",
      "6608\n",
      "6609\n",
      "6610\n",
      "6611\n",
      "6612\n",
      "6613\n",
      "6614\n",
      "6615\n",
      "6616\n",
      "6617\n",
      "6618\n",
      "6619\n",
      "6620\n",
      "6621\n",
      "6622\n",
      "6623\n",
      "6624\n",
      "6625\n",
      "6626\n",
      "6627\n",
      "6628\n",
      "6629\n",
      "6630\n",
      "6631\n",
      "6632\n",
      "6633\n",
      "6634\n",
      "6635\n",
      "6636\n",
      "6637\n",
      "6638\n",
      "6639\n",
      "6640\n",
      "6641\n",
      "6642\n",
      "6643\n",
      "6644\n",
      "6645\n",
      "6646\n",
      "6647\n",
      "6648\n",
      "6649\n",
      "6650\n",
      "6651\n",
      "6652\n",
      "6653\n",
      "6654\n",
      "6655\n",
      "6656\n",
      "6657\n",
      "6658\n",
      "6659\n",
      "6660\n",
      "6661\n",
      "6662\n",
      "6663\n",
      "6664\n",
      "6665\n",
      "6666\n",
      "6667\n",
      "6668\n",
      "6669\n",
      "6670\n",
      "6671\n",
      "6672\n",
      "6673\n",
      "6674\n",
      "6675\n",
      "6676\n",
      "6677\n",
      "6678\n",
      "6679\n",
      "6680\n",
      "6681\n",
      "6682\n",
      "6683\n",
      "6684\n",
      "6685\n",
      "6686\n",
      "6687\n",
      "6688\n",
      "6689\n",
      "6690\n",
      "6691\n",
      "6692\n",
      "6693\n",
      "6694\n",
      "6695\n",
      "6696\n",
      "6697\n",
      "6698\n",
      "6699\n",
      "6700\n",
      "6701\n",
      "6702\n",
      "6703\n",
      "6704\n",
      "6705\n",
      "6706\n",
      "6707\n",
      "6708\n",
      "6709\n",
      "6710\n",
      "6711\n",
      "6712\n",
      "6713\n",
      "6714\n",
      "6715\n",
      "6716\n",
      "6717\n",
      "6718\n",
      "6719\n",
      "6720\n",
      "6721\n",
      "6722\n",
      "6723\n",
      "6724\n",
      "6725\n",
      "6726\n",
      "6727\n",
      "6728\n",
      "6729\n",
      "6730\n",
      "6731\n",
      "6732\n",
      "6733\n",
      "6734\n",
      "6735\n",
      "6736\n",
      "6737\n",
      "6738\n",
      "6739\n",
      "6740\n",
      "6741\n",
      "6742\n",
      "6743\n",
      "6744\n",
      "6745\n",
      "6746\n",
      "6747\n",
      "6748\n",
      "6749\n",
      "6750\n",
      "6751\n",
      "6752\n",
      "6753\n",
      "6754\n",
      "6755\n",
      "6756\n",
      "6757\n",
      "6758\n",
      "6759\n",
      "6760\n",
      "6761\n",
      "6762\n",
      "6763\n",
      "6764\n",
      "6765\n",
      "6766\n",
      "6767\n",
      "6768\n",
      "6769\n",
      "6770\n",
      "6771\n",
      "6772\n",
      "6773\n",
      "6774\n",
      "6775\n",
      "6776\n",
      "6777\n",
      "6778\n",
      "6779\n",
      "6780\n",
      "6781\n",
      "6782\n",
      "6783\n",
      "6784\n",
      "6785\n",
      "6786\n",
      "6787\n",
      "6788\n",
      "6789\n",
      "6790\n",
      "6791\n",
      "6792\n",
      "6793\n",
      "6794\n",
      "6795\n",
      "6796\n",
      "6797\n",
      "6798\n",
      "6799\n",
      "6800\n",
      "6801\n",
      "6802\n",
      "6803\n",
      "6804\n",
      "6805\n",
      "6806\n",
      "6807\n",
      "6808\n",
      "6809\n",
      "6810\n",
      "6811\n",
      "6812\n",
      "6813\n",
      "6814\n",
      "6815\n",
      "6816\n",
      "6817\n",
      "6818\n",
      "6819\n",
      "6820\n",
      "6821\n",
      "6822\n",
      "6823\n",
      "6824\n",
      "6825\n",
      "6826\n",
      "6827\n",
      "6828\n",
      "6829\n",
      "6830\n",
      "6831\n",
      "6832\n",
      "6833\n",
      "6834\n",
      "6835\n",
      "6836\n",
      "6837\n",
      "6838\n",
      "6839\n",
      "6840\n",
      "6841\n",
      "6842\n",
      "6843\n",
      "6844\n",
      "6845\n",
      "6846\n",
      "6847\n",
      "6848\n",
      "6849\n",
      "6850\n",
      "6851\n",
      "6852\n",
      "6853\n",
      "6854\n",
      "6855\n",
      "6856\n",
      "6857\n",
      "6858\n",
      "6859\n",
      "6860\n",
      "6861\n",
      "6862\n",
      "6863\n",
      "6864\n",
      "6865\n",
      "6866\n",
      "6867\n",
      "6868\n",
      "6869\n",
      "6870\n",
      "6871\n",
      "6872\n",
      "6873\n",
      "6874\n",
      "6875\n",
      "6876\n",
      "6877\n",
      "6878\n",
      "6879\n",
      "6880\n",
      "6881\n",
      "6882\n",
      "6883\n",
      "6884\n",
      "6885\n",
      "6886\n",
      "6887\n",
      "6888\n",
      "6889\n",
      "6890\n",
      "6891\n",
      "6892\n",
      "6893\n",
      "6894\n",
      "6895\n",
      "6896\n",
      "6897\n",
      "6898\n",
      "6899\n",
      "6900\n",
      "6901\n",
      "6902\n",
      "6903\n",
      "6904\n",
      "6905\n",
      "6906\n",
      "6907\n",
      "6908\n",
      "6909\n",
      "6910\n",
      "6911\n",
      "6912\n",
      "6913\n",
      "6914\n",
      "6915\n",
      "6916\n",
      "6917\n",
      "6918\n",
      "6919\n",
      "6920\n",
      "6921\n",
      "6922\n",
      "6923\n",
      "6924\n",
      "6925\n",
      "6926\n",
      "6927\n",
      "6928\n",
      "6929\n",
      "6930\n",
      "6931\n",
      "6932\n",
      "6933\n",
      "6934\n",
      "6935\n",
      "6936\n",
      "6937\n",
      "6938\n",
      "6939\n",
      "6940\n",
      "6941\n",
      "6942\n",
      "6943\n",
      "6944\n",
      "6945\n",
      "6946\n",
      "6947\n",
      "6948\n",
      "6949\n",
      "6950\n",
      "6951\n",
      "6952\n",
      "6953\n",
      "6954\n",
      "6955\n",
      "6956\n",
      "6957\n",
      "6958\n",
      "6959\n",
      "6960\n",
      "6961\n",
      "6962\n",
      "6963\n",
      "6964\n",
      "6965\n",
      "6966\n",
      "6967\n",
      "6968\n",
      "6969\n",
      "6970\n",
      "6971\n",
      "6972\n",
      "6973\n",
      "6974\n",
      "6975\n",
      "6976\n",
      "6977\n",
      "6978\n",
      "6979\n",
      "6980\n",
      "6981\n",
      "6982\n",
      "6983\n",
      "6984\n",
      "6985\n",
      "6986\n",
      "6987\n",
      "6988\n",
      "6989\n",
      "6990\n",
      "6991\n",
      "6992\n",
      "6993\n",
      "6994\n",
      "6995\n",
      "6996\n",
      "6997\n",
      "6998\n",
      "6999\n",
      "7000\n",
      "7001\n",
      "7002\n",
      "7003\n",
      "7004\n",
      "7005\n",
      "7006\n",
      "7007\n",
      "7008\n",
      "7009\n",
      "7010\n",
      "7011\n",
      "7012\n",
      "7013\n",
      "7014\n",
      "7015\n",
      "7016\n",
      "7017\n",
      "7018\n",
      "7019\n",
      "7020\n",
      "7021\n",
      "7022\n",
      "7023\n",
      "7024\n",
      "7025\n",
      "7026\n",
      "7027\n",
      "7028\n",
      "7029\n",
      "7030\n",
      "7031\n",
      "7032\n",
      "7033\n",
      "7034\n",
      "7035\n",
      "7036\n",
      "7037\n",
      "7038\n",
      "7039\n",
      "7040\n",
      "7041\n",
      "7042\n",
      "7043\n",
      "7044\n",
      "7045\n",
      "7046\n",
      "7047\n",
      "7048\n",
      "7049\n",
      "7050\n",
      "7051\n",
      "7052\n",
      "7053\n",
      "7054\n",
      "7055\n",
      "7056\n",
      "7057\n",
      "7058\n",
      "7059\n",
      "7060\n",
      "7061\n",
      "7062\n",
      "7063\n",
      "7064\n",
      "7065\n",
      "7066\n",
      "7067\n",
      "7068\n",
      "7069\n",
      "7070\n",
      "7071\n",
      "7072\n",
      "7073\n",
      "7074\n",
      "7075\n",
      "7076\n",
      "7077\n",
      "7078\n",
      "7079\n",
      "7080\n",
      "7081\n",
      "7082\n",
      "7083\n",
      "7084\n",
      "7085\n",
      "7086\n",
      "7087\n",
      "7088\n",
      "7089\n",
      "7090\n",
      "7091\n",
      "7092\n",
      "7093\n",
      "7094\n",
      "7095\n",
      "7096\n",
      "7097\n",
      "7098\n",
      "7099\n",
      "7100\n",
      "7101\n",
      "7102\n",
      "7103\n",
      "7104\n",
      "7105\n",
      "7106\n",
      "7107\n",
      "7108\n",
      "7109\n",
      "7110\n",
      "7111\n",
      "7112\n",
      "7113\n",
      "7114\n",
      "7115\n",
      "7116\n",
      "7117\n",
      "7118\n",
      "7119\n",
      "7120\n",
      "7121\n",
      "7122\n",
      "7123\n",
      "7124\n",
      "7125\n",
      "7126\n",
      "7127\n",
      "7128\n",
      "7129\n",
      "7130\n",
      "7131\n",
      "7132\n",
      "7133\n",
      "7134\n",
      "7135\n",
      "7136\n",
      "7137\n",
      "7138\n",
      "7139\n",
      "7140\n",
      "7141\n",
      "7142\n",
      "7143\n",
      "7144\n",
      "7145\n",
      "7146\n",
      "7147\n",
      "7148\n",
      "7149\n",
      "7150\n",
      "7151\n",
      "7152\n",
      "7153\n",
      "7154\n",
      "7155\n",
      "7156\n",
      "7157\n",
      "7158\n",
      "7159\n",
      "7160\n",
      "7161\n",
      "7162\n",
      "7163\n",
      "7164\n",
      "7165\n",
      "7166\n",
      "7167\n",
      "7168\n",
      "7169\n",
      "7170\n",
      "7171\n",
      "7172\n",
      "7173\n",
      "7174\n",
      "7175\n",
      "7176\n",
      "7177\n",
      "7178\n",
      "7179\n",
      "7180\n",
      "7181\n",
      "7182\n",
      "7183\n",
      "7184\n",
      "7185\n",
      "7186\n",
      "7187\n",
      "7188\n",
      "7189\n",
      "7190\n",
      "7191\n",
      "7192\n",
      "7193\n",
      "7194\n",
      "7195\n",
      "7196\n",
      "7197\n",
      "7198\n",
      "7199\n",
      "7200\n",
      "7201\n",
      "7202\n",
      "7203\n",
      "7204\n",
      "7205\n",
      "7206\n",
      "7207\n",
      "7208\n",
      "7209\n",
      "7210\n",
      "7211\n",
      "7212\n",
      "7213\n",
      "7214\n",
      "7215\n",
      "7216\n",
      "7217\n",
      "7218\n",
      "7219\n",
      "7220\n",
      "7221\n",
      "7222\n",
      "7223\n",
      "7224\n",
      "7225\n",
      "7226\n",
      "7227\n",
      "7228\n",
      "7229\n",
      "7230\n",
      "7231\n",
      "7232\n",
      "7233\n",
      "7234\n",
      "7235\n",
      "7236\n",
      "7237\n",
      "7238\n",
      "7239\n",
      "7240\n",
      "7241\n",
      "7242\n",
      "7243\n",
      "7244\n",
      "7245\n",
      "7246\n",
      "7247\n",
      "7248\n",
      "7249\n",
      "7250\n",
      "7251\n",
      "7252\n",
      "7253\n",
      "7254\n",
      "7255\n",
      "7256\n",
      "7257\n",
      "7258\n",
      "7259\n",
      "7260\n",
      "7261\n",
      "7262\n",
      "7263\n",
      "7264\n",
      "7265\n",
      "7266\n",
      "7267\n",
      "7268\n",
      "7269\n",
      "7270\n",
      "7271\n",
      "7272\n",
      "7273\n",
      "7274\n",
      "7275\n",
      "7276\n",
      "7277\n",
      "7278\n",
      "7279\n",
      "7280\n",
      "7281\n",
      "7282\n",
      "7283\n",
      "7284\n",
      "7285\n",
      "7286\n",
      "7287\n",
      "7288\n",
      "7289\n",
      "7290\n",
      "7291\n",
      "7292\n",
      "7293\n",
      "7294\n",
      "7295\n",
      "7296\n",
      "7297\n",
      "7298\n",
      "7299\n",
      "7300\n",
      "7301\n",
      "7302\n",
      "7303\n",
      "7304\n",
      "7305\n",
      "7306\n",
      "7307\n",
      "7308\n",
      "7309\n",
      "7310\n",
      "7311\n",
      "7312\n",
      "7313\n",
      "7314\n",
      "7315\n",
      "7316\n",
      "7317\n",
      "7318\n",
      "7319\n",
      "7320\n",
      "7321\n",
      "7322\n",
      "7323\n",
      "7324\n",
      "7325\n",
      "7326\n",
      "7327\n",
      "7328\n",
      "7329\n",
      "7330\n",
      "7331\n",
      "7332\n",
      "7333\n",
      "7334\n",
      "7335\n",
      "7336\n",
      "7337\n",
      "7338\n",
      "7339\n",
      "7340\n",
      "7341\n",
      "7342\n",
      "7343\n",
      "7344\n",
      "7345\n",
      "7346\n",
      "7347\n",
      "7348\n",
      "7349\n",
      "7350\n",
      "7351\n",
      "7352\n",
      "7353\n",
      "7354\n",
      "7355\n",
      "7356\n",
      "7357\n",
      "7358\n",
      "7359\n",
      "7360\n",
      "7361\n",
      "7362\n",
      "7363\n",
      "7364\n",
      "7365\n",
      "7366\n",
      "7367\n",
      "7368\n",
      "7369\n",
      "7370\n",
      "7371\n",
      "7372\n",
      "7373\n",
      "7374\n",
      "7375\n",
      "7376\n",
      "7377\n",
      "7378\n",
      "7379\n",
      "7380\n",
      "7381\n",
      "7382\n",
      "7383\n",
      "7384\n",
      "7385\n",
      "7386\n",
      "7387\n",
      "7388\n",
      "7389\n",
      "7390\n",
      "7391\n",
      "7392\n",
      "7393\n",
      "7394\n",
      "7395\n",
      "7396\n",
      "7397\n",
      "7398\n",
      "7399\n",
      "7400\n",
      "7401\n",
      "7402\n",
      "7403\n",
      "7404\n",
      "7405\n",
      "7406\n",
      "7407\n",
      "7408\n",
      "7409\n",
      "7410\n",
      "7411\n",
      "7412\n",
      "7413\n",
      "7414\n",
      "7415\n",
      "7416\n",
      "7417\n",
      "7418\n",
      "7419\n",
      "7420\n",
      "7421\n",
      "7422\n",
      "7423\n",
      "7424\n",
      "7425\n",
      "7426\n",
      "7427\n",
      "7428\n",
      "7429\n",
      "7430\n",
      "7431\n",
      "7432\n",
      "7433\n",
      "7434\n",
      "7435\n",
      "7436\n",
      "7437\n",
      "7438\n",
      "7439\n",
      "7440\n",
      "7441\n",
      "7442\n",
      "7443\n",
      "7444\n",
      "7445\n",
      "7446\n",
      "7447\n",
      "7448\n",
      "7449\n",
      "7450\n",
      "7451\n",
      "7452\n",
      "7453\n",
      "7454\n",
      "7455\n",
      "7456\n",
      "7457\n",
      "7458\n",
      "7459\n",
      "7460\n",
      "7461\n",
      "7462\n",
      "7463\n",
      "7464\n",
      "7465\n",
      "7466\n",
      "7467\n",
      "7468\n",
      "7469\n",
      "7470\n",
      "7471\n",
      "7472\n",
      "7473\n",
      "7474\n",
      "7475\n",
      "7476\n",
      "7477\n",
      "7478\n",
      "7479\n",
      "7480\n",
      "7481\n",
      "7482\n",
      "7483\n",
      "7484\n",
      "7485\n",
      "7486\n",
      "7487\n",
      "7488\n",
      "7489\n",
      "7490\n",
      "7491\n",
      "7492\n",
      "7493\n",
      "7494\n",
      "7495\n",
      "7496\n",
      "7497\n",
      "7498\n",
      "7499\n",
      "7500\n",
      "7501\n",
      "7502\n",
      "7503\n",
      "7504\n",
      "7505\n",
      "7506\n",
      "7507\n",
      "7508\n",
      "7509\n",
      "7510\n",
      "7511\n",
      "7512\n",
      "7513\n",
      "7514\n",
      "7515\n",
      "7516\n",
      "7517\n",
      "7518\n",
      "7519\n",
      "7520\n",
      "7521\n",
      "7522\n",
      "7523\n",
      "7524\n",
      "7525\n",
      "7526\n",
      "7527\n",
      "7528\n",
      "7529\n",
      "7530\n",
      "7531\n",
      "7532\n",
      "7533\n",
      "7534\n",
      "7535\n",
      "7536\n",
      "7537\n",
      "7538\n",
      "7539\n",
      "7540\n",
      "7541\n",
      "7542\n",
      "7543\n",
      "7544\n",
      "7545\n",
      "7546\n",
      "7547\n",
      "7548\n",
      "7549\n",
      "7550\n",
      "7551\n",
      "7552\n",
      "7553\n",
      "7554\n",
      "7555\n",
      "7556\n",
      "7557\n",
      "7558\n",
      "7559\n",
      "7560\n",
      "7561\n",
      "7562\n",
      "7563\n",
      "7564\n",
      "7565\n",
      "7566\n",
      "7567\n",
      "7568\n",
      "7569\n",
      "7570\n",
      "7571\n",
      "7572\n",
      "7573\n",
      "7574\n",
      "7575\n",
      "7576\n",
      "7577\n",
      "7578\n",
      "7579\n",
      "7580\n",
      "7581\n",
      "7582\n",
      "7583\n",
      "7584\n",
      "7585\n",
      "7586\n",
      "7587\n",
      "7588\n",
      "7589\n",
      "7590\n",
      "7591\n",
      "7592\n",
      "7593\n",
      "7594\n",
      "7595\n",
      "7596\n",
      "7597\n",
      "7598\n",
      "7599\n",
      "7600\n",
      "7601\n",
      "7602\n",
      "7603\n",
      "7604\n",
      "7605\n",
      "7606\n",
      "7607\n",
      "7608\n",
      "7609\n",
      "7610\n",
      "7611\n",
      "7612\n",
      "7613\n",
      "7614\n",
      "7615\n",
      "7616\n",
      "7617\n",
      "7618\n",
      "7619\n",
      "7620\n",
      "7621\n",
      "7622\n",
      "7623\n",
      "7624\n",
      "7625\n",
      "7626\n",
      "7627\n",
      "7628\n",
      "7629\n",
      "7630\n",
      "7631\n",
      "7632\n",
      "7633\n",
      "7634\n",
      "7635\n",
      "7636\n",
      "7637\n",
      "7638\n",
      "7639\n",
      "7640\n",
      "7641\n",
      "7642\n",
      "7643\n",
      "7644\n",
      "7645\n",
      "7646\n",
      "7647\n",
      "7648\n",
      "7649\n",
      "7650\n",
      "7651\n",
      "7652\n",
      "7653\n",
      "7654\n",
      "7655\n",
      "7656\n",
      "7657\n",
      "7658\n",
      "7659\n",
      "7660\n",
      "7661\n",
      "7662\n",
      "7663\n",
      "7664\n",
      "7665\n",
      "7666\n",
      "7667\n",
      "7668\n",
      "7669\n",
      "7670\n",
      "7671\n",
      "7672\n",
      "7673\n",
      "7674\n",
      "7675\n",
      "7676\n",
      "7677\n",
      "7678\n",
      "7679\n",
      "7680\n",
      "7681\n",
      "7682\n",
      "7683\n",
      "7684\n",
      "7685\n",
      "7686\n",
      "7687\n",
      "7688\n",
      "7689\n",
      "7690\n",
      "7691\n",
      "7692\n",
      "7693\n",
      "7694\n",
      "7695\n",
      "7696\n",
      "7697\n",
      "7698\n",
      "7699\n",
      "7700\n",
      "7701\n",
      "7702\n",
      "7703\n",
      "7704\n",
      "7705\n",
      "7706\n",
      "7707\n",
      "7708\n",
      "7709\n",
      "7710\n",
      "7711\n",
      "7712\n",
      "7713\n",
      "7714\n",
      "7715\n",
      "7716\n",
      "7717\n",
      "7718\n",
      "7719\n",
      "7720\n",
      "7721\n",
      "7722\n",
      "7723\n",
      "7724\n",
      "7725\n",
      "7726\n",
      "7727\n",
      "7728\n",
      "7729\n",
      "7730\n",
      "7731\n",
      "7732\n",
      "7733\n",
      "7734\n",
      "7735\n",
      "7736\n",
      "7737\n",
      "7738\n",
      "7739\n",
      "7740\n",
      "7741\n",
      "7742\n",
      "7743\n",
      "7744\n",
      "7745\n",
      "7746\n",
      "7747\n",
      "7748\n",
      "7749\n",
      "7750\n",
      "7751\n",
      "7752\n",
      "7753\n",
      "7754\n",
      "7755\n",
      "7756\n",
      "7757\n",
      "7758\n",
      "7759\n",
      "7760\n",
      "7761\n",
      "7762\n",
      "7763\n",
      "7764\n",
      "7765\n",
      "7766\n",
      "7767\n",
      "7768\n",
      "7769\n",
      "7770\n",
      "7771\n",
      "7772\n",
      "7773\n",
      "7774\n",
      "7775\n",
      "7776\n",
      "7777\n",
      "7778\n",
      "7779\n",
      "7780\n",
      "7781\n",
      "7782\n",
      "7783\n",
      "7784\n",
      "7785\n",
      "7786\n",
      "7787\n",
      "7788\n",
      "7789\n",
      "7790\n",
      "7791\n",
      "7792\n",
      "7793\n",
      "7794\n",
      "7795\n",
      "7796\n",
      "7797\n",
      "7798\n",
      "7799\n",
      "7800\n",
      "7801\n",
      "7802\n",
      "7803\n",
      "7804\n",
      "7805\n",
      "7806\n",
      "7807\n",
      "7808\n",
      "7809\n",
      "7810\n",
      "7811\n",
      "7812\n",
      "7813\n",
      "7814\n",
      "7815\n",
      "7816\n",
      "7817\n",
      "7818\n",
      "7819\n",
      "7820\n",
      "7821\n",
      "7822\n",
      "7823\n",
      "7824\n",
      "7825\n",
      "7826\n",
      "7827\n",
      "7828\n",
      "7829\n",
      "7830\n",
      "7831\n",
      "7832\n",
      "7833\n",
      "7834\n",
      "7835\n",
      "7836\n",
      "7837\n",
      "7838\n",
      "7839\n",
      "7840\n",
      "7841\n",
      "7842\n",
      "7843\n",
      "7844\n",
      "7845\n",
      "7846\n",
      "7847\n",
      "7848\n",
      "7849\n",
      "7850\n",
      "7851\n",
      "7852\n",
      "7853\n",
      "7854\n",
      "7855\n",
      "7856\n",
      "7857\n",
      "7858\n",
      "7859\n",
      "7860\n",
      "7861\n",
      "7862\n",
      "7863\n",
      "7864\n",
      "7865\n",
      "7866\n",
      "7867\n",
      "7868\n",
      "7869\n",
      "7870\n",
      "7871\n",
      "7872\n",
      "7873\n",
      "7874\n",
      "7875\n",
      "7876\n",
      "7877\n",
      "7878\n",
      "7879\n",
      "7880\n",
      "7881\n",
      "7882\n",
      "7883\n",
      "7884\n",
      "7885\n",
      "7886\n",
      "7887\n",
      "7888\n",
      "7889\n",
      "7890\n",
      "7891\n",
      "7892\n",
      "7893\n",
      "7894\n",
      "7895\n",
      "7896\n",
      "7897\n",
      "7898\n",
      "7899\n",
      "7900\n",
      "7901\n",
      "7902\n",
      "7903\n",
      "7904\n",
      "7905\n",
      "7906\n",
      "7907\n",
      "7908\n",
      "7909\n",
      "7910\n",
      "7911\n",
      "7912\n",
      "7913\n",
      "7914\n",
      "7915\n",
      "7916\n",
      "7917\n",
      "7918\n",
      "7919\n",
      "7920\n",
      "7921\n",
      "7922\n",
      "7923\n",
      "7924\n",
      "7925\n",
      "7926\n",
      "7927\n",
      "7928\n",
      "7929\n",
      "7930\n",
      "7931\n",
      "7932\n",
      "7933\n",
      "7934\n",
      "7935\n",
      "7936\n",
      "7937\n",
      "7938\n",
      "7939\n",
      "7940\n",
      "7941\n",
      "7942\n",
      "7943\n",
      "7944\n",
      "7945\n",
      "7946\n",
      "7947\n",
      "7948\n",
      "7949\n",
      "7950\n",
      "7951\n",
      "7952\n",
      "7953\n",
      "7954\n",
      "7955\n",
      "7956\n",
      "7957\n",
      "7958\n",
      "7959\n",
      "7960\n",
      "7961\n",
      "7962\n",
      "7963\n",
      "7964\n",
      "7965\n",
      "7966\n",
      "7967\n",
      "7968\n",
      "7969\n",
      "7970\n",
      "7971\n",
      "7972\n",
      "7973\n",
      "7974\n",
      "7975\n",
      "7976\n",
      "7977\n",
      "7978\n",
      "7979\n",
      "7980\n",
      "7981\n",
      "7982\n",
      "7983\n",
      "7984\n",
      "7985\n",
      "7986\n",
      "7987\n",
      "7988\n",
      "7989\n",
      "7990\n",
      "7991\n",
      "7992\n",
      "7993\n",
      "7994\n",
      "7995\n",
      "7996\n",
      "7997\n",
      "7998\n",
      "7999\n",
      "8000\n",
      "8001\n",
      "8002\n",
      "8003\n",
      "8004\n",
      "8005\n",
      "8006\n",
      "8007\n",
      "8008\n",
      "8009\n",
      "8010\n",
      "8011\n",
      "8012\n",
      "8013\n",
      "8014\n",
      "8015\n",
      "8016\n",
      "8017\n",
      "8018\n",
      "8019\n",
      "8020\n",
      "8021\n",
      "8022\n",
      "8023\n",
      "8024\n",
      "8025\n",
      "8026\n",
      "8027\n",
      "8028\n",
      "8029\n",
      "8030\n",
      "8031\n",
      "8032\n",
      "8033\n",
      "8034\n",
      "8035\n",
      "8036\n",
      "8037\n",
      "8038\n",
      "8039\n",
      "8040\n",
      "8041\n",
      "8042\n",
      "8043\n",
      "8044\n",
      "8045\n",
      "8046\n",
      "8047\n",
      "8048\n",
      "8049\n",
      "8050\n",
      "8051\n",
      "8052\n",
      "8053\n",
      "8054\n",
      "8055\n",
      "8056\n",
      "8057\n",
      "8058\n",
      "8059\n",
      "8060\n",
      "8061\n",
      "8062\n",
      "8063\n",
      "8064\n",
      "8065\n",
      "8066\n",
      "8067\n",
      "8068\n",
      "8069\n",
      "8070\n",
      "8071\n",
      "8072\n",
      "8073\n",
      "8074\n",
      "8075\n",
      "8076\n",
      "8077\n",
      "8078\n",
      "8079\n",
      "8080\n",
      "8081\n",
      "8082\n",
      "8083\n",
      "8084\n",
      "8085\n",
      "8086\n",
      "8087\n",
      "8088\n",
      "8089\n",
      "8090\n",
      "8091\n",
      "8092\n",
      "8093\n",
      "8094\n",
      "8095\n",
      "8096\n",
      "8097\n",
      "8098\n",
      "8099\n",
      "8100\n",
      "8101\n",
      "8102\n",
      "8103\n",
      "8104\n",
      "8105\n",
      "8106\n",
      "8107\n",
      "8108\n",
      "8109\n",
      "8110\n",
      "8111\n",
      "8112\n",
      "8113\n",
      "8114\n",
      "8115\n",
      "8116\n",
      "8117\n",
      "8118\n",
      "8119\n",
      "8120\n",
      "8121\n",
      "8122\n",
      "8123\n",
      "8124\n",
      "8125\n",
      "8126\n",
      "8127\n",
      "8128\n",
      "8129\n",
      "8130\n",
      "8131\n",
      "8132\n",
      "8133\n",
      "8134\n",
      "8135\n",
      "8136\n",
      "8137\n",
      "8138\n",
      "8139\n",
      "8140\n",
      "8141\n",
      "8142\n",
      "8143\n",
      "8144\n",
      "8145\n",
      "8146\n",
      "8147\n",
      "8148\n",
      "8149\n",
      "8150\n",
      "8151\n",
      "8152\n",
      "8153\n",
      "8154\n",
      "8155\n",
      "8156\n",
      "8157\n",
      "8158\n",
      "8159\n",
      "8160\n",
      "8161\n",
      "8162\n",
      "8163\n",
      "8164\n",
      "8165\n",
      "8166\n",
      "8167\n",
      "8168\n",
      "8169\n",
      "8170\n",
      "8171\n",
      "8172\n",
      "8173\n",
      "8174\n",
      "8175\n",
      "8176\n",
      "8177\n",
      "8178\n",
      "8179\n",
      "8180\n",
      "8181\n",
      "8182\n",
      "8183\n",
      "8184\n",
      "8185\n",
      "8186\n",
      "8187\n",
      "8188\n",
      "8189\n",
      "8190\n",
      "8191\n",
      "8192\n",
      "8193\n",
      "8194\n",
      "8195\n",
      "8196\n",
      "8197\n",
      "8198\n",
      "8199\n",
      "8200\n",
      "8201\n",
      "8202\n",
      "8203\n",
      "8204\n",
      "8205\n",
      "8206\n",
      "8207\n",
      "8208\n",
      "8209\n",
      "8210\n",
      "8211\n",
      "8212\n",
      "8213\n",
      "8214\n",
      "8215\n",
      "8216\n",
      "8217\n",
      "8218\n",
      "8219\n",
      "8220\n",
      "8221\n",
      "8222\n",
      "8223\n",
      "8224\n",
      "8225\n",
      "8226\n",
      "8227\n",
      "8228\n",
      "8229\n",
      "8230\n",
      "8231\n",
      "8232\n",
      "8233\n",
      "8234\n",
      "8235\n",
      "8236\n",
      "8237\n",
      "8238\n",
      "8239\n",
      "8240\n",
      "8241\n",
      "8242\n",
      "8243\n",
      "8244\n",
      "8245\n",
      "8246\n",
      "8247\n",
      "8248\n",
      "8249\n",
      "8250\n",
      "8251\n",
      "8252\n",
      "8253\n",
      "8254\n",
      "8255\n",
      "8256\n",
      "8257\n",
      "8258\n",
      "8259\n",
      "8260\n",
      "8261\n",
      "8262\n",
      "8263\n",
      "8264\n",
      "8265\n",
      "8266\n",
      "8267\n",
      "8268\n",
      "8269\n",
      "8270\n",
      "8271\n",
      "8272\n",
      "8273\n",
      "8274\n",
      "8275\n",
      "8276\n",
      "8277\n",
      "8278\n",
      "8279\n",
      "8280\n",
      "8281\n",
      "8282\n",
      "8283\n",
      "8284\n",
      "8285\n",
      "8286\n",
      "8287\n",
      "8288\n",
      "8289\n",
      "8290\n",
      "8291\n",
      "8292\n",
      "8293\n",
      "8294\n",
      "8295\n",
      "8296\n",
      "8297\n",
      "8298\n",
      "8299\n",
      "8300\n",
      "8301\n",
      "8302\n",
      "8303\n",
      "8304\n",
      "8305\n",
      "8306\n",
      "8307\n",
      "8308\n",
      "8309\n",
      "8310\n",
      "8311\n",
      "8312\n",
      "8313\n",
      "8314\n",
      "8315\n",
      "8316\n",
      "8317\n",
      "8318\n",
      "8319\n",
      "8320\n",
      "8321\n",
      "8322\n",
      "8323\n",
      "8324\n",
      "8325\n",
      "8326\n",
      "8327\n",
      "8328\n",
      "8329\n",
      "8330\n",
      "8331\n",
      "8332\n",
      "8333\n",
      "8334\n",
      "8335\n",
      "8336\n",
      "8337\n",
      "8338\n",
      "8339\n",
      "8340\n",
      "8341\n",
      "8342\n",
      "8343\n",
      "8344\n",
      "8345\n",
      "8346\n",
      "8347\n",
      "8348\n",
      "8349\n",
      "8350\n",
      "8351\n",
      "8352\n",
      "8353\n",
      "8354\n",
      "8355\n",
      "8356\n",
      "8357\n",
      "8358\n",
      "8359\n",
      "8360\n",
      "8361\n",
      "8362\n",
      "8363\n",
      "8364\n",
      "8365\n",
      "8366\n",
      "8367\n",
      "8368\n",
      "8369\n",
      "8370\n",
      "8371\n",
      "8372\n",
      "8373\n",
      "8374\n",
      "8375\n",
      "8376\n",
      "8377\n",
      "8378\n",
      "8379\n",
      "8380\n",
      "8381\n",
      "8382\n",
      "8383\n",
      "8384\n",
      "8385\n",
      "8386\n",
      "8387\n",
      "8388\n",
      "8389\n",
      "8390\n",
      "8391\n",
      "8392\n",
      "8393\n",
      "8394\n",
      "8395\n",
      "8396\n",
      "8397\n",
      "8398\n",
      "8399\n",
      "8400\n",
      "8401\n",
      "8402\n",
      "8403\n",
      "8404\n",
      "8405\n",
      "8406\n",
      "8407\n",
      "8408\n",
      "8409\n",
      "8410\n",
      "8411\n",
      "8412\n",
      "8413\n",
      "8414\n",
      "8415\n",
      "8416\n",
      "8417\n",
      "8418\n",
      "8419\n",
      "8420\n",
      "8421\n",
      "8422\n",
      "8423\n",
      "8424\n",
      "8425\n",
      "8426\n",
      "8427\n",
      "8428\n",
      "8429\n",
      "8430\n",
      "8431\n",
      "8432\n",
      "8433\n",
      "8434\n",
      "8435\n",
      "8436\n",
      "8437\n",
      "8438\n",
      "8439\n",
      "8440\n",
      "8441\n",
      "8442\n",
      "8443\n",
      "8444\n",
      "8445\n",
      "8446\n",
      "8447\n",
      "8448\n",
      "8449\n",
      "8450\n",
      "8451\n",
      "8452\n",
      "8453\n",
      "8454\n",
      "8455\n",
      "8456\n",
      "8457\n",
      "8458\n",
      "8459\n",
      "8460\n",
      "8461\n",
      "8462\n",
      "8463\n",
      "8464\n",
      "8465\n",
      "8466\n",
      "8467\n",
      "8468\n",
      "8469\n",
      "8470\n",
      "8471\n",
      "8472\n",
      "8473\n",
      "8474\n",
      "8475\n",
      "8476\n",
      "8477\n",
      "8478\n",
      "8479\n",
      "8480\n",
      "8481\n",
      "8482\n",
      "8483\n",
      "8484\n",
      "8485\n",
      "8486\n",
      "8487\n",
      "8488\n",
      "8489\n",
      "8490\n",
      "8491\n",
      "8492\n",
      "8493\n",
      "8494\n",
      "8495\n",
      "8496\n",
      "8497\n",
      "8498\n",
      "8499\n",
      "8500\n",
      "8501\n",
      "8502\n",
      "8503\n",
      "8504\n",
      "8505\n",
      "8506\n",
      "8507\n",
      "8508\n",
      "8509\n",
      "8510\n",
      "8511\n",
      "8512\n",
      "8513\n",
      "8514\n",
      "8515\n",
      "8516\n",
      "8517\n",
      "8518\n",
      "8519\n",
      "8520\n",
      "8521\n",
      "8522\n",
      "8523\n",
      "8524\n",
      "8525\n",
      "8526\n",
      "8527\n",
      "8528\n",
      "8529\n",
      "8530\n",
      "8531\n",
      "8532\n",
      "8533\n",
      "8534\n",
      "8535\n",
      "8536\n",
      "8537\n",
      "8538\n",
      "8539\n",
      "8540\n",
      "8541\n",
      "8542\n",
      "8543\n",
      "8544\n",
      "8545\n",
      "8546\n",
      "8547\n",
      "8548\n",
      "8549\n",
      "8550\n",
      "8551\n",
      "8552\n",
      "8553\n",
      "8554\n",
      "8555\n",
      "8556\n",
      "8557\n",
      "8558\n",
      "8559\n",
      "8560\n",
      "8561\n",
      "8562\n",
      "8563\n",
      "8564\n",
      "8565\n",
      "8566\n",
      "8567\n",
      "8568\n",
      "8569\n",
      "8570\n",
      "8571\n",
      "8572\n",
      "8573\n",
      "8574\n",
      "8575\n",
      "8576\n",
      "8577\n",
      "8578\n",
      "8579\n",
      "8580\n",
      "8581\n",
      "8582\n",
      "8583\n",
      "8584\n",
      "8585\n",
      "8586\n",
      "8587\n",
      "8588\n",
      "8589\n",
      "8590\n",
      "8591\n",
      "8592\n",
      "8593\n",
      "8594\n",
      "8595\n",
      "8596\n",
      "8597\n",
      "8598\n",
      "8599\n",
      "8600\n",
      "8601\n",
      "8602\n",
      "8603\n",
      "8604\n",
      "8605\n",
      "8606\n",
      "8607\n",
      "8608\n",
      "8609\n",
      "8610\n",
      "8611\n",
      "8612\n",
      "8613\n",
      "8614\n",
      "8615\n",
      "8616\n",
      "8617\n",
      "8618\n",
      "8619\n",
      "8620\n",
      "8621\n",
      "8622\n",
      "8623\n",
      "8624\n",
      "8625\n",
      "8626\n",
      "8627\n",
      "8628\n",
      "8629\n",
      "8630\n",
      "8631\n",
      "8632\n",
      "8633\n",
      "8634\n",
      "8635\n",
      "8636\n",
      "8637\n",
      "8638\n",
      "8639\n",
      "8640\n",
      "8641\n",
      "8642\n",
      "8643\n",
      "8644\n",
      "8645\n",
      "8646\n",
      "8647\n",
      "8648\n",
      "8649\n",
      "8650\n",
      "8651\n",
      "8652\n",
      "8653\n",
      "8654\n",
      "8655\n",
      "8656\n",
      "8657\n",
      "8658\n",
      "8659\n",
      "8660\n",
      "8661\n",
      "8662\n",
      "8663\n",
      "8664\n",
      "8665\n",
      "8666\n",
      "8667\n",
      "8668\n",
      "8669\n",
      "8670\n",
      "8671\n",
      "8672\n",
      "8673\n",
      "8674\n",
      "8675\n",
      "8676\n",
      "8677\n",
      "8678\n",
      "8679\n",
      "8680\n",
      "8681\n",
      "8682\n",
      "8683\n",
      "8684\n",
      "8685\n",
      "8686\n",
      "8687\n",
      "8688\n",
      "8689\n",
      "8690\n",
      "8691\n",
      "8692\n",
      "8693\n",
      "8694\n",
      "8695\n",
      "8696\n",
      "8697\n",
      "8698\n",
      "8699\n",
      "8700\n",
      "8701\n",
      "8702\n",
      "8703\n",
      "8704\n",
      "8705\n",
      "8706\n",
      "8707\n",
      "8708\n",
      "8709\n",
      "8710\n",
      "8711\n",
      "8712\n",
      "8713\n",
      "8714\n",
      "8715\n",
      "8716\n",
      "8717\n",
      "8718\n",
      "8719\n",
      "8720\n",
      "8721\n",
      "8722\n",
      "8723\n",
      "8724\n",
      "8725\n",
      "8726\n",
      "8727\n",
      "8728\n",
      "8729\n",
      "8730\n",
      "8731\n",
      "8732\n",
      "8733\n",
      "8734\n",
      "8735\n",
      "8736\n",
      "8737\n",
      "8738\n",
      "8739\n",
      "8740\n",
      "8741\n",
      "8742\n",
      "8743\n",
      "8744\n",
      "8745\n",
      "8746\n",
      "8747\n",
      "8748\n",
      "8749\n",
      "8750\n",
      "8751\n",
      "8752\n",
      "8753\n",
      "8754\n",
      "8755\n",
      "8756\n",
      "8757\n",
      "8758\n",
      "8759\n",
      "8760\n",
      "8761\n",
      "8762\n",
      "8763\n",
      "8764\n",
      "8765\n",
      "8766\n",
      "8767\n",
      "8768\n",
      "8769\n",
      "8770\n",
      "8771\n",
      "8772\n",
      "8773\n",
      "8774\n",
      "8775\n",
      "8776\n",
      "8777\n",
      "8778\n",
      "8779\n",
      "8780\n",
      "8781\n",
      "8782\n",
      "8783\n",
      "8784\n",
      "8785\n",
      "8786\n",
      "8787\n",
      "8788\n",
      "8789\n",
      "8790\n",
      "8791\n",
      "8792\n",
      "8793\n",
      "8794\n",
      "8795\n",
      "8796\n",
      "8797\n",
      "8798\n",
      "8799\n",
      "8800\n",
      "8801\n",
      "8802\n",
      "8803\n",
      "8804\n",
      "8805\n",
      "8806\n",
      "8807\n",
      "8808\n",
      "8809\n",
      "8810\n",
      "8811\n",
      "8812\n",
      "8813\n",
      "8814\n",
      "8815\n",
      "8816\n",
      "8817\n",
      "8818\n",
      "8819\n",
      "8820\n",
      "8821\n",
      "8822\n",
      "8823\n",
      "8824\n",
      "8825\n",
      "8826\n",
      "8827\n",
      "8828\n",
      "8829\n",
      "8830\n",
      "8831\n",
      "8832\n",
      "8833\n",
      "8834\n",
      "8835\n",
      "8836\n",
      "8837\n",
      "8838\n",
      "8839\n",
      "8840\n",
      "8841\n",
      "8842\n",
      "8843\n",
      "8844\n",
      "8845\n",
      "8846\n",
      "8847\n",
      "8848\n",
      "8849\n",
      "8850\n",
      "8851\n",
      "8852\n",
      "8853\n",
      "8854\n",
      "8855\n",
      "8856\n",
      "8857\n",
      "8858\n",
      "8859\n",
      "8860\n",
      "8861\n",
      "8862\n",
      "8863\n",
      "8864\n",
      "8865\n",
      "8866\n",
      "8867\n",
      "8868\n",
      "8869\n",
      "8870\n",
      "8871\n",
      "8872\n",
      "8873\n",
      "8874\n",
      "8875\n",
      "8876\n",
      "8877\n",
      "8878\n",
      "8879\n",
      "8880\n",
      "8881\n",
      "8882\n",
      "8883\n",
      "8884\n",
      "8885\n",
      "8886\n",
      "8887\n",
      "8888\n",
      "8889\n",
      "8890\n",
      "8891\n",
      "8892\n",
      "8893\n",
      "8894\n",
      "8895\n",
      "8896\n",
      "8897\n",
      "8898\n",
      "8899\n",
      "8900\n",
      "8901\n",
      "8902\n",
      "8903\n",
      "8904\n",
      "8905\n",
      "8906\n",
      "8907\n",
      "8908\n",
      "8909\n",
      "8910\n",
      "8911\n",
      "8912\n",
      "8913\n",
      "8914\n",
      "8915\n",
      "8916\n",
      "8917\n",
      "8918\n",
      "8919\n",
      "8920\n",
      "8921\n",
      "8922\n",
      "8923\n",
      "8924\n",
      "8925\n",
      "8926\n",
      "8927\n",
      "8928\n",
      "8929\n",
      "8930\n",
      "8931\n",
      "8932\n",
      "8933\n",
      "8934\n",
      "8935\n",
      "8936\n",
      "8937\n",
      "8938\n",
      "8939\n",
      "8940\n",
      "8941\n",
      "8942\n",
      "8943\n",
      "8944\n",
      "8945\n",
      "8946\n",
      "8947\n",
      "8948\n",
      "8949\n",
      "8950\n",
      "8951\n",
      "8952\n",
      "8953\n",
      "8954\n",
      "8955\n",
      "8956\n",
      "8957\n",
      "8958\n",
      "8959\n",
      "8960\n",
      "8961\n",
      "8962\n",
      "8963\n",
      "8964\n",
      "8965\n",
      "8966\n",
      "8967\n",
      "8968\n",
      "8969\n",
      "8970\n",
      "8971\n",
      "8972\n",
      "8973\n",
      "8974\n",
      "8975\n",
      "8976\n",
      "8977\n",
      "8978\n",
      "8979\n",
      "8980\n",
      "8981\n",
      "8982\n",
      "8983\n",
      "8984\n",
      "8985\n",
      "8986\n",
      "8987\n",
      "8988\n",
      "8989\n",
      "8990\n",
      "8991\n",
      "8992\n",
      "8993\n",
      "8994\n",
      "8995\n",
      "8996\n",
      "8997\n",
      "8998\n",
      "8999\n",
      "9000\n",
      "9001\n",
      "9002\n",
      "9003\n",
      "9004\n",
      "9005\n",
      "9006\n",
      "9007\n",
      "9008\n",
      "9009\n",
      "9010\n",
      "9011\n",
      "9012\n",
      "9013\n",
      "9014\n",
      "9015\n",
      "9016\n",
      "9017\n",
      "9018\n",
      "9019\n",
      "9020\n",
      "9021\n",
      "9022\n",
      "9023\n",
      "9024\n",
      "9025\n",
      "9026\n",
      "9027\n",
      "9028\n",
      "9029\n",
      "9030\n",
      "9031\n",
      "9032\n",
      "9033\n",
      "9034\n",
      "9035\n",
      "9036\n",
      "9037\n",
      "9038\n",
      "9039\n",
      "9040\n",
      "9041\n",
      "9042\n",
      "9043\n",
      "9044\n",
      "9045\n",
      "9046\n",
      "9047\n",
      "9048\n",
      "9049\n",
      "9050\n",
      "9051\n",
      "9052\n",
      "9053\n",
      "9054\n",
      "9055\n",
      "9056\n",
      "9057\n",
      "9058\n",
      "9059\n",
      "9060\n",
      "9061\n",
      "9062\n",
      "9063\n",
      "9064\n",
      "9065\n",
      "9066\n",
      "9067\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "dir_path = \"/content/gdrive/MyDrive/GDS/walks/with_onehot\"\n",
    "\n",
    "import random\n",
    "\n",
    "num_neighbors =  15 # num_walks * (walk_length - 1)\n",
    "\n",
    "num_walks_primary = 40\n",
    "walk_length = 10 # 3\n",
    "\n",
    "min_neighbors_tolerance = 10\n",
    "max_num_secondary_rws = 8\n",
    "\n",
    "\n",
    "walk_length_list = [5, 10]\n",
    "walk_params_list = [[2, 2, 2, 2],\n",
    "                [5, 10, 1, 1],\n",
    "               [5, 10, 5, 10],\n",
    "               [10, 10, 1 ,1],\n",
    "               [1, 1, 1, 1]]\n",
    "\n",
    "for walk_length in walk_length_list:\n",
    "\n",
    "  for walk_params in walk_params_list:\n",
    "\n",
    "    name_extension = \"wl_\" + str(walk_length)\n",
    "    for x in walk_params:\n",
    "      name_extension += \"_\" + str(x)\n",
    "\n",
    "    print(name_extension)\n",
    "\n",
    "    random.seed(564)\n",
    "    np.random.seed(456)\n",
    "\n",
    "    pyg_graph, neigh_matrix, negative_array, node_list  = get_pyg_object(\"Graph_er_weighted_wfeatures_onehot\", \"Graph_er\", \"/content/gdrive/MyDrive/GDS/pickles/\", \n",
    "                                                                         walk_params[0], walk_params[1], walk_params[2], walk_params[3],\n",
    "                                                                          num_neighbors, walk_length, num_walks_primary, min_neighbors_tolerance, \n",
    "                                                                         max_num_secondary_rws , pages_to_remove)\n",
    "\n",
    "\n",
    "    dump_pickle_file(pyg_graph, \"pyg_graph_\" + name_extension, dir_path)\n",
    "    dump_pickle_file(neigh_matrix, \"neigh_matrix_\" + name_extension, dir_path)\n",
    "    dump_pickle_file(negative_array, \"negative_array_\" + name_extension, dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EvSt63ellYD0"
   },
   "source": [
    "# Generate alternative walks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WKDNBSw1xmB"
   },
   "source": [
    "# Fit models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPveT-8XZgke"
   },
   "source": [
    "## Edgeweight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "82YtrSK6lWRx",
    "outputId": "9cd8728c-a1de-4484-88dd-cbd2ce32671e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wl_5_2_2_2_2\n",
      "GCN_edgeweight(\n",
      "  (conv1): GCNConv(1555, 128)\n",
      "  (conv2): GCNConv(128, 16)\n",
      "  (linear): Linear(in_features=16, out_features=10, bias=True)\n",
      ")\n",
      "tensor(1.4081, grad_fn=<DivBackward0>)\n",
      "tensor(1.4081, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 0\n",
      "Epoch: 0 loss:  1.4080994129180908\n",
      "tensor(1.3945, grad_fn=<DivBackward0>)\n",
      "tensor(1.3945, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 1\n",
      "Epoch: 1 loss:  1.39447021484375\n",
      "tensor(1.3779, grad_fn=<DivBackward0>)\n",
      "tensor(1.3775, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 2\n",
      "Epoch: 2 loss:  1.3779343366622925\n",
      "tensor(1.3684, grad_fn=<DivBackward0>)\n",
      "tensor(1.3675, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 3\n",
      "Epoch: 3 loss:  1.36841881275177\n",
      "tensor(1.3522, grad_fn=<DivBackward0>)\n",
      "tensor(1.3518, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 4\n",
      "Epoch: 4 loss:  1.3521831035614014\n",
      "tensor(1.3451, grad_fn=<DivBackward0>)\n",
      "tensor(1.3448, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 5\n",
      "Epoch: 5 loss:  1.345137119293213\n",
      "tensor(1.3319, grad_fn=<DivBackward0>)\n",
      "tensor(1.3315, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 6\n",
      "Epoch: 6 loss:  1.3319379091262817\n",
      "tensor(1.3224, grad_fn=<DivBackward0>)\n",
      "tensor(1.3213, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 7\n",
      "Epoch: 7 loss:  1.3224108219146729\n",
      "tensor(1.3183, grad_fn=<DivBackward0>)\n",
      "tensor(1.3169, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 8\n",
      "Epoch: 8 loss:  1.318274974822998\n",
      "tensor(1.3091, grad_fn=<DivBackward0>)\n",
      "tensor(1.3087, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 9\n",
      "Epoch: 9 loss:  1.3091384172439575\n",
      "tensor(1.3087, grad_fn=<DivBackward0>)\n",
      "tensor(1.3084, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 10\n",
      "Epoch: 10 loss:  1.3087480068206787\n",
      "tensor(1.3049, grad_fn=<DivBackward0>)\n",
      "tensor(1.3049, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 11\n",
      "Epoch: 11 loss:  1.3048545122146606\n",
      "tensor(1.3035, grad_fn=<DivBackward0>)\n",
      "tensor(1.3029, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 12\n",
      "Epoch: 12 loss:  1.303485631942749\n",
      "tensor(1.2994, grad_fn=<DivBackward0>)\n",
      "tensor(1.2992, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 13\n",
      "Epoch: 13 loss:  1.2993927001953125\n",
      "tensor(1.2939, grad_fn=<DivBackward0>)\n",
      "tensor(1.2944, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 14\n",
      "Epoch: 14 loss:  1.2939389944076538\n",
      "tensor(1.2911, grad_fn=<DivBackward0>)\n",
      "tensor(1.2924, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 15\n",
      "Epoch: 15 loss:  1.2910898923873901\n",
      "tensor(1.2875, grad_fn=<DivBackward0>)\n",
      "tensor(1.2890, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 16\n",
      "Epoch: 16 loss:  1.287496566772461\n",
      "tensor(1.2854, grad_fn=<DivBackward0>)\n",
      "tensor(1.2873, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 17\n",
      "Epoch: 17 loss:  1.2853741645812988\n",
      "tensor(1.2820, grad_fn=<DivBackward0>)\n",
      "tensor(1.2849, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 18\n",
      "Epoch: 18 loss:  1.2819898128509521\n",
      "tensor(1.2805, grad_fn=<DivBackward0>)\n",
      "tensor(1.2834, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 19\n",
      "Epoch: 19 loss:  1.2805359363555908\n",
      "tensor(1.2771, grad_fn=<DivBackward0>)\n",
      "tensor(1.2811, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 20\n",
      "Epoch: 20 loss:  1.2771118879318237\n",
      "tensor(1.2768, grad_fn=<DivBackward0>)\n",
      "tensor(1.2793, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 21\n",
      "Epoch: 21 loss:  1.276815414428711\n",
      "tensor(1.2745, grad_fn=<DivBackward0>)\n",
      "tensor(1.2775, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 22\n",
      "Epoch: 22 loss:  1.2744832038879395\n",
      "tensor(1.2732, grad_fn=<DivBackward0>)\n",
      "tensor(1.2762, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 23\n",
      "Epoch: 23 loss:  1.2731598615646362\n",
      "tensor(1.2719, grad_fn=<DivBackward0>)\n",
      "tensor(1.2737, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 24\n",
      "Epoch: 24 loss:  1.2719168663024902\n",
      "tensor(1.2702, grad_fn=<DivBackward0>)\n",
      "tensor(1.2722, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 25\n",
      "Epoch: 25 loss:  1.2702302932739258\n",
      "tensor(1.2691, grad_fn=<DivBackward0>)\n",
      "tensor(1.2718, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 26\n",
      "Epoch: 26 loss:  1.2691054344177246\n",
      "tensor(1.2685, grad_fn=<DivBackward0>)\n",
      "tensor(1.2712, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 27\n",
      "Epoch: 27 loss:  1.2685357332229614\n",
      "tensor(1.2673, grad_fn=<DivBackward0>)\n",
      "tensor(1.2692, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 28\n",
      "Epoch: 28 loss:  1.2673180103302002\n",
      "tensor(1.2660, grad_fn=<DivBackward0>)\n",
      "tensor(1.2690, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 29\n",
      "Epoch: 29 loss:  1.2659870386123657\n",
      "tensor(1.2661, grad_fn=<DivBackward0>)\n",
      "tensor(1.2693, grad_fn=<DivBackward0>)\n",
      "Epoch: 30 loss:  1.2661265134811401\n",
      "tensor(1.2645, grad_fn=<DivBackward0>)\n",
      "tensor(1.2668, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 31\n",
      "Epoch: 31 loss:  1.2644635438919067\n",
      "tensor(1.2626, grad_fn=<DivBackward0>)\n",
      "tensor(1.2646, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 32\n",
      "Epoch: 32 loss:  1.2626361846923828\n",
      "tensor(1.2633, grad_fn=<DivBackward0>)\n",
      "tensor(1.2651, grad_fn=<DivBackward0>)\n",
      "Epoch: 33 loss:  1.2632843255996704\n",
      "tensor(1.2628, grad_fn=<DivBackward0>)\n",
      "tensor(1.2669, grad_fn=<DivBackward0>)\n",
      "Epoch: 34 loss:  1.2628339529037476\n",
      "tensor(1.2616, grad_fn=<DivBackward0>)\n",
      "tensor(1.2639, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 35\n",
      "Epoch: 35 loss:  1.2615548372268677\n",
      "tensor(1.2607, grad_fn=<DivBackward0>)\n",
      "tensor(1.2658, grad_fn=<DivBackward0>)\n",
      "Epoch: 36 loss:  1.2607039213180542\n",
      "tensor(1.2614, grad_fn=<DivBackward0>)\n",
      "tensor(1.2660, grad_fn=<DivBackward0>)\n",
      "Epoch: 37 loss:  1.26139497756958\n",
      "tensor(1.2599, grad_fn=<DivBackward0>)\n",
      "tensor(1.2634, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 38\n",
      "Epoch: 38 loss:  1.2599430084228516\n",
      "tensor(1.2598, grad_fn=<DivBackward0>)\n",
      "tensor(1.2631, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 39\n",
      "Epoch: 39 loss:  1.2598152160644531\n",
      "tensor(1.2602, grad_fn=<DivBackward0>)\n",
      "tensor(1.2638, grad_fn=<DivBackward0>)\n",
      "Epoch: 40 loss:  1.260196566581726\n",
      "tensor(1.2583, grad_fn=<DivBackward0>)\n",
      "tensor(1.2632, grad_fn=<DivBackward0>)\n",
      "Epoch: 41 loss:  1.2583444118499756\n",
      "tensor(1.2573, grad_fn=<DivBackward0>)\n",
      "tensor(1.2626, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 42\n",
      "Epoch: 42 loss:  1.2573086023330688\n",
      "tensor(1.2569, grad_fn=<DivBackward0>)\n",
      "tensor(1.2639, grad_fn=<DivBackward0>)\n",
      "Epoch: 43 loss:  1.2568690776824951\n",
      "tensor(1.2566, grad_fn=<DivBackward0>)\n",
      "tensor(1.2605, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 44\n",
      "Epoch: 44 loss:  1.2566295862197876\n",
      "tensor(1.2561, grad_fn=<DivBackward0>)\n",
      "tensor(1.2592, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 45\n",
      "Epoch: 45 loss:  1.2560882568359375\n",
      "tensor(1.2553, grad_fn=<DivBackward0>)\n",
      "tensor(1.2598, grad_fn=<DivBackward0>)\n",
      "Epoch: 46 loss:  1.2552889585494995\n",
      "tensor(1.2554, grad_fn=<DivBackward0>)\n",
      "tensor(1.2620, grad_fn=<DivBackward0>)\n",
      "Epoch: 47 loss:  1.25540030002594\n",
      "tensor(1.2536, grad_fn=<DivBackward0>)\n",
      "tensor(1.2621, grad_fn=<DivBackward0>)\n",
      "Epoch: 48 loss:  1.2536097764968872\n",
      "tensor(1.2533, grad_fn=<DivBackward0>)\n",
      "tensor(1.2605, grad_fn=<DivBackward0>)\n",
      "Epoch: 49 loss:  1.2532809972763062\n",
      "tensor(1.2532, grad_fn=<DivBackward0>)\n",
      "tensor(1.2588, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 50\n",
      "Epoch: 50 loss:  1.2532323598861694\n",
      "tensor(1.2528, grad_fn=<DivBackward0>)\n",
      "tensor(1.2595, grad_fn=<DivBackward0>)\n",
      "Epoch: 51 loss:  1.2528202533721924\n",
      "tensor(1.2518, grad_fn=<DivBackward0>)\n",
      "tensor(1.2570, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 52\n",
      "Epoch: 52 loss:  1.251802682876587\n",
      "tensor(1.2515, grad_fn=<DivBackward0>)\n",
      "tensor(1.2578, grad_fn=<DivBackward0>)\n",
      "Epoch: 53 loss:  1.2515050172805786\n",
      "tensor(1.2501, grad_fn=<DivBackward0>)\n",
      "tensor(1.2578, grad_fn=<DivBackward0>)\n",
      "Epoch: 54 loss:  1.2500770092010498\n",
      "tensor(1.2498, grad_fn=<DivBackward0>)\n",
      "tensor(1.2545, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 55\n",
      "Epoch: 55 loss:  1.2497549057006836\n",
      "tensor(1.2479, grad_fn=<DivBackward0>)\n",
      "tensor(1.2550, grad_fn=<DivBackward0>)\n",
      "Epoch: 56 loss:  1.2479251623153687\n",
      "tensor(1.2477, grad_fn=<DivBackward0>)\n",
      "tensor(1.2556, grad_fn=<DivBackward0>)\n",
      "Epoch: 57 loss:  1.247746229171753\n",
      "tensor(1.2478, grad_fn=<DivBackward0>)\n",
      "tensor(1.2540, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 58\n",
      "Epoch: 58 loss:  1.2478278875350952\n",
      "tensor(1.2464, grad_fn=<DivBackward0>)\n",
      "tensor(1.2551, grad_fn=<DivBackward0>)\n",
      "Epoch: 59 loss:  1.2463897466659546\n",
      "tensor(1.2457, grad_fn=<DivBackward0>)\n",
      "tensor(1.2533, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 60\n",
      "Epoch: 60 loss:  1.2457383871078491\n",
      "tensor(1.2450, grad_fn=<DivBackward0>)\n",
      "tensor(1.2534, grad_fn=<DivBackward0>)\n",
      "Epoch: 61 loss:  1.244980812072754\n",
      "tensor(1.2442, grad_fn=<DivBackward0>)\n",
      "tensor(1.2512, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 62\n",
      "Epoch: 62 loss:  1.2442179918289185\n",
      "tensor(1.2435, grad_fn=<DivBackward0>)\n",
      "tensor(1.2524, grad_fn=<DivBackward0>)\n",
      "Epoch: 63 loss:  1.2434914112091064\n",
      "tensor(1.2430, grad_fn=<DivBackward0>)\n",
      "tensor(1.2533, grad_fn=<DivBackward0>)\n",
      "Epoch: 64 loss:  1.2429534196853638\n",
      "tensor(1.2428, grad_fn=<DivBackward0>)\n",
      "tensor(1.2506, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 65\n",
      "Epoch: 65 loss:  1.2428447008132935\n",
      "tensor(1.2415, grad_fn=<DivBackward0>)\n",
      "tensor(1.2507, grad_fn=<DivBackward0>)\n",
      "Epoch: 66 loss:  1.2415449619293213\n",
      "tensor(1.2415, grad_fn=<DivBackward0>)\n",
      "tensor(1.2523, grad_fn=<DivBackward0>)\n",
      "Epoch: 67 loss:  1.2415070533752441\n",
      "tensor(1.2395, grad_fn=<DivBackward0>)\n",
      "tensor(1.2487, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 68\n",
      "Epoch: 68 loss:  1.2394802570343018\n",
      "tensor(1.2394, grad_fn=<DivBackward0>)\n",
      "tensor(1.2502, grad_fn=<DivBackward0>)\n",
      "Epoch: 69 loss:  1.2394434213638306\n",
      "tensor(1.2383, grad_fn=<DivBackward0>)\n",
      "tensor(1.2504, grad_fn=<DivBackward0>)\n",
      "Epoch: 70 loss:  1.2382838726043701\n",
      "tensor(1.2383, grad_fn=<DivBackward0>)\n",
      "tensor(1.2480, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 71\n",
      "Epoch: 71 loss:  1.2383290529251099\n",
      "tensor(1.2369, grad_fn=<DivBackward0>)\n",
      "tensor(1.2487, grad_fn=<DivBackward0>)\n",
      "Epoch: 72 loss:  1.2368733882904053\n",
      "tensor(1.2388, grad_fn=<DivBackward0>)\n",
      "tensor(1.2495, grad_fn=<DivBackward0>)\n",
      "Epoch: 73 loss:  1.2388492822647095\n",
      "tensor(1.2412, grad_fn=<DivBackward0>)\n",
      "tensor(1.2531, grad_fn=<DivBackward0>)\n",
      "Epoch: 74 loss:  1.2412364482879639\n",
      "tensor(1.2363, grad_fn=<DivBackward0>)\n",
      "tensor(1.2494, grad_fn=<DivBackward0>)\n",
      "Epoch: 75 loss:  1.2362927198410034\n",
      "tensor(1.2348, grad_fn=<DivBackward0>)\n",
      "tensor(1.2442, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 76\n",
      "Epoch: 76 loss:  1.2348145246505737\n",
      "tensor(1.2354, grad_fn=<DivBackward0>)\n",
      "tensor(1.2461, grad_fn=<DivBackward0>)\n",
      "Epoch: 77 loss:  1.235351800918579\n",
      "tensor(1.2363, grad_fn=<DivBackward0>)\n",
      "tensor(1.2481, grad_fn=<DivBackward0>)\n",
      "Epoch: 78 loss:  1.2363189458847046\n",
      "tensor(1.2333, grad_fn=<DivBackward0>)\n",
      "tensor(1.2471, grad_fn=<DivBackward0>)\n",
      "Epoch: 79 loss:  1.2333205938339233\n",
      "tensor(1.2328, grad_fn=<DivBackward0>)\n",
      "tensor(1.2464, grad_fn=<DivBackward0>)\n",
      "Epoch: 80 loss:  1.2328096628189087\n",
      "tensor(1.2338, grad_fn=<DivBackward0>)\n",
      "tensor(1.2504, grad_fn=<DivBackward0>)\n",
      "Epoch: 81 loss:  1.2338247299194336\n",
      "tensor(1.2329, grad_fn=<DivBackward0>)\n",
      "tensor(1.2454, grad_fn=<DivBackward0>)\n",
      "Epoch: 82 loss:  1.2329293489456177\n",
      "tensor(1.2308, grad_fn=<DivBackward0>)\n",
      "tensor(1.2444, grad_fn=<DivBackward0>)\n",
      "Epoch: 83 loss:  1.2307898998260498\n",
      "tensor(1.2329, grad_fn=<DivBackward0>)\n",
      "tensor(1.2454, grad_fn=<DivBackward0>)\n",
      "Epoch: 84 loss:  1.232931137084961\n",
      "tensor(1.2340, grad_fn=<DivBackward0>)\n",
      "tensor(1.2477, grad_fn=<DivBackward0>)\n",
      "Epoch: 85 loss:  1.233978271484375\n",
      "tensor(1.2306, grad_fn=<DivBackward0>)\n",
      "tensor(1.2487, grad_fn=<DivBackward0>)\n",
      "Epoch: 86 loss:  1.2305604219436646\n",
      "tensor(1.2323, grad_fn=<DivBackward0>)\n",
      "tensor(1.2454, grad_fn=<DivBackward0>)\n",
      "Epoch: 87 loss:  1.2322503328323364\n",
      "tensor(1.2325, grad_fn=<DivBackward0>)\n",
      "tensor(1.2427, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 88\n",
      "Epoch: 88 loss:  1.2324652671813965\n",
      "tensor(1.2279, grad_fn=<DivBackward0>)\n",
      "tensor(1.2408, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 89\n",
      "Epoch: 89 loss:  1.2278788089752197\n",
      "tensor(1.2365, grad_fn=<DivBackward0>)\n",
      "tensor(1.2512, grad_fn=<DivBackward0>)\n",
      "Epoch: 90 loss:  1.2364857196807861\n",
      "tensor(1.2342, grad_fn=<DivBackward0>)\n",
      "tensor(1.2475, grad_fn=<DivBackward0>)\n",
      "Epoch: 91 loss:  1.2342431545257568\n",
      "tensor(1.2285, grad_fn=<DivBackward0>)\n",
      "tensor(1.2418, grad_fn=<DivBackward0>)\n",
      "Epoch: 92 loss:  1.2285069227218628\n",
      "tensor(1.2374, grad_fn=<DivBackward0>)\n",
      "tensor(1.2494, grad_fn=<DivBackward0>)\n",
      "Epoch: 93 loss:  1.2374436855316162\n",
      "tensor(1.2272, grad_fn=<DivBackward0>)\n",
      "tensor(1.2413, grad_fn=<DivBackward0>)\n",
      "Epoch: 94 loss:  1.227190613746643\n",
      "tensor(1.2284, grad_fn=<DivBackward0>)\n",
      "tensor(1.2414, grad_fn=<DivBackward0>)\n",
      "Epoch: 95 loss:  1.2284038066864014\n",
      "tensor(1.2263, grad_fn=<DivBackward0>)\n",
      "tensor(1.2385, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 96\n",
      "Epoch: 96 loss:  1.226337194442749\n",
      "tensor(1.2329, grad_fn=<DivBackward0>)\n",
      "tensor(1.2497, grad_fn=<DivBackward0>)\n",
      "Epoch: 97 loss:  1.2328650951385498\n",
      "tensor(1.2271, grad_fn=<DivBackward0>)\n",
      "tensor(1.2407, grad_fn=<DivBackward0>)\n",
      "Epoch: 98 loss:  1.227116346359253\n",
      "tensor(1.2276, grad_fn=<DivBackward0>)\n",
      "tensor(1.2393, grad_fn=<DivBackward0>)\n",
      "Epoch: 99 loss:  1.227567195892334\n",
      "[0.99944395 0.9999238  0.9999777  ... 0.7402279  0.9814845  0.79958355]\n",
      "[0.999887   0.99911153 0.99908507 ... 0.76908875 0.9889535  0.8251176 ]\n",
      "[0.99985725 0.9994954  0.9995191  ... 0.76148117 0.98707473 0.8193039 ]\n",
      "wl_5_5_10_1_1\n",
      "GCN_edgeweight(\n",
      "  (conv1): GCNConv(1555, 128)\n",
      "  (conv2): GCNConv(128, 16)\n",
      "  (linear): Linear(in_features=16, out_features=10, bias=True)\n",
      ")\n",
      "tensor(1.4080, grad_fn=<DivBackward0>)\n",
      "tensor(1.4079, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 0\n",
      "Epoch: 0 loss:  1.4080085754394531\n",
      "tensor(1.3934, grad_fn=<DivBackward0>)\n",
      "tensor(1.3926, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 1\n",
      "Epoch: 1 loss:  1.3933695554733276\n",
      "tensor(1.3761, grad_fn=<DivBackward0>)\n",
      "tensor(1.3740, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 2\n",
      "Epoch: 2 loss:  1.3761074542999268\n",
      "tensor(1.3671, grad_fn=<DivBackward0>)\n",
      "tensor(1.3632, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 3\n",
      "Epoch: 3 loss:  1.3671391010284424\n",
      "tensor(1.3495, grad_fn=<DivBackward0>)\n",
      "tensor(1.3466, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 4\n",
      "Epoch: 4 loss:  1.3495100736618042\n",
      "tensor(1.3425, grad_fn=<DivBackward0>)\n",
      "tensor(1.3402, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 5\n",
      "Epoch: 5 loss:  1.3424931764602661\n",
      "tensor(1.3291, grad_fn=<DivBackward0>)\n",
      "tensor(1.3260, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 6\n",
      "Epoch: 6 loss:  1.3291409015655518\n",
      "tensor(1.3192, grad_fn=<DivBackward0>)\n",
      "tensor(1.3136, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 7\n",
      "Epoch: 7 loss:  1.3191624879837036\n",
      "tensor(1.3157, grad_fn=<DivBackward0>)\n",
      "tensor(1.3087, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 8\n",
      "Epoch: 8 loss:  1.315685749053955\n",
      "tensor(1.3057, grad_fn=<DivBackward0>)\n",
      "tensor(1.3005, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 9\n",
      "Epoch: 9 loss:  1.3057079315185547\n",
      "tensor(1.3055, grad_fn=<DivBackward0>)\n",
      "tensor(1.3013, grad_fn=<DivBackward0>)\n",
      "Epoch: 10 loss:  1.3054898977279663\n",
      "tensor(1.3017, grad_fn=<DivBackward0>)\n",
      "tensor(1.2968, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 11\n",
      "Epoch: 11 loss:  1.301700234413147\n",
      "tensor(1.2997, grad_fn=<DivBackward0>)\n",
      "tensor(1.2932, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 12\n",
      "Epoch: 12 loss:  1.2996845245361328\n",
      "tensor(1.2966, grad_fn=<DivBackward0>)\n",
      "tensor(1.2899, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 13\n",
      "Epoch: 13 loss:  1.296589970588684\n",
      "tensor(1.2904, grad_fn=<DivBackward0>)\n",
      "tensor(1.2864, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 14\n",
      "Epoch: 14 loss:  1.2904348373413086\n",
      "tensor(1.2880, grad_fn=<DivBackward0>)\n",
      "tensor(1.2840, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 15\n",
      "Epoch: 15 loss:  1.2879815101623535\n",
      "tensor(1.2839, grad_fn=<DivBackward0>)\n",
      "tensor(1.2796, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 16\n",
      "Epoch: 16 loss:  1.2839142084121704\n",
      "tensor(1.2821, grad_fn=<DivBackward0>)\n",
      "tensor(1.2780, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 17\n",
      "Epoch: 17 loss:  1.2821179628372192\n",
      "tensor(1.2782, grad_fn=<DivBackward0>)\n",
      "tensor(1.2742, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 18\n",
      "Epoch: 18 loss:  1.2781867980957031\n",
      "tensor(1.2769, grad_fn=<DivBackward0>)\n",
      "tensor(1.2749, grad_fn=<DivBackward0>)\n",
      "Epoch: 19 loss:  1.276948094367981\n",
      "tensor(1.2734, grad_fn=<DivBackward0>)\n",
      "tensor(1.2723, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 20\n",
      "Epoch: 20 loss:  1.2734367847442627\n",
      "tensor(1.2733, grad_fn=<DivBackward0>)\n",
      "tensor(1.2701, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 21\n",
      "Epoch: 21 loss:  1.2732938528060913\n",
      "tensor(1.2705, grad_fn=<DivBackward0>)\n",
      "tensor(1.2698, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 22\n",
      "Epoch: 22 loss:  1.2705135345458984\n",
      "tensor(1.2698, grad_fn=<DivBackward0>)\n",
      "tensor(1.2687, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 23\n",
      "Epoch: 23 loss:  1.2697741985321045\n",
      "tensor(1.2679, grad_fn=<DivBackward0>)\n",
      "tensor(1.2664, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 24\n",
      "Epoch: 24 loss:  1.2679290771484375\n",
      "tensor(1.2666, grad_fn=<DivBackward0>)\n",
      "tensor(1.2651, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 25\n",
      "Epoch: 25 loss:  1.2665770053863525\n",
      "tensor(1.2654, grad_fn=<DivBackward0>)\n",
      "tensor(1.2663, grad_fn=<DivBackward0>)\n",
      "Epoch: 26 loss:  1.2653969526290894\n",
      "tensor(1.2640, grad_fn=<DivBackward0>)\n",
      "tensor(1.2641, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 27\n",
      "Epoch: 27 loss:  1.2640498876571655\n",
      "tensor(1.2633, grad_fn=<DivBackward0>)\n",
      "tensor(1.2648, grad_fn=<DivBackward0>)\n",
      "Epoch: 28 loss:  1.2633379697799683\n",
      "tensor(1.2626, grad_fn=<DivBackward0>)\n",
      "tensor(1.2662, grad_fn=<DivBackward0>)\n",
      "Epoch: 29 loss:  1.2626279592514038\n",
      "tensor(1.2622, grad_fn=<DivBackward0>)\n",
      "tensor(1.2630, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 30\n",
      "Epoch: 30 loss:  1.2622483968734741\n",
      "tensor(1.2596, grad_fn=<DivBackward0>)\n",
      "tensor(1.2626, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 31\n",
      "Epoch: 31 loss:  1.259610891342163\n",
      "tensor(1.2588, grad_fn=<DivBackward0>)\n",
      "tensor(1.2615, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 32\n",
      "Epoch: 32 loss:  1.2588043212890625\n",
      "tensor(1.2587, grad_fn=<DivBackward0>)\n",
      "tensor(1.2602, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 33\n",
      "Epoch: 33 loss:  1.2587051391601562\n",
      "tensor(1.2587, grad_fn=<DivBackward0>)\n",
      "tensor(1.2601, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 34\n",
      "Epoch: 34 loss:  1.2587475776672363\n",
      "tensor(1.2575, grad_fn=<DivBackward0>)\n",
      "tensor(1.2602, grad_fn=<DivBackward0>)\n",
      "Epoch: 35 loss:  1.257467269897461\n",
      "tensor(1.2566, grad_fn=<DivBackward0>)\n",
      "tensor(1.2574, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 36\n",
      "Epoch: 36 loss:  1.2565935850143433\n",
      "tensor(1.2570, grad_fn=<DivBackward0>)\n",
      "tensor(1.2594, grad_fn=<DivBackward0>)\n",
      "Epoch: 37 loss:  1.2569634914398193\n",
      "tensor(1.2556, grad_fn=<DivBackward0>)\n",
      "tensor(1.2574, grad_fn=<DivBackward0>)\n",
      "Epoch: 38 loss:  1.2555882930755615\n",
      "tensor(1.2557, grad_fn=<DivBackward0>)\n",
      "tensor(1.2582, grad_fn=<DivBackward0>)\n",
      "Epoch: 39 loss:  1.255722165107727\n",
      "tensor(1.2552, grad_fn=<DivBackward0>)\n",
      "tensor(1.2590, grad_fn=<DivBackward0>)\n",
      "Epoch: 40 loss:  1.2552114725112915\n",
      "tensor(1.2545, grad_fn=<DivBackward0>)\n",
      "tensor(1.2559, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 41\n",
      "Epoch: 41 loss:  1.25448477268219\n",
      "tensor(1.2561, grad_fn=<DivBackward0>)\n",
      "tensor(1.2611, grad_fn=<DivBackward0>)\n",
      "Epoch: 42 loss:  1.2560662031173706\n",
      "tensor(1.2545, grad_fn=<DivBackward0>)\n",
      "tensor(1.2564, grad_fn=<DivBackward0>)\n",
      "Epoch: 43 loss:  1.254453182220459\n",
      "tensor(1.2521, grad_fn=<DivBackward0>)\n",
      "tensor(1.2553, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 44\n",
      "Epoch: 44 loss:  1.252102255821228\n",
      "tensor(1.2524, grad_fn=<DivBackward0>)\n",
      "tensor(1.2571, grad_fn=<DivBackward0>)\n",
      "Epoch: 45 loss:  1.2524148225784302\n",
      "tensor(1.2528, grad_fn=<DivBackward0>)\n",
      "tensor(1.2548, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 46\n",
      "Epoch: 46 loss:  1.2528355121612549\n",
      "tensor(1.2519, grad_fn=<DivBackward0>)\n",
      "tensor(1.2548, grad_fn=<DivBackward0>)\n",
      "Epoch: 47 loss:  1.2518560886383057\n",
      "tensor(1.2497, grad_fn=<DivBackward0>)\n",
      "tensor(1.2555, grad_fn=<DivBackward0>)\n",
      "Epoch: 48 loss:  1.2497482299804688\n",
      "tensor(1.2503, grad_fn=<DivBackward0>)\n",
      "tensor(1.2535, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 49\n",
      "Epoch: 49 loss:  1.25032377243042\n",
      "tensor(1.2485, grad_fn=<DivBackward0>)\n",
      "tensor(1.2533, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 50\n",
      "Epoch: 50 loss:  1.248476505279541\n",
      "tensor(1.2489, grad_fn=<DivBackward0>)\n",
      "tensor(1.2534, grad_fn=<DivBackward0>)\n",
      "Epoch: 51 loss:  1.2488901615142822\n",
      "tensor(1.2482, grad_fn=<DivBackward0>)\n",
      "tensor(1.2505, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 52\n",
      "Epoch: 52 loss:  1.2481836080551147\n",
      "tensor(1.2478, grad_fn=<DivBackward0>)\n",
      "tensor(1.2538, grad_fn=<DivBackward0>)\n",
      "Epoch: 53 loss:  1.2478283643722534\n",
      "tensor(1.2470, grad_fn=<DivBackward0>)\n",
      "tensor(1.2513, grad_fn=<DivBackward0>)\n",
      "Epoch: 54 loss:  1.246987223625183\n",
      "tensor(1.2459, grad_fn=<DivBackward0>)\n",
      "tensor(1.2500, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 55\n",
      "Epoch: 55 loss:  1.2459439039230347\n",
      "tensor(1.2450, grad_fn=<DivBackward0>)\n",
      "tensor(1.2504, grad_fn=<DivBackward0>)\n",
      "Epoch: 56 loss:  1.2449653148651123\n",
      "tensor(1.2459, grad_fn=<DivBackward0>)\n",
      "tensor(1.2473, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 57\n",
      "Epoch: 57 loss:  1.2458513975143433\n",
      "tensor(1.2439, grad_fn=<DivBackward0>)\n",
      "tensor(1.2496, grad_fn=<DivBackward0>)\n",
      "Epoch: 58 loss:  1.2439239025115967\n",
      "tensor(1.2429, grad_fn=<DivBackward0>)\n",
      "tensor(1.2472, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 59\n",
      "Epoch: 59 loss:  1.2428816556930542\n",
      "tensor(1.2429, grad_fn=<DivBackward0>)\n",
      "tensor(1.2474, grad_fn=<DivBackward0>)\n",
      "Epoch: 60 loss:  1.2429184913635254\n",
      "tensor(1.2419, grad_fn=<DivBackward0>)\n",
      "tensor(1.2482, grad_fn=<DivBackward0>)\n",
      "Epoch: 61 loss:  1.2419277429580688\n",
      "tensor(1.2431, grad_fn=<DivBackward0>)\n",
      "tensor(1.2475, grad_fn=<DivBackward0>)\n",
      "Epoch: 62 loss:  1.2430882453918457\n",
      "tensor(1.2410, grad_fn=<DivBackward0>)\n",
      "tensor(1.2475, grad_fn=<DivBackward0>)\n",
      "Epoch: 63 loss:  1.2409929037094116\n",
      "tensor(1.2401, grad_fn=<DivBackward0>)\n",
      "tensor(1.2440, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 64\n",
      "Epoch: 64 loss:  1.2401381731033325\n",
      "tensor(1.2401, grad_fn=<DivBackward0>)\n",
      "tensor(1.2421, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 65\n",
      "Epoch: 65 loss:  1.240058183670044\n",
      "tensor(1.2387, grad_fn=<DivBackward0>)\n",
      "tensor(1.2439, grad_fn=<DivBackward0>)\n",
      "Epoch: 66 loss:  1.2387170791625977\n",
      "tensor(1.2388, grad_fn=<DivBackward0>)\n",
      "tensor(1.2436, grad_fn=<DivBackward0>)\n",
      "Epoch: 67 loss:  1.2388213872909546\n",
      "tensor(1.2363, grad_fn=<DivBackward0>)\n",
      "tensor(1.2413, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 68\n",
      "Epoch: 68 loss:  1.236340045928955\n",
      "tensor(1.2361, grad_fn=<DivBackward0>)\n",
      "tensor(1.2421, grad_fn=<DivBackward0>)\n",
      "Epoch: 69 loss:  1.2361068725585938\n",
      "tensor(1.2362, grad_fn=<DivBackward0>)\n",
      "tensor(1.2381, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 70\n",
      "Epoch: 70 loss:  1.2362054586410522\n",
      "tensor(1.2348, grad_fn=<DivBackward0>)\n",
      "tensor(1.2415, grad_fn=<DivBackward0>)\n",
      "Epoch: 71 loss:  1.2348289489746094\n",
      "tensor(1.2341, grad_fn=<DivBackward0>)\n",
      "tensor(1.2382, grad_fn=<DivBackward0>)\n",
      "Epoch: 72 loss:  1.2340911626815796\n",
      "tensor(1.2331, grad_fn=<DivBackward0>)\n",
      "tensor(1.2378, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 73\n",
      "Epoch: 73 loss:  1.2331217527389526\n",
      "tensor(1.2328, grad_fn=<DivBackward0>)\n",
      "tensor(1.2382, grad_fn=<DivBackward0>)\n",
      "Epoch: 74 loss:  1.2328232526779175\n",
      "tensor(1.2312, grad_fn=<DivBackward0>)\n",
      "tensor(1.2389, grad_fn=<DivBackward0>)\n",
      "Epoch: 75 loss:  1.2311668395996094\n",
      "tensor(1.2314, grad_fn=<DivBackward0>)\n",
      "tensor(1.2380, grad_fn=<DivBackward0>)\n",
      "Epoch: 76 loss:  1.2313868999481201\n",
      "tensor(1.2302, grad_fn=<DivBackward0>)\n",
      "tensor(1.2385, grad_fn=<DivBackward0>)\n",
      "Epoch: 77 loss:  1.2301772832870483\n",
      "tensor(1.2305, grad_fn=<DivBackward0>)\n",
      "tensor(1.2371, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 78\n",
      "Epoch: 78 loss:  1.230463981628418\n",
      "tensor(1.2294, grad_fn=<DivBackward0>)\n",
      "tensor(1.2334, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 79\n",
      "Epoch: 79 loss:  1.229421854019165\n",
      "tensor(1.2312, grad_fn=<DivBackward0>)\n",
      "tensor(1.2339, grad_fn=<DivBackward0>)\n",
      "Epoch: 80 loss:  1.2311557531356812\n",
      "tensor(1.2309, grad_fn=<DivBackward0>)\n",
      "tensor(1.2391, grad_fn=<DivBackward0>)\n",
      "Epoch: 81 loss:  1.2308993339538574\n",
      "tensor(1.2275, grad_fn=<DivBackward0>)\n",
      "tensor(1.2343, grad_fn=<DivBackward0>)\n",
      "Epoch: 82 loss:  1.2274683713912964\n",
      "tensor(1.2266, grad_fn=<DivBackward0>)\n",
      "tensor(1.2303, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 83\n",
      "Epoch: 83 loss:  1.2265923023223877\n",
      "tensor(1.2257, grad_fn=<DivBackward0>)\n",
      "tensor(1.2342, grad_fn=<DivBackward0>)\n",
      "Epoch: 84 loss:  1.2257121801376343\n",
      "tensor(1.2263, grad_fn=<DivBackward0>)\n",
      "tensor(1.2340, grad_fn=<DivBackward0>)\n",
      "Epoch: 85 loss:  1.2262859344482422\n",
      "tensor(1.2251, grad_fn=<DivBackward0>)\n",
      "tensor(1.2347, grad_fn=<DivBackward0>)\n",
      "Epoch: 86 loss:  1.225113034248352\n",
      "tensor(1.2245, grad_fn=<DivBackward0>)\n",
      "tensor(1.2336, grad_fn=<DivBackward0>)\n",
      "Epoch: 87 loss:  1.2244603633880615\n",
      "tensor(1.2257, grad_fn=<DivBackward0>)\n",
      "tensor(1.2303, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 88\n",
      "Epoch: 88 loss:  1.2257301807403564\n",
      "tensor(1.2236, grad_fn=<DivBackward0>)\n",
      "tensor(1.2335, grad_fn=<DivBackward0>)\n",
      "Epoch: 89 loss:  1.2236250638961792\n",
      "tensor(1.2230, grad_fn=<DivBackward0>)\n",
      "tensor(1.2311, grad_fn=<DivBackward0>)\n",
      "Epoch: 90 loss:  1.2230011224746704\n",
      "tensor(1.2224, grad_fn=<DivBackward0>)\n",
      "tensor(1.2295, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 91\n",
      "Epoch: 91 loss:  1.222408652305603\n",
      "tensor(1.2218, grad_fn=<DivBackward0>)\n",
      "tensor(1.2324, grad_fn=<DivBackward0>)\n",
      "Epoch: 92 loss:  1.2217578887939453\n",
      "tensor(1.2214, grad_fn=<DivBackward0>)\n",
      "tensor(1.2284, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 93\n",
      "Epoch: 93 loss:  1.2214254140853882\n",
      "tensor(1.2211, grad_fn=<DivBackward0>)\n",
      "tensor(1.2262, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 94\n",
      "Epoch: 94 loss:  1.2211244106292725\n",
      "tensor(1.2190, grad_fn=<DivBackward0>)\n",
      "tensor(1.2282, grad_fn=<DivBackward0>)\n",
      "Epoch: 95 loss:  1.2190227508544922\n",
      "tensor(1.2213, grad_fn=<DivBackward0>)\n",
      "tensor(1.2274, grad_fn=<DivBackward0>)\n",
      "Epoch: 96 loss:  1.2212591171264648\n",
      "tensor(1.2199, grad_fn=<DivBackward0>)\n",
      "tensor(1.2235, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 97\n",
      "Epoch: 97 loss:  1.2198781967163086\n",
      "tensor(1.2196, grad_fn=<DivBackward0>)\n",
      "tensor(1.2212, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 98\n",
      "Epoch: 98 loss:  1.2196415662765503\n",
      "tensor(1.2170, grad_fn=<DivBackward0>)\n",
      "tensor(1.2247, grad_fn=<DivBackward0>)\n",
      "Epoch: 99 loss:  1.2170114517211914\n",
      "[0.99683297 0.9998854  0.99993455 ... 0.64247674 0.9813529  0.7686247 ]\n",
      "[0.9998358  0.9972402  0.99740595 ... 0.6994675  0.9902657  0.8147065 ]\n",
      "[0.99929893 0.99792665 0.9980339  ... 0.69175696 0.9884916  0.8102024 ]\n",
      "wl_5_5_10_5_10\n",
      "GCN_edgeweight(\n",
      "  (conv1): GCNConv(1555, 128)\n",
      "  (conv2): GCNConv(128, 16)\n",
      "  (linear): Linear(in_features=16, out_features=10, bias=True)\n",
      ")\n",
      "tensor(1.4080, grad_fn=<DivBackward0>)\n",
      "tensor(1.4080, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 0\n",
      "Epoch: 0 loss:  1.407952904701233\n",
      "tensor(1.3936, grad_fn=<DivBackward0>)\n",
      "tensor(1.3937, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 1\n",
      "Epoch: 1 loss:  1.3935787677764893\n",
      "tensor(1.3758, grad_fn=<DivBackward0>)\n",
      "tensor(1.3764, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 2\n",
      "Epoch: 2 loss:  1.3758220672607422\n",
      "tensor(1.3678, grad_fn=<DivBackward0>)\n",
      "tensor(1.3698, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 3\n",
      "Epoch: 3 loss:  1.3678088188171387\n",
      "tensor(1.3494, grad_fn=<DivBackward0>)\n",
      "tensor(1.3504, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 4\n",
      "Epoch: 4 loss:  1.34940505027771\n",
      "tensor(1.3441, grad_fn=<DivBackward0>)\n",
      "tensor(1.3449, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 5\n",
      "Epoch: 5 loss:  1.344107985496521\n",
      "tensor(1.3322, grad_fn=<DivBackward0>)\n",
      "tensor(1.3337, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 6\n",
      "Epoch: 6 loss:  1.332189679145813\n",
      "tensor(1.3188, grad_fn=<DivBackward0>)\n",
      "tensor(1.3217, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 7\n",
      "Epoch: 7 loss:  1.3187578916549683\n",
      "tensor(1.3201, grad_fn=<DivBackward0>)\n",
      "tensor(1.3262, grad_fn=<DivBackward0>)\n",
      "Epoch: 8 loss:  1.3201158046722412\n",
      "tensor(1.3065, grad_fn=<DivBackward0>)\n",
      "tensor(1.3114, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 9\n",
      "Epoch: 9 loss:  1.3064610958099365\n",
      "tensor(1.3047, grad_fn=<DivBackward0>)\n",
      "tensor(1.3084, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 10\n",
      "Epoch: 10 loss:  1.3047465085983276\n",
      "tensor(1.3032, grad_fn=<DivBackward0>)\n",
      "tensor(1.3073, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 11\n",
      "Epoch: 11 loss:  1.3031728267669678\n",
      "tensor(1.2976, grad_fn=<DivBackward0>)\n",
      "tensor(1.3036, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 12\n",
      "Epoch: 12 loss:  1.2975685596466064\n",
      "tensor(1.2987, grad_fn=<DivBackward0>)\n",
      "tensor(1.3075, grad_fn=<DivBackward0>)\n",
      "Epoch: 13 loss:  1.2987314462661743\n",
      "tensor(1.2915, grad_fn=<DivBackward0>)\n",
      "tensor(1.3001, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 14\n",
      "Epoch: 14 loss:  1.2914648056030273\n",
      "tensor(1.2887, grad_fn=<DivBackward0>)\n",
      "tensor(1.2959, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 15\n",
      "Epoch: 15 loss:  1.2886500358581543\n",
      "tensor(1.2856, grad_fn=<DivBackward0>)\n",
      "tensor(1.2948, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 16\n",
      "Epoch: 16 loss:  1.2856394052505493\n",
      "tensor(1.2807, grad_fn=<DivBackward0>)\n",
      "tensor(1.2916, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 17\n",
      "Epoch: 17 loss:  1.2807332277297974\n",
      "tensor(1.2800, grad_fn=<DivBackward0>)\n",
      "tensor(1.2936, grad_fn=<DivBackward0>)\n",
      "Epoch: 18 loss:  1.2799948453903198\n",
      "tensor(1.2755, grad_fn=<DivBackward0>)\n",
      "tensor(1.2875, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 19\n",
      "Epoch: 19 loss:  1.2754852771759033\n",
      "tensor(1.2744, grad_fn=<DivBackward0>)\n",
      "tensor(1.2846, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 20\n",
      "Epoch: 20 loss:  1.2743746042251587\n",
      "tensor(1.2713, grad_fn=<DivBackward0>)\n",
      "tensor(1.2834, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 21\n",
      "Epoch: 21 loss:  1.2713438272476196\n",
      "tensor(1.2706, grad_fn=<DivBackward0>)\n",
      "tensor(1.2856, grad_fn=<DivBackward0>)\n",
      "Epoch: 22 loss:  1.2705967426300049\n",
      "tensor(1.2688, grad_fn=<DivBackward0>)\n",
      "tensor(1.2827, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 23\n",
      "Epoch: 23 loss:  1.2688275575637817\n",
      "tensor(1.2676, grad_fn=<DivBackward0>)\n",
      "tensor(1.2804, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 24\n",
      "Epoch: 24 loss:  1.2676118612289429\n",
      "tensor(1.2661, grad_fn=<DivBackward0>)\n",
      "tensor(1.2801, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 25\n",
      "Epoch: 25 loss:  1.2661417722702026\n",
      "tensor(1.2658, grad_fn=<DivBackward0>)\n",
      "tensor(1.2812, grad_fn=<DivBackward0>)\n",
      "Epoch: 26 loss:  1.2657579183578491\n",
      "tensor(1.2638, grad_fn=<DivBackward0>)\n",
      "tensor(1.2800, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 27\n",
      "Epoch: 27 loss:  1.263798475265503\n",
      "tensor(1.2634, grad_fn=<DivBackward0>)\n",
      "tensor(1.2777, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 28\n",
      "Epoch: 28 loss:  1.2633625268936157\n",
      "tensor(1.2610, grad_fn=<DivBackward0>)\n",
      "tensor(1.2789, grad_fn=<DivBackward0>)\n",
      "Epoch: 29 loss:  1.2609543800354004\n",
      "tensor(1.2619, grad_fn=<DivBackward0>)\n",
      "tensor(1.2835, grad_fn=<DivBackward0>)\n",
      "Epoch: 30 loss:  1.2619080543518066\n",
      "tensor(1.2603, grad_fn=<DivBackward0>)\n",
      "tensor(1.2792, grad_fn=<DivBackward0>)\n",
      "Epoch: 31 loss:  1.2603482007980347\n",
      "tensor(1.2582, grad_fn=<DivBackward0>)\n",
      "tensor(1.2786, grad_fn=<DivBackward0>)\n",
      "Epoch: 32 loss:  1.2581948041915894\n",
      "tensor(1.2595, grad_fn=<DivBackward0>)\n",
      "tensor(1.2830, grad_fn=<DivBackward0>)\n",
      "Epoch: 33 loss:  1.259501338005066\n",
      "tensor(1.2577, grad_fn=<DivBackward0>)\n",
      "tensor(1.2778, grad_fn=<DivBackward0>)\n",
      "Epoch: 34 loss:  1.2576619386672974\n",
      "tensor(1.2563, grad_fn=<DivBackward0>)\n",
      "tensor(1.2766, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 35\n",
      "Epoch: 35 loss:  1.256253957748413\n",
      "tensor(1.2567, grad_fn=<DivBackward0>)\n",
      "tensor(1.2822, grad_fn=<DivBackward0>)\n",
      "Epoch: 36 loss:  1.2566628456115723\n",
      "tensor(1.2547, grad_fn=<DivBackward0>)\n",
      "tensor(1.2766, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 37\n",
      "Epoch: 37 loss:  1.254724144935608\n",
      "tensor(1.2555, grad_fn=<DivBackward0>)\n",
      "tensor(1.2755, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 38\n",
      "Epoch: 38 loss:  1.2555155754089355\n",
      "tensor(1.2549, grad_fn=<DivBackward0>)\n",
      "tensor(1.2801, grad_fn=<DivBackward0>)\n",
      "Epoch: 39 loss:  1.2548834085464478\n",
      "tensor(1.2533, grad_fn=<DivBackward0>)\n",
      "tensor(1.2789, grad_fn=<DivBackward0>)\n",
      "Epoch: 40 loss:  1.253299593925476\n",
      "tensor(1.2528, grad_fn=<DivBackward0>)\n",
      "tensor(1.2771, grad_fn=<DivBackward0>)\n",
      "Epoch: 41 loss:  1.2528233528137207\n",
      "tensor(1.2515, grad_fn=<DivBackward0>)\n",
      "tensor(1.2783, grad_fn=<DivBackward0>)\n",
      "Epoch: 42 loss:  1.2514599561691284\n",
      "tensor(1.2526, grad_fn=<DivBackward0>)\n",
      "tensor(1.2811, grad_fn=<DivBackward0>)\n",
      "Epoch: 43 loss:  1.2525837421417236\n",
      "tensor(1.2507, grad_fn=<DivBackward0>)\n",
      "tensor(1.2749, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 44\n",
      "Epoch: 44 loss:  1.2506651878356934\n",
      "tensor(1.2495, grad_fn=<DivBackward0>)\n",
      "tensor(1.2728, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 45\n",
      "Epoch: 45 loss:  1.2494730949401855\n",
      "tensor(1.2506, grad_fn=<DivBackward0>)\n",
      "tensor(1.2780, grad_fn=<DivBackward0>)\n",
      "Epoch: 46 loss:  1.250627875328064\n",
      "tensor(1.2494, grad_fn=<DivBackward0>)\n",
      "tensor(1.2750, grad_fn=<DivBackward0>)\n",
      "Epoch: 47 loss:  1.2493659257888794\n",
      "tensor(1.2481, grad_fn=<DivBackward0>)\n",
      "tensor(1.2724, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 48\n",
      "Epoch: 48 loss:  1.248124122619629\n",
      "tensor(1.2481, grad_fn=<DivBackward0>)\n",
      "tensor(1.2770, grad_fn=<DivBackward0>)\n",
      "Epoch: 49 loss:  1.248146414756775\n",
      "tensor(1.2465, grad_fn=<DivBackward0>)\n",
      "tensor(1.2766, grad_fn=<DivBackward0>)\n",
      "Epoch: 50 loss:  1.2465245723724365\n",
      "tensor(1.2467, grad_fn=<DivBackward0>)\n",
      "tensor(1.2752, grad_fn=<DivBackward0>)\n",
      "Epoch: 51 loss:  1.2467237710952759\n",
      "tensor(1.2456, grad_fn=<DivBackward0>)\n",
      "tensor(1.2773, grad_fn=<DivBackward0>)\n",
      "Epoch: 52 loss:  1.2455732822418213\n",
      "tensor(1.2448, grad_fn=<DivBackward0>)\n",
      "tensor(1.2765, grad_fn=<DivBackward0>)\n",
      "Epoch: 53 loss:  1.24478018283844\n",
      "tensor(1.2442, grad_fn=<DivBackward0>)\n",
      "tensor(1.2742, grad_fn=<DivBackward0>)\n",
      "Epoch: 54 loss:  1.2442004680633545\n",
      "tensor(1.2439, grad_fn=<DivBackward0>)\n",
      "tensor(1.2768, grad_fn=<DivBackward0>)\n",
      "Epoch: 55 loss:  1.243852138519287\n",
      "tensor(1.2429, grad_fn=<DivBackward0>)\n",
      "tensor(1.2764, grad_fn=<DivBackward0>)\n",
      "Epoch: 56 loss:  1.242889642715454\n",
      "tensor(1.2424, grad_fn=<DivBackward0>)\n",
      "tensor(1.2740, grad_fn=<DivBackward0>)\n",
      "Epoch: 57 loss:  1.242380976676941\n",
      "tensor(1.2421, grad_fn=<DivBackward0>)\n",
      "tensor(1.2778, grad_fn=<DivBackward0>)\n",
      "Epoch: 58 loss:  1.2420803308486938\n",
      "tensor(1.2400, grad_fn=<DivBackward0>)\n",
      "tensor(1.2767, grad_fn=<DivBackward0>)\n",
      "Epoch: 59 loss:  1.2400000095367432\n",
      "tensor(1.2394, grad_fn=<DivBackward0>)\n",
      "tensor(1.2773, grad_fn=<DivBackward0>)\n",
      "Epoch: 60 loss:  1.2393900156021118\n",
      "tensor(1.2385, grad_fn=<DivBackward0>)\n",
      "tensor(1.2752, grad_fn=<DivBackward0>)\n",
      "Epoch: 61 loss:  1.2385042905807495\n",
      "tensor(1.2388, grad_fn=<DivBackward0>)\n",
      "tensor(1.2762, grad_fn=<DivBackward0>)\n",
      "Epoch: 62 loss:  1.2388265132904053\n",
      "tensor(1.2381, grad_fn=<DivBackward0>)\n",
      "tensor(1.2743, grad_fn=<DivBackward0>)\n",
      "Epoch: 63 loss:  1.2380962371826172\n",
      "tensor(1.2371, grad_fn=<DivBackward0>)\n",
      "tensor(1.2736, grad_fn=<DivBackward0>)\n",
      "Epoch: 64 loss:  1.2371089458465576\n",
      "tensor(1.2373, grad_fn=<DivBackward0>)\n",
      "tensor(1.2746, grad_fn=<DivBackward0>)\n",
      "Epoch: 65 loss:  1.2372525930404663\n",
      "tensor(1.2361, grad_fn=<DivBackward0>)\n",
      "tensor(1.2727, grad_fn=<DivBackward0>)\n",
      "Epoch: 66 loss:  1.236056923866272\n",
      "tensor(1.2360, grad_fn=<DivBackward0>)\n",
      "tensor(1.2781, grad_fn=<DivBackward0>)\n",
      "Epoch: 67 loss:  1.2359791994094849\n",
      "tensor(1.2355, grad_fn=<DivBackward0>)\n",
      "tensor(1.2715, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 68\n",
      "Epoch: 68 loss:  1.235500454902649\n",
      "tensor(1.2341, grad_fn=<DivBackward0>)\n",
      "tensor(1.2725, grad_fn=<DivBackward0>)\n",
      "Epoch: 69 loss:  1.2340666055679321\n",
      "tensor(1.2325, grad_fn=<DivBackward0>)\n",
      "tensor(1.2712, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 70\n",
      "Epoch: 70 loss:  1.2325359582901\n",
      "tensor(1.2323, grad_fn=<DivBackward0>)\n",
      "tensor(1.2690, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 71\n",
      "Epoch: 71 loss:  1.2322748899459839\n",
      "tensor(1.2329, grad_fn=<DivBackward0>)\n",
      "tensor(1.2718, grad_fn=<DivBackward0>)\n",
      "Epoch: 72 loss:  1.232865571975708\n",
      "tensor(1.2319, grad_fn=<DivBackward0>)\n",
      "tensor(1.2687, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 73\n",
      "Epoch: 73 loss:  1.2319016456604004\n",
      "tensor(1.2312, grad_fn=<DivBackward0>)\n",
      "tensor(1.2740, grad_fn=<DivBackward0>)\n",
      "Epoch: 74 loss:  1.231229543685913\n",
      "tensor(1.2284, grad_fn=<DivBackward0>)\n",
      "tensor(1.2727, grad_fn=<DivBackward0>)\n",
      "Epoch: 75 loss:  1.2283786535263062\n",
      "tensor(1.2291, grad_fn=<DivBackward0>)\n",
      "tensor(1.2692, grad_fn=<DivBackward0>)\n",
      "Epoch: 76 loss:  1.229119896888733\n",
      "tensor(1.2296, grad_fn=<DivBackward0>)\n",
      "tensor(1.2773, grad_fn=<DivBackward0>)\n",
      "Epoch: 77 loss:  1.229642629623413\n",
      "tensor(1.2281, grad_fn=<DivBackward0>)\n",
      "tensor(1.2691, grad_fn=<DivBackward0>)\n",
      "Epoch: 78 loss:  1.228103756904602\n",
      "tensor(1.2259, grad_fn=<DivBackward0>)\n",
      "tensor(1.2730, grad_fn=<DivBackward0>)\n",
      "Epoch: 79 loss:  1.2258704900741577\n",
      "tensor(1.2263, grad_fn=<DivBackward0>)\n",
      "tensor(1.2741, grad_fn=<DivBackward0>)\n",
      "Epoch: 80 loss:  1.226256251335144\n",
      "tensor(1.2271, grad_fn=<DivBackward0>)\n",
      "tensor(1.2713, grad_fn=<DivBackward0>)\n",
      "Epoch: 81 loss:  1.2271416187286377\n",
      "tensor(1.2273, grad_fn=<DivBackward0>)\n",
      "tensor(1.2797, grad_fn=<DivBackward0>)\n",
      "Epoch: 82 loss:  1.2273279428482056\n",
      "tensor(1.2259, grad_fn=<DivBackward0>)\n",
      "tensor(1.2700, grad_fn=<DivBackward0>)\n",
      "Epoch: 83 loss:  1.2258615493774414\n",
      "tensor(1.2242, grad_fn=<DivBackward0>)\n",
      "tensor(1.2688, grad_fn=<DivBackward0>)\n",
      "Epoch: 84 loss:  1.224215030670166\n",
      "tensor(1.2237, grad_fn=<DivBackward0>)\n",
      "tensor(1.2736, grad_fn=<DivBackward0>)\n",
      "Epoch: 85 loss:  1.2237080335617065\n",
      "tensor(1.2239, grad_fn=<DivBackward0>)\n",
      "tensor(1.2666, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 86\n",
      "Epoch: 86 loss:  1.223853588104248\n",
      "tensor(1.2226, grad_fn=<DivBackward0>)\n",
      "tensor(1.2724, grad_fn=<DivBackward0>)\n",
      "Epoch: 87 loss:  1.2225898504257202\n",
      "tensor(1.2232, grad_fn=<DivBackward0>)\n",
      "tensor(1.2750, grad_fn=<DivBackward0>)\n",
      "Epoch: 88 loss:  1.2232284545898438\n",
      "tensor(1.2243, grad_fn=<DivBackward0>)\n",
      "tensor(1.2669, grad_fn=<DivBackward0>)\n",
      "Epoch: 89 loss:  1.224329948425293\n",
      "tensor(1.2226, grad_fn=<DivBackward0>)\n",
      "tensor(1.2793, grad_fn=<DivBackward0>)\n",
      "Epoch: 90 loss:  1.2225919961929321\n",
      "tensor(1.2195, grad_fn=<DivBackward0>)\n",
      "tensor(1.2722, grad_fn=<DivBackward0>)\n",
      "Epoch: 91 loss:  1.2195258140563965\n",
      "tensor(1.2191, grad_fn=<DivBackward0>)\n",
      "tensor(1.2698, grad_fn=<DivBackward0>)\n",
      "Epoch: 92 loss:  1.219093680381775\n",
      "tensor(1.2208, grad_fn=<DivBackward0>)\n",
      "tensor(1.2747, grad_fn=<DivBackward0>)\n",
      "Epoch: 93 loss:  1.220760464668274\n",
      "tensor(1.2194, grad_fn=<DivBackward0>)\n",
      "tensor(1.2667, grad_fn=<DivBackward0>)\n",
      "Epoch: 94 loss:  1.21942138671875\n",
      "tensor(1.2184, grad_fn=<DivBackward0>)\n",
      "tensor(1.2696, grad_fn=<DivBackward0>)\n",
      "Epoch: 95 loss:  1.2184361219406128\n",
      "tensor(1.2180, grad_fn=<DivBackward0>)\n",
      "tensor(1.2765, grad_fn=<DivBackward0>)\n",
      "Epoch: 96 loss:  1.217970371246338\n",
      "tensor(1.2183, grad_fn=<DivBackward0>)\n",
      "tensor(1.2643, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 97\n",
      "Epoch: 97 loss:  1.218344807624817\n",
      "tensor(1.2170, grad_fn=<DivBackward0>)\n",
      "tensor(1.2727, grad_fn=<DivBackward0>)\n",
      "Epoch: 98 loss:  1.2169651985168457\n",
      "tensor(1.2168, grad_fn=<DivBackward0>)\n",
      "tensor(1.2691, grad_fn=<DivBackward0>)\n",
      "Epoch: 99 loss:  1.2168306112289429\n",
      "[0.99925905 0.9998086  0.99990255 ... 0.9292917  0.89771044 0.8618228 ]\n",
      "[0.99989116 0.9992434  0.9991115  ... 0.9388658  0.90898335 0.87525094]\n",
      "[0.9996266  0.9994375  0.9992719  ... 0.93752843 0.9072167  0.872983  ]\n",
      "wl_5_10_10_1_1\n",
      "GCN_edgeweight(\n",
      "  (conv1): GCNConv(1555, 128)\n",
      "  (conv2): GCNConv(128, 16)\n",
      "  (linear): Linear(in_features=16, out_features=10, bias=True)\n",
      ")\n",
      "tensor(1.4080, grad_fn=<DivBackward0>)\n",
      "tensor(1.4080, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 0\n",
      "Epoch: 0 loss:  1.408009648323059\n",
      "tensor(1.3933, grad_fn=<DivBackward0>)\n",
      "tensor(1.3938, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 1\n",
      "Epoch: 1 loss:  1.3933372497558594\n",
      "tensor(1.3760, grad_fn=<DivBackward0>)\n",
      "tensor(1.3772, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 2\n",
      "Epoch: 2 loss:  1.3759511709213257\n",
      "tensor(1.3666, grad_fn=<DivBackward0>)\n",
      "tensor(1.3684, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 3\n",
      "Epoch: 3 loss:  1.3665841817855835\n",
      "tensor(1.3494, grad_fn=<DivBackward0>)\n",
      "tensor(1.3513, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 4\n",
      "Epoch: 4 loss:  1.3493589162826538\n",
      "tensor(1.3421, grad_fn=<DivBackward0>)\n",
      "tensor(1.3441, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 5\n",
      "Epoch: 5 loss:  1.3420820236206055\n",
      "tensor(1.3284, grad_fn=<DivBackward0>)\n",
      "tensor(1.3309, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 6\n",
      "Epoch: 6 loss:  1.328363060951233\n",
      "tensor(1.3190, grad_fn=<DivBackward0>)\n",
      "tensor(1.3220, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 7\n",
      "Epoch: 7 loss:  1.318986415863037\n",
      "tensor(1.3147, grad_fn=<DivBackward0>)\n",
      "tensor(1.3180, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 8\n",
      "Epoch: 8 loss:  1.3146579265594482\n",
      "tensor(1.3054, grad_fn=<DivBackward0>)\n",
      "tensor(1.3088, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 9\n",
      "Epoch: 9 loss:  1.3054250478744507\n",
      "tensor(1.3050, grad_fn=<DivBackward0>)\n",
      "tensor(1.3085, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 10\n",
      "Epoch: 10 loss:  1.304978847503662\n",
      "tensor(1.3010, grad_fn=<DivBackward0>)\n",
      "tensor(1.3048, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 11\n",
      "Epoch: 11 loss:  1.3010311126708984\n",
      "tensor(1.2994, grad_fn=<DivBackward0>)\n",
      "tensor(1.3029, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 12\n",
      "Epoch: 12 loss:  1.2994273900985718\n",
      "tensor(1.2955, grad_fn=<DivBackward0>)\n",
      "tensor(1.2992, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 13\n",
      "Epoch: 13 loss:  1.2955424785614014\n",
      "tensor(1.2900, grad_fn=<DivBackward0>)\n",
      "tensor(1.2943, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 14\n",
      "Epoch: 14 loss:  1.2900255918502808\n",
      "tensor(1.2872, grad_fn=<DivBackward0>)\n",
      "tensor(1.2925, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 15\n",
      "Epoch: 15 loss:  1.2872072458267212\n",
      "tensor(1.2833, grad_fn=<DivBackward0>)\n",
      "tensor(1.2888, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 16\n",
      "Epoch: 16 loss:  1.2833203077316284\n",
      "tensor(1.2814, grad_fn=<DivBackward0>)\n",
      "tensor(1.2880, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 17\n",
      "Epoch: 17 loss:  1.281446933746338\n",
      "tensor(1.2775, grad_fn=<DivBackward0>)\n",
      "tensor(1.2847, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 18\n",
      "Epoch: 18 loss:  1.2774862051010132\n",
      "tensor(1.2763, grad_fn=<DivBackward0>)\n",
      "tensor(1.2841, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 19\n",
      "Epoch: 19 loss:  1.2763175964355469\n",
      "tensor(1.2728, grad_fn=<DivBackward0>)\n",
      "tensor(1.2802, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 20\n",
      "Epoch: 20 loss:  1.2728354930877686\n",
      "tensor(1.2726, grad_fn=<DivBackward0>)\n",
      "tensor(1.2804, grad_fn=<DivBackward0>)\n",
      "Epoch: 21 loss:  1.2726130485534668\n",
      "tensor(1.2701, grad_fn=<DivBackward0>)\n",
      "tensor(1.2776, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 22\n",
      "Epoch: 22 loss:  1.2700952291488647\n",
      "tensor(1.2686, grad_fn=<DivBackward0>)\n",
      "tensor(1.2773, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 23\n",
      "Epoch: 23 loss:  1.2685866355895996\n",
      "tensor(1.2674, grad_fn=<DivBackward0>)\n",
      "tensor(1.2774, grad_fn=<DivBackward0>)\n",
      "Epoch: 24 loss:  1.2674484252929688\n",
      "tensor(1.2655, grad_fn=<DivBackward0>)\n",
      "tensor(1.2759, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 25\n",
      "Epoch: 25 loss:  1.2654603719711304\n",
      "tensor(1.2643, grad_fn=<DivBackward0>)\n",
      "tensor(1.2743, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 26\n",
      "Epoch: 26 loss:  1.2643396854400635\n",
      "tensor(1.2633, grad_fn=<DivBackward0>)\n",
      "tensor(1.2734, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 27\n",
      "Epoch: 27 loss:  1.2633311748504639\n",
      "tensor(1.2620, grad_fn=<DivBackward0>)\n",
      "tensor(1.2735, grad_fn=<DivBackward0>)\n",
      "Epoch: 28 loss:  1.2619843482971191\n",
      "tensor(1.2611, grad_fn=<DivBackward0>)\n",
      "tensor(1.2725, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 29\n",
      "Epoch: 29 loss:  1.261113166809082\n",
      "tensor(1.2610, grad_fn=<DivBackward0>)\n",
      "tensor(1.2730, grad_fn=<DivBackward0>)\n",
      "Epoch: 30 loss:  1.260999321937561\n",
      "tensor(1.2592, grad_fn=<DivBackward0>)\n",
      "tensor(1.2727, grad_fn=<DivBackward0>)\n",
      "Epoch: 31 loss:  1.2591971158981323\n",
      "tensor(1.2577, grad_fn=<DivBackward0>)\n",
      "tensor(1.2684, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 32\n",
      "Epoch: 32 loss:  1.2576825618743896\n",
      "tensor(1.2580, grad_fn=<DivBackward0>)\n",
      "tensor(1.2703, grad_fn=<DivBackward0>)\n",
      "Epoch: 33 loss:  1.2580024003982544\n",
      "tensor(1.2577, grad_fn=<DivBackward0>)\n",
      "tensor(1.2722, grad_fn=<DivBackward0>)\n",
      "Epoch: 34 loss:  1.2576984167099\n",
      "tensor(1.2567, grad_fn=<DivBackward0>)\n",
      "tensor(1.2686, grad_fn=<DivBackward0>)\n",
      "Epoch: 35 loss:  1.2567123174667358\n",
      "tensor(1.2554, grad_fn=<DivBackward0>)\n",
      "tensor(1.2685, grad_fn=<DivBackward0>)\n",
      "Epoch: 36 loss:  1.2554237842559814\n",
      "tensor(1.2556, grad_fn=<DivBackward0>)\n",
      "tensor(1.2707, grad_fn=<DivBackward0>)\n",
      "Epoch: 37 loss:  1.2556389570236206\n",
      "tensor(1.2548, grad_fn=<DivBackward0>)\n",
      "tensor(1.2685, grad_fn=<DivBackward0>)\n",
      "Epoch: 38 loss:  1.2547624111175537\n",
      "tensor(1.2544, grad_fn=<DivBackward0>)\n",
      "tensor(1.2688, grad_fn=<DivBackward0>)\n",
      "Epoch: 39 loss:  1.254442811012268\n",
      "tensor(1.2539, grad_fn=<DivBackward0>)\n",
      "tensor(1.2671, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 40\n",
      "Epoch: 40 loss:  1.2538940906524658\n",
      "tensor(1.2528, grad_fn=<DivBackward0>)\n",
      "tensor(1.2677, grad_fn=<DivBackward0>)\n",
      "Epoch: 41 loss:  1.2527647018432617\n",
      "tensor(1.2532, grad_fn=<DivBackward0>)\n",
      "tensor(1.2668, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 42\n",
      "Epoch: 42 loss:  1.2532310485839844\n",
      "tensor(1.2532, grad_fn=<DivBackward0>)\n",
      "tensor(1.2673, grad_fn=<DivBackward0>)\n",
      "Epoch: 43 loss:  1.2531747817993164\n",
      "tensor(1.2508, grad_fn=<DivBackward0>)\n",
      "tensor(1.2661, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 44\n",
      "Epoch: 44 loss:  1.2507736682891846\n",
      "tensor(1.2511, grad_fn=<DivBackward0>)\n",
      "tensor(1.2657, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 45\n",
      "Epoch: 45 loss:  1.251103162765503\n",
      "tensor(1.2512, grad_fn=<DivBackward0>)\n",
      "tensor(1.2649, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 46\n",
      "Epoch: 46 loss:  1.2512162923812866\n",
      "tensor(1.2511, grad_fn=<DivBackward0>)\n",
      "tensor(1.2675, grad_fn=<DivBackward0>)\n",
      "Epoch: 47 loss:  1.2510747909545898\n",
      "tensor(1.2488, grad_fn=<DivBackward0>)\n",
      "tensor(1.2648, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 48\n",
      "Epoch: 48 loss:  1.248826026916504\n",
      "tensor(1.2482, grad_fn=<DivBackward0>)\n",
      "tensor(1.2631, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 49\n",
      "Epoch: 49 loss:  1.2481638193130493\n",
      "tensor(1.2467, grad_fn=<DivBackward0>)\n",
      "tensor(1.2627, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 50\n",
      "Epoch: 50 loss:  1.2466981410980225\n",
      "tensor(1.2475, grad_fn=<DivBackward0>)\n",
      "tensor(1.2652, grad_fn=<DivBackward0>)\n",
      "Epoch: 51 loss:  1.2475048303604126\n",
      "tensor(1.2462, grad_fn=<DivBackward0>)\n",
      "tensor(1.2630, grad_fn=<DivBackward0>)\n",
      "Epoch: 52 loss:  1.2461581230163574\n",
      "tensor(1.2461, grad_fn=<DivBackward0>)\n",
      "tensor(1.2641, grad_fn=<DivBackward0>)\n",
      "Epoch: 53 loss:  1.2460687160491943\n",
      "tensor(1.2441, grad_fn=<DivBackward0>)\n",
      "tensor(1.2630, grad_fn=<DivBackward0>)\n",
      "Epoch: 54 loss:  1.244055986404419\n",
      "tensor(1.2436, grad_fn=<DivBackward0>)\n",
      "tensor(1.2608, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 55\n",
      "Epoch: 55 loss:  1.2435534000396729\n",
      "tensor(1.2430, grad_fn=<DivBackward0>)\n",
      "tensor(1.2623, grad_fn=<DivBackward0>)\n",
      "Epoch: 56 loss:  1.242987871170044\n",
      "tensor(1.2426, grad_fn=<DivBackward0>)\n",
      "tensor(1.2622, grad_fn=<DivBackward0>)\n",
      "Epoch: 57 loss:  1.2425901889801025\n",
      "tensor(1.2421, grad_fn=<DivBackward0>)\n",
      "tensor(1.2617, grad_fn=<DivBackward0>)\n",
      "Epoch: 58 loss:  1.2421233654022217\n",
      "tensor(1.2414, grad_fn=<DivBackward0>)\n",
      "tensor(1.2611, grad_fn=<DivBackward0>)\n",
      "Epoch: 59 loss:  1.2413806915283203\n",
      "tensor(1.2424, grad_fn=<DivBackward0>)\n",
      "tensor(1.2616, grad_fn=<DivBackward0>)\n",
      "Epoch: 60 loss:  1.2423804998397827\n",
      "tensor(1.2415, grad_fn=<DivBackward0>)\n",
      "tensor(1.2638, grad_fn=<DivBackward0>)\n",
      "Epoch: 61 loss:  1.2415395975112915\n",
      "tensor(1.2401, grad_fn=<DivBackward0>)\n",
      "tensor(1.2599, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 62\n",
      "Epoch: 62 loss:  1.2400586605072021\n",
      "tensor(1.2382, grad_fn=<DivBackward0>)\n",
      "tensor(1.2588, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 63\n",
      "Epoch: 63 loss:  1.2381633520126343\n",
      "tensor(1.2387, grad_fn=<DivBackward0>)\n",
      "tensor(1.2586, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 64\n",
      "Epoch: 64 loss:  1.2386554479599\n",
      "tensor(1.2386, grad_fn=<DivBackward0>)\n",
      "tensor(1.2600, grad_fn=<DivBackward0>)\n",
      "Epoch: 65 loss:  1.2385952472686768\n",
      "tensor(1.2363, grad_fn=<DivBackward0>)\n",
      "tensor(1.2579, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 66\n",
      "Epoch: 66 loss:  1.236271619796753\n",
      "tensor(1.2354, grad_fn=<DivBackward0>)\n",
      "tensor(1.2589, grad_fn=<DivBackward0>)\n",
      "Epoch: 67 loss:  1.2353503704071045\n",
      "tensor(1.2349, grad_fn=<DivBackward0>)\n",
      "tensor(1.2581, grad_fn=<DivBackward0>)\n",
      "Epoch: 68 loss:  1.2349321842193604\n",
      "tensor(1.2340, grad_fn=<DivBackward0>)\n",
      "tensor(1.2587, grad_fn=<DivBackward0>)\n",
      "Epoch: 69 loss:  1.2340315580368042\n",
      "tensor(1.2339, grad_fn=<DivBackward0>)\n",
      "tensor(1.2582, grad_fn=<DivBackward0>)\n",
      "Epoch: 70 loss:  1.233894944190979\n",
      "tensor(1.2321, grad_fn=<DivBackward0>)\n",
      "tensor(1.2576, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 71\n",
      "Epoch: 71 loss:  1.2321242094039917\n",
      "tensor(1.2314, grad_fn=<DivBackward0>)\n",
      "tensor(1.2544, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 72\n",
      "Epoch: 72 loss:  1.2313567399978638\n",
      "tensor(1.2328, grad_fn=<DivBackward0>)\n",
      "tensor(1.2595, grad_fn=<DivBackward0>)\n",
      "Epoch: 73 loss:  1.2328211069107056\n",
      "tensor(1.2315, grad_fn=<DivBackward0>)\n",
      "tensor(1.2556, grad_fn=<DivBackward0>)\n",
      "Epoch: 74 loss:  1.231531023979187\n",
      "tensor(1.2294, grad_fn=<DivBackward0>)\n",
      "tensor(1.2543, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 75\n",
      "Epoch: 75 loss:  1.2293531894683838\n",
      "tensor(1.2301, grad_fn=<DivBackward0>)\n",
      "tensor(1.2586, grad_fn=<DivBackward0>)\n",
      "Epoch: 76 loss:  1.2301461696624756\n",
      "tensor(1.2299, grad_fn=<DivBackward0>)\n",
      "tensor(1.2546, grad_fn=<DivBackward0>)\n",
      "Epoch: 77 loss:  1.2298848628997803\n",
      "tensor(1.2295, grad_fn=<DivBackward0>)\n",
      "tensor(1.2559, grad_fn=<DivBackward0>)\n",
      "Epoch: 78 loss:  1.2294960021972656\n",
      "tensor(1.2267, grad_fn=<DivBackward0>)\n",
      "tensor(1.2521, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 79\n",
      "Epoch: 79 loss:  1.2267049551010132\n",
      "tensor(1.2275, grad_fn=<DivBackward0>)\n",
      "tensor(1.2543, grad_fn=<DivBackward0>)\n",
      "Epoch: 80 loss:  1.2274961471557617\n",
      "tensor(1.2270, grad_fn=<DivBackward0>)\n",
      "tensor(1.2541, grad_fn=<DivBackward0>)\n",
      "Epoch: 81 loss:  1.2270362377166748\n",
      "tensor(1.2247, grad_fn=<DivBackward0>)\n",
      "tensor(1.2515, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 82\n",
      "Epoch: 82 loss:  1.2247084379196167\n",
      "tensor(1.2250, grad_fn=<DivBackward0>)\n",
      "tensor(1.2533, grad_fn=<DivBackward0>)\n",
      "Epoch: 83 loss:  1.2250382900238037\n",
      "tensor(1.2234, grad_fn=<DivBackward0>)\n",
      "tensor(1.2506, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 84\n",
      "Epoch: 84 loss:  1.223397135734558\n",
      "tensor(1.2225, grad_fn=<DivBackward0>)\n",
      "tensor(1.2514, grad_fn=<DivBackward0>)\n",
      "Epoch: 85 loss:  1.2225323915481567\n",
      "tensor(1.2244, grad_fn=<DivBackward0>)\n",
      "tensor(1.2541, grad_fn=<DivBackward0>)\n",
      "Epoch: 86 loss:  1.2243832349777222\n",
      "tensor(1.2241, grad_fn=<DivBackward0>)\n",
      "tensor(1.2539, grad_fn=<DivBackward0>)\n",
      "Epoch: 87 loss:  1.224122405052185\n",
      "tensor(1.2238, grad_fn=<DivBackward0>)\n",
      "tensor(1.2512, grad_fn=<DivBackward0>)\n",
      "Epoch: 88 loss:  1.2237681150436401\n",
      "tensor(1.2212, grad_fn=<DivBackward0>)\n",
      "tensor(1.2489, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 89\n",
      "Epoch: 89 loss:  1.221221685409546\n",
      "tensor(1.2219, grad_fn=<DivBackward0>)\n",
      "tensor(1.2516, grad_fn=<DivBackward0>)\n",
      "Epoch: 90 loss:  1.2218841314315796\n",
      "tensor(1.2199, grad_fn=<DivBackward0>)\n",
      "tensor(1.2516, grad_fn=<DivBackward0>)\n",
      "Epoch: 91 loss:  1.2199389934539795\n",
      "tensor(1.2196, grad_fn=<DivBackward0>)\n",
      "tensor(1.2507, grad_fn=<DivBackward0>)\n",
      "Epoch: 92 loss:  1.2196073532104492\n",
      "tensor(1.2200, grad_fn=<DivBackward0>)\n",
      "tensor(1.2508, grad_fn=<DivBackward0>)\n",
      "Epoch: 93 loss:  1.2199615240097046\n",
      "tensor(1.2195, grad_fn=<DivBackward0>)\n",
      "tensor(1.2477, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 94\n",
      "Epoch: 94 loss:  1.2195377349853516\n",
      "tensor(1.2183, grad_fn=<DivBackward0>)\n",
      "tensor(1.2474, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 95\n",
      "Epoch: 95 loss:  1.2182854413986206\n",
      "tensor(1.2187, grad_fn=<DivBackward0>)\n",
      "tensor(1.2478, grad_fn=<DivBackward0>)\n",
      "Epoch: 96 loss:  1.2186717987060547\n",
      "tensor(1.2168, grad_fn=<DivBackward0>)\n",
      "tensor(1.2433, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 97\n",
      "Epoch: 97 loss:  1.21677565574646\n",
      "tensor(1.2170, grad_fn=<DivBackward0>)\n",
      "tensor(1.2462, grad_fn=<DivBackward0>)\n",
      "Epoch: 98 loss:  1.2170426845550537\n",
      "tensor(1.2167, grad_fn=<DivBackward0>)\n",
      "tensor(1.2419, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 99\n",
      "Epoch: 99 loss:  1.2166835069656372\n",
      "[0.99835825 0.99983793 0.9999287  ... 0.73114955 0.99073356 0.8145245 ]\n",
      "[0.9998162  0.9988357  0.99907213 ... 0.75378054 0.9922378  0.83762276]\n",
      "[0.99947876 0.9987816  0.9992774  ... 0.7578715  0.9903143  0.8378189 ]\n",
      "wl_5_1_1_1_1\n",
      "GCN_edgeweight(\n",
      "  (conv1): GCNConv(1555, 128)\n",
      "  (conv2): GCNConv(128, 16)\n",
      "  (linear): Linear(in_features=16, out_features=10, bias=True)\n",
      ")\n",
      "tensor(1.4081, grad_fn=<DivBackward0>)\n",
      "tensor(1.4082, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 0\n",
      "Epoch: 0 loss:  1.4081342220306396\n",
      "tensor(1.3948, grad_fn=<DivBackward0>)\n",
      "tensor(1.3953, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 1\n",
      "Epoch: 1 loss:  1.3947718143463135\n",
      "tensor(1.3786, grad_fn=<DivBackward0>)\n",
      "tensor(1.3799, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 2\n",
      "Epoch: 2 loss:  1.3785678148269653\n",
      "tensor(1.3688, grad_fn=<DivBackward0>)\n",
      "tensor(1.3706, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 3\n",
      "Epoch: 3 loss:  1.3688476085662842\n",
      "tensor(1.3531, grad_fn=<DivBackward0>)\n",
      "tensor(1.3546, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 4\n",
      "Epoch: 4 loss:  1.3531030416488647\n",
      "tensor(1.3458, grad_fn=<DivBackward0>)\n",
      "tensor(1.3471, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 5\n",
      "Epoch: 5 loss:  1.345767855644226\n",
      "tensor(1.3325, grad_fn=<DivBackward0>)\n",
      "tensor(1.3340, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 6\n",
      "Epoch: 6 loss:  1.3324930667877197\n",
      "tensor(1.3241, grad_fn=<DivBackward0>)\n",
      "tensor(1.3262, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 7\n",
      "Epoch: 7 loss:  1.324141263961792\n",
      "tensor(1.3184, grad_fn=<DivBackward0>)\n",
      "tensor(1.3208, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 8\n",
      "Epoch: 8 loss:  1.318385124206543\n",
      "tensor(1.3108, grad_fn=<DivBackward0>)\n",
      "tensor(1.3127, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 9\n",
      "Epoch: 9 loss:  1.3108230829238892\n",
      "tensor(1.3100, grad_fn=<DivBackward0>)\n",
      "tensor(1.3115, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 10\n",
      "Epoch: 10 loss:  1.3100446462631226\n",
      "tensor(1.3062, grad_fn=<DivBackward0>)\n",
      "tensor(1.3084, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 11\n",
      "Epoch: 11 loss:  1.306206464767456\n",
      "tensor(1.3054, grad_fn=<DivBackward0>)\n",
      "tensor(1.3083, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 12\n",
      "Epoch: 12 loss:  1.3054430484771729\n",
      "tensor(1.2999, grad_fn=<DivBackward0>)\n",
      "tensor(1.3036, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 13\n",
      "Epoch: 13 loss:  1.2998979091644287\n",
      "tensor(1.2956, grad_fn=<DivBackward0>)\n",
      "tensor(1.2988, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 14\n",
      "Epoch: 14 loss:  1.295568823814392\n",
      "tensor(1.2920, grad_fn=<DivBackward0>)\n",
      "tensor(1.2959, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 15\n",
      "Epoch: 15 loss:  1.2920193672180176\n",
      "tensor(1.2898, grad_fn=<DivBackward0>)\n",
      "tensor(1.2951, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 16\n",
      "Epoch: 16 loss:  1.2897871732711792\n",
      "tensor(1.2862, grad_fn=<DivBackward0>)\n",
      "tensor(1.2918, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 17\n",
      "Epoch: 17 loss:  1.2861534357070923\n",
      "tensor(1.2842, grad_fn=<DivBackward0>)\n",
      "tensor(1.2888, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 18\n",
      "Epoch: 18 loss:  1.2841933965682983\n",
      "tensor(1.2817, grad_fn=<DivBackward0>)\n",
      "tensor(1.2867, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 19\n",
      "Epoch: 19 loss:  1.2817386388778687\n",
      "tensor(1.2804, grad_fn=<DivBackward0>)\n",
      "tensor(1.2870, grad_fn=<DivBackward0>)\n",
      "Epoch: 20 loss:  1.280389666557312\n",
      "tensor(1.2779, grad_fn=<DivBackward0>)\n",
      "tensor(1.2841, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 21\n",
      "Epoch: 21 loss:  1.27794349193573\n",
      "tensor(1.2771, grad_fn=<DivBackward0>)\n",
      "tensor(1.2829, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 22\n",
      "Epoch: 22 loss:  1.277063012123108\n",
      "tensor(1.2756, grad_fn=<DivBackward0>)\n",
      "tensor(1.2817, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 23\n",
      "Epoch: 23 loss:  1.2755743265151978\n",
      "tensor(1.2739, grad_fn=<DivBackward0>)\n",
      "tensor(1.2815, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 24\n",
      "Epoch: 24 loss:  1.2739022970199585\n",
      "tensor(1.2731, grad_fn=<DivBackward0>)\n",
      "tensor(1.2792, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 25\n",
      "Epoch: 25 loss:  1.2731199264526367\n",
      "tensor(1.2711, grad_fn=<DivBackward0>)\n",
      "tensor(1.2790, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 26\n",
      "Epoch: 26 loss:  1.2710542678833008\n",
      "tensor(1.2704, grad_fn=<DivBackward0>)\n",
      "tensor(1.2773, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 27\n",
      "Epoch: 27 loss:  1.2703696489334106\n",
      "tensor(1.2695, grad_fn=<DivBackward0>)\n",
      "tensor(1.2780, grad_fn=<DivBackward0>)\n",
      "Epoch: 28 loss:  1.269507646560669\n",
      "tensor(1.2680, grad_fn=<DivBackward0>)\n",
      "tensor(1.2754, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 29\n",
      "Epoch: 29 loss:  1.267984390258789\n",
      "tensor(1.2670, grad_fn=<DivBackward0>)\n",
      "tensor(1.2767, grad_fn=<DivBackward0>)\n",
      "Epoch: 30 loss:  1.2669681310653687\n",
      "tensor(1.2668, grad_fn=<DivBackward0>)\n",
      "tensor(1.2732, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 31\n",
      "Epoch: 31 loss:  1.2667618989944458\n",
      "tensor(1.2660, grad_fn=<DivBackward0>)\n",
      "tensor(1.2757, grad_fn=<DivBackward0>)\n",
      "Epoch: 32 loss:  1.2659968137741089\n",
      "tensor(1.2642, grad_fn=<DivBackward0>)\n",
      "tensor(1.2746, grad_fn=<DivBackward0>)\n",
      "Epoch: 33 loss:  1.2641777992248535\n",
      "tensor(1.2643, grad_fn=<DivBackward0>)\n",
      "tensor(1.2745, grad_fn=<DivBackward0>)\n",
      "Epoch: 34 loss:  1.264288306236267\n",
      "tensor(1.2643, grad_fn=<DivBackward0>)\n",
      "tensor(1.2760, grad_fn=<DivBackward0>)\n",
      "Epoch: 35 loss:  1.2643249034881592\n",
      "tensor(1.2632, grad_fn=<DivBackward0>)\n",
      "tensor(1.2748, grad_fn=<DivBackward0>)\n",
      "Epoch: 36 loss:  1.2631686925888062\n",
      "tensor(1.2626, grad_fn=<DivBackward0>)\n",
      "tensor(1.2760, grad_fn=<DivBackward0>)\n",
      "Epoch: 37 loss:  1.2626359462738037\n",
      "tensor(1.2612, grad_fn=<DivBackward0>)\n",
      "tensor(1.2747, grad_fn=<DivBackward0>)\n",
      "Epoch: 38 loss:  1.2612372636795044\n",
      "tensor(1.2615, grad_fn=<DivBackward0>)\n",
      "tensor(1.2751, grad_fn=<DivBackward0>)\n",
      "Epoch: 39 loss:  1.261527180671692\n",
      "tensor(1.2609, grad_fn=<DivBackward0>)\n",
      "tensor(1.2774, grad_fn=<DivBackward0>)\n",
      "Epoch: 40 loss:  1.2609083652496338\n",
      "tensor(1.2588, grad_fn=<DivBackward0>)\n",
      "tensor(1.2749, grad_fn=<DivBackward0>)\n",
      "Epoch: 41 loss:  1.258762240409851\n",
      "tensor(1.2589, grad_fn=<DivBackward0>)\n",
      "tensor(1.2735, grad_fn=<DivBackward0>)\n",
      "Epoch: 42 loss:  1.2588779926300049\n",
      "tensor(1.2584, grad_fn=<DivBackward0>)\n",
      "tensor(1.2760, grad_fn=<DivBackward0>)\n",
      "Epoch: 43 loss:  1.2584428787231445\n",
      "tensor(1.2574, grad_fn=<DivBackward0>)\n",
      "tensor(1.2751, grad_fn=<DivBackward0>)\n",
      "Epoch: 44 loss:  1.257449746131897\n",
      "tensor(1.2572, grad_fn=<DivBackward0>)\n",
      "tensor(1.2735, grad_fn=<DivBackward0>)\n",
      "Epoch: 45 loss:  1.2571501731872559\n",
      "tensor(1.2562, grad_fn=<DivBackward0>)\n",
      "tensor(1.2734, grad_fn=<DivBackward0>)\n",
      "Epoch: 46 loss:  1.2561604976654053\n",
      "tensor(1.2566, grad_fn=<DivBackward0>)\n",
      "tensor(1.2717, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 47\n",
      "Epoch: 47 loss:  1.2566148042678833\n",
      "tensor(1.2554, grad_fn=<DivBackward0>)\n",
      "tensor(1.2723, grad_fn=<DivBackward0>)\n",
      "Epoch: 48 loss:  1.2553937435150146\n",
      "tensor(1.2548, grad_fn=<DivBackward0>)\n",
      "tensor(1.2707, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 49\n",
      "Epoch: 49 loss:  1.2547717094421387\n",
      "tensor(1.2536, grad_fn=<DivBackward0>)\n",
      "tensor(1.2703, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 50\n",
      "Epoch: 50 loss:  1.2536181211471558\n",
      "tensor(1.2533, grad_fn=<DivBackward0>)\n",
      "tensor(1.2702, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 51\n",
      "Epoch: 51 loss:  1.2532602548599243\n",
      "tensor(1.2525, grad_fn=<DivBackward0>)\n",
      "tensor(1.2701, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 52\n",
      "Epoch: 52 loss:  1.2524582147598267\n",
      "tensor(1.2521, grad_fn=<DivBackward0>)\n",
      "tensor(1.2700, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 53\n",
      "Epoch: 53 loss:  1.25211501121521\n",
      "tensor(1.2513, grad_fn=<DivBackward0>)\n",
      "tensor(1.2678, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 54\n",
      "Epoch: 54 loss:  1.2513355016708374\n",
      "tensor(1.2512, grad_fn=<DivBackward0>)\n",
      "tensor(1.2696, grad_fn=<DivBackward0>)\n",
      "Epoch: 55 loss:  1.2512332201004028\n",
      "tensor(1.2498, grad_fn=<DivBackward0>)\n",
      "tensor(1.2681, grad_fn=<DivBackward0>)\n",
      "Epoch: 56 loss:  1.24978768825531\n",
      "tensor(1.2500, grad_fn=<DivBackward0>)\n",
      "tensor(1.2693, grad_fn=<DivBackward0>)\n",
      "Epoch: 57 loss:  1.2500096559524536\n",
      "tensor(1.2483, grad_fn=<DivBackward0>)\n",
      "tensor(1.2687, grad_fn=<DivBackward0>)\n",
      "Epoch: 58 loss:  1.2482719421386719\n",
      "tensor(1.2476, grad_fn=<DivBackward0>)\n",
      "tensor(1.2669, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 59\n",
      "Epoch: 59 loss:  1.2475730180740356\n",
      "tensor(1.2477, grad_fn=<DivBackward0>)\n",
      "tensor(1.2669, grad_fn=<DivBackward0>)\n",
      "Epoch: 60 loss:  1.2476602792739868\n",
      "tensor(1.2473, grad_fn=<DivBackward0>)\n",
      "tensor(1.2655, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 61\n",
      "Epoch: 61 loss:  1.2472585439682007\n",
      "tensor(1.2488, grad_fn=<DivBackward0>)\n",
      "tensor(1.2697, grad_fn=<DivBackward0>)\n",
      "Epoch: 62 loss:  1.2488031387329102\n",
      "tensor(1.2465, grad_fn=<DivBackward0>)\n",
      "tensor(1.2648, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 63\n",
      "Epoch: 63 loss:  1.2465033531188965\n",
      "tensor(1.2457, grad_fn=<DivBackward0>)\n",
      "tensor(1.2653, grad_fn=<DivBackward0>)\n",
      "Epoch: 64 loss:  1.2457352876663208\n",
      "tensor(1.2454, grad_fn=<DivBackward0>)\n",
      "tensor(1.2657, grad_fn=<DivBackward0>)\n",
      "Epoch: 65 loss:  1.2453606128692627\n",
      "tensor(1.2451, grad_fn=<DivBackward0>)\n",
      "tensor(1.2630, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 66\n",
      "Epoch: 66 loss:  1.2451380491256714\n",
      "tensor(1.2436, grad_fn=<DivBackward0>)\n",
      "tensor(1.2659, grad_fn=<DivBackward0>)\n",
      "Epoch: 67 loss:  1.2436141967773438\n",
      "tensor(1.2416, grad_fn=<DivBackward0>)\n",
      "tensor(1.2653, grad_fn=<DivBackward0>)\n",
      "Epoch: 68 loss:  1.2415827512741089\n",
      "tensor(1.2420, grad_fn=<DivBackward0>)\n",
      "tensor(1.2619, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 69\n",
      "Epoch: 69 loss:  1.242018222808838\n",
      "tensor(1.2424, grad_fn=<DivBackward0>)\n",
      "tensor(1.2656, grad_fn=<DivBackward0>)\n",
      "Epoch: 70 loss:  1.2423914670944214\n",
      "tensor(1.2404, grad_fn=<DivBackward0>)\n",
      "tensor(1.2624, grad_fn=<DivBackward0>)\n",
      "Epoch: 71 loss:  1.2404409646987915\n",
      "tensor(1.2394, grad_fn=<DivBackward0>)\n",
      "tensor(1.2618, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 72\n",
      "Epoch: 72 loss:  1.2393865585327148\n",
      "tensor(1.2408, grad_fn=<DivBackward0>)\n",
      "tensor(1.2643, grad_fn=<DivBackward0>)\n",
      "Epoch: 73 loss:  1.2408461570739746\n",
      "tensor(1.2398, grad_fn=<DivBackward0>)\n",
      "tensor(1.2586, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 74\n",
      "Epoch: 74 loss:  1.239829182624817\n",
      "tensor(1.2377, grad_fn=<DivBackward0>)\n",
      "tensor(1.2603, grad_fn=<DivBackward0>)\n",
      "Epoch: 75 loss:  1.237673044204712\n",
      "tensor(1.2383, grad_fn=<DivBackward0>)\n",
      "tensor(1.2618, grad_fn=<DivBackward0>)\n",
      "Epoch: 76 loss:  1.2383447885513306\n",
      "tensor(1.2373, grad_fn=<DivBackward0>)\n",
      "tensor(1.2589, grad_fn=<DivBackward0>)\n",
      "Epoch: 77 loss:  1.237251877784729\n",
      "tensor(1.2382, grad_fn=<DivBackward0>)\n",
      "tensor(1.2604, grad_fn=<DivBackward0>)\n",
      "Epoch: 78 loss:  1.2381500005722046\n",
      "tensor(1.2355, grad_fn=<DivBackward0>)\n",
      "tensor(1.2550, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 79\n",
      "Epoch: 79 loss:  1.235498309135437\n",
      "tensor(1.2356, grad_fn=<DivBackward0>)\n",
      "tensor(1.2577, grad_fn=<DivBackward0>)\n",
      "Epoch: 80 loss:  1.235630750656128\n",
      "tensor(1.2351, grad_fn=<DivBackward0>)\n",
      "tensor(1.2604, grad_fn=<DivBackward0>)\n",
      "Epoch: 81 loss:  1.235116958618164\n",
      "tensor(1.2341, grad_fn=<DivBackward0>)\n",
      "tensor(1.2556, grad_fn=<DivBackward0>)\n",
      "Epoch: 82 loss:  1.2341017723083496\n",
      "tensor(1.2345, grad_fn=<DivBackward0>)\n",
      "tensor(1.2569, grad_fn=<DivBackward0>)\n",
      "Epoch: 83 loss:  1.2344677448272705\n",
      "tensor(1.2329, grad_fn=<DivBackward0>)\n",
      "tensor(1.2565, grad_fn=<DivBackward0>)\n",
      "Epoch: 84 loss:  1.232892394065857\n",
      "tensor(1.2331, grad_fn=<DivBackward0>)\n",
      "tensor(1.2563, grad_fn=<DivBackward0>)\n",
      "Epoch: 85 loss:  1.2331424951553345\n",
      "tensor(1.2338, grad_fn=<DivBackward0>)\n",
      "tensor(1.2546, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 86\n",
      "Epoch: 86 loss:  1.23381769657135\n",
      "tensor(1.2330, grad_fn=<DivBackward0>)\n",
      "tensor(1.2542, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 87\n",
      "Epoch: 87 loss:  1.233036756515503\n",
      "tensor(1.2353, grad_fn=<DivBackward0>)\n",
      "tensor(1.2596, grad_fn=<DivBackward0>)\n",
      "Epoch: 88 loss:  1.2352988719940186\n",
      "tensor(1.2339, grad_fn=<DivBackward0>)\n",
      "tensor(1.2534, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 89\n",
      "Epoch: 89 loss:  1.2339153289794922\n",
      "tensor(1.2328, grad_fn=<DivBackward0>)\n",
      "tensor(1.2579, grad_fn=<DivBackward0>)\n",
      "Epoch: 90 loss:  1.2327598333358765\n",
      "tensor(1.2306, grad_fn=<DivBackward0>)\n",
      "tensor(1.2527, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 91\n",
      "Epoch: 91 loss:  1.2305868864059448\n",
      "tensor(1.2305, grad_fn=<DivBackward0>)\n",
      "tensor(1.2541, grad_fn=<DivBackward0>)\n",
      "Epoch: 92 loss:  1.2304648160934448\n",
      "tensor(1.2317, grad_fn=<DivBackward0>)\n",
      "tensor(1.2540, grad_fn=<DivBackward0>)\n",
      "Epoch: 93 loss:  1.2317109107971191\n",
      "tensor(1.2298, grad_fn=<DivBackward0>)\n",
      "tensor(1.2528, grad_fn=<DivBackward0>)\n",
      "Epoch: 94 loss:  1.229763388633728\n",
      "tensor(1.2280, grad_fn=<DivBackward0>)\n",
      "tensor(1.2519, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 95\n",
      "Epoch: 95 loss:  1.2279691696166992\n",
      "tensor(1.2284, grad_fn=<DivBackward0>)\n",
      "tensor(1.2521, grad_fn=<DivBackward0>)\n",
      "Epoch: 96 loss:  1.2284281253814697\n",
      "tensor(1.2284, grad_fn=<DivBackward0>)\n",
      "tensor(1.2510, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 97\n",
      "Epoch: 97 loss:  1.2283625602722168\n",
      "tensor(1.2270, grad_fn=<DivBackward0>)\n",
      "tensor(1.2507, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 98\n",
      "Epoch: 98 loss:  1.2270464897155762\n",
      "tensor(1.2279, grad_fn=<DivBackward0>)\n",
      "tensor(1.2523, grad_fn=<DivBackward0>)\n",
      "Epoch: 99 loss:  1.2279428243637085\n",
      "[0.9966732  0.99856913 0.9994771  ... 0.711866   0.9650805  0.7624567 ]\n",
      "[0.99991155 0.9994592  0.9989798  ... 0.75576293 0.9808141  0.80229914]\n",
      "[0.9997207 0.9993016 0.9986792 ... 0.7603264 0.9824308 0.8096423]\n",
      "wl_10_2_2_2_2\n",
      "GCN_edgeweight(\n",
      "  (conv1): GCNConv(1555, 128)\n",
      "  (conv2): GCNConv(128, 16)\n",
      "  (linear): Linear(in_features=16, out_features=10, bias=True)\n",
      ")\n",
      "tensor(1.4083, grad_fn=<DivBackward0>)\n",
      "tensor(1.4084, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 0\n",
      "Epoch: 0 loss:  1.4083359241485596\n",
      "tensor(1.3974, grad_fn=<DivBackward0>)\n",
      "tensor(1.3972, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 1\n",
      "Epoch: 1 loss:  1.3973513841629028\n",
      "tensor(1.3837, grad_fn=<DivBackward0>)\n",
      "tensor(1.3832, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 2\n",
      "Epoch: 2 loss:  1.3837023973464966\n",
      "tensor(1.3721, grad_fn=<DivBackward0>)\n",
      "tensor(1.3716, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 3\n",
      "Epoch: 3 loss:  1.3720952272415161\n",
      "tensor(1.3586, grad_fn=<DivBackward0>)\n",
      "tensor(1.3579, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 4\n",
      "Epoch: 4 loss:  1.3586137294769287\n",
      "tensor(1.3476, grad_fn=<DivBackward0>)\n",
      "tensor(1.3471, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 5\n",
      "Epoch: 5 loss:  1.3476111888885498\n",
      "tensor(1.3350, grad_fn=<DivBackward0>)\n",
      "tensor(1.3350, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 6\n",
      "Epoch: 6 loss:  1.3350216150283813\n",
      "tensor(1.3290, grad_fn=<DivBackward0>)\n",
      "tensor(1.3298, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 7\n",
      "Epoch: 7 loss:  1.3289570808410645\n",
      "tensor(1.3201, grad_fn=<DivBackward0>)\n",
      "tensor(1.3206, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 8\n",
      "Epoch: 8 loss:  1.3201113939285278\n",
      "tensor(1.3179, grad_fn=<DivBackward0>)\n",
      "tensor(1.3182, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 9\n",
      "Epoch: 9 loss:  1.3179290294647217\n",
      "tensor(1.3163, grad_fn=<DivBackward0>)\n",
      "tensor(1.3171, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 10\n",
      "Epoch: 10 loss:  1.3163197040557861\n",
      "tensor(1.3123, grad_fn=<DivBackward0>)\n",
      "tensor(1.3140, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 11\n",
      "Epoch: 11 loss:  1.3123046159744263\n",
      "tensor(1.3074, grad_fn=<DivBackward0>)\n",
      "tensor(1.3099, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 12\n",
      "Epoch: 12 loss:  1.3073986768722534\n",
      "tensor(1.3034, grad_fn=<DivBackward0>)\n",
      "tensor(1.3071, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 13\n",
      "Epoch: 13 loss:  1.3033994436264038\n",
      "tensor(1.2995, grad_fn=<DivBackward0>)\n",
      "tensor(1.3046, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 14\n",
      "Epoch: 14 loss:  1.2994829416275024\n",
      "tensor(1.2972, grad_fn=<DivBackward0>)\n",
      "tensor(1.3014, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 15\n",
      "Epoch: 15 loss:  1.2972009181976318\n",
      "tensor(1.2947, grad_fn=<DivBackward0>)\n",
      "tensor(1.3003, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 16\n",
      "Epoch: 16 loss:  1.2947359085083008\n",
      "tensor(1.2917, grad_fn=<DivBackward0>)\n",
      "tensor(1.2989, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 17\n",
      "Epoch: 17 loss:  1.2917293310165405\n",
      "tensor(1.2897, grad_fn=<DivBackward0>)\n",
      "tensor(1.2980, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 18\n",
      "Epoch: 18 loss:  1.289654016494751\n",
      "tensor(1.2880, grad_fn=<DivBackward0>)\n",
      "tensor(1.2955, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 19\n",
      "Epoch: 19 loss:  1.2880326509475708\n",
      "tensor(1.2860, grad_fn=<DivBackward0>)\n",
      "tensor(1.2940, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 20\n",
      "Epoch: 20 loss:  1.2859772443771362\n",
      "tensor(1.2839, grad_fn=<DivBackward0>)\n",
      "tensor(1.2922, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 21\n",
      "Epoch: 21 loss:  1.2838598489761353\n",
      "tensor(1.2817, grad_fn=<DivBackward0>)\n",
      "tensor(1.2895, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 22\n",
      "Epoch: 22 loss:  1.281704068183899\n",
      "tensor(1.2813, grad_fn=<DivBackward0>)\n",
      "tensor(1.2908, grad_fn=<DivBackward0>)\n",
      "Epoch: 23 loss:  1.2812578678131104\n",
      "tensor(1.2808, grad_fn=<DivBackward0>)\n",
      "tensor(1.2896, grad_fn=<DivBackward0>)\n",
      "Epoch: 24 loss:  1.2808289527893066\n",
      "tensor(1.2803, grad_fn=<DivBackward0>)\n",
      "tensor(1.2896, grad_fn=<DivBackward0>)\n",
      "Epoch: 25 loss:  1.2803341150283813\n",
      "tensor(1.2778, grad_fn=<DivBackward0>)\n",
      "tensor(1.2857, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 26\n",
      "Epoch: 26 loss:  1.2777550220489502\n",
      "tensor(1.2759, grad_fn=<DivBackward0>)\n",
      "tensor(1.2852, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 27\n",
      "Epoch: 27 loss:  1.2759466171264648\n",
      "tensor(1.2767, grad_fn=<DivBackward0>)\n",
      "tensor(1.2860, grad_fn=<DivBackward0>)\n",
      "Epoch: 28 loss:  1.276721715927124\n",
      "tensor(1.2750, grad_fn=<DivBackward0>)\n",
      "tensor(1.2848, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 29\n",
      "Epoch: 29 loss:  1.2749613523483276\n",
      "tensor(1.2733, grad_fn=<DivBackward0>)\n",
      "tensor(1.2836, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 30\n",
      "Epoch: 30 loss:  1.2732858657836914\n",
      "tensor(1.2741, grad_fn=<DivBackward0>)\n",
      "tensor(1.2845, grad_fn=<DivBackward0>)\n",
      "Epoch: 31 loss:  1.274078369140625\n",
      "tensor(1.2707, grad_fn=<DivBackward0>)\n",
      "tensor(1.2808, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 32\n",
      "Epoch: 32 loss:  1.2707240581512451\n",
      "tensor(1.2710, grad_fn=<DivBackward0>)\n",
      "tensor(1.2829, grad_fn=<DivBackward0>)\n",
      "Epoch: 33 loss:  1.271033525466919\n",
      "tensor(1.2701, grad_fn=<DivBackward0>)\n",
      "tensor(1.2824, grad_fn=<DivBackward0>)\n",
      "Epoch: 34 loss:  1.2700778245925903\n",
      "tensor(1.2687, grad_fn=<DivBackward0>)\n",
      "tensor(1.2813, grad_fn=<DivBackward0>)\n",
      "Epoch: 35 loss:  1.2686612606048584\n",
      "tensor(1.2684, grad_fn=<DivBackward0>)\n",
      "tensor(1.2820, grad_fn=<DivBackward0>)\n",
      "Epoch: 36 loss:  1.2684046030044556\n",
      "tensor(1.2674, grad_fn=<DivBackward0>)\n",
      "tensor(1.2813, grad_fn=<DivBackward0>)\n",
      "Epoch: 37 loss:  1.2674405574798584\n",
      "tensor(1.2677, grad_fn=<DivBackward0>)\n",
      "tensor(1.2831, grad_fn=<DivBackward0>)\n",
      "Epoch: 38 loss:  1.267655849456787\n",
      "tensor(1.2672, grad_fn=<DivBackward0>)\n",
      "tensor(1.2799, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 39\n",
      "Epoch: 39 loss:  1.2671940326690674\n",
      "tensor(1.2648, grad_fn=<DivBackward0>)\n",
      "tensor(1.2788, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 40\n",
      "Epoch: 40 loss:  1.264792561531067\n",
      "tensor(1.2646, grad_fn=<DivBackward0>)\n",
      "tensor(1.2798, grad_fn=<DivBackward0>)\n",
      "Epoch: 41 loss:  1.2645827531814575\n",
      "tensor(1.2634, grad_fn=<DivBackward0>)\n",
      "tensor(1.2786, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 42\n",
      "Epoch: 42 loss:  1.2634162902832031\n",
      "tensor(1.2634, grad_fn=<DivBackward0>)\n",
      "tensor(1.2777, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 43\n",
      "Epoch: 43 loss:  1.2634316682815552\n",
      "tensor(1.2638, grad_fn=<DivBackward0>)\n",
      "tensor(1.2788, grad_fn=<DivBackward0>)\n",
      "Epoch: 44 loss:  1.2637717723846436\n",
      "tensor(1.2607, grad_fn=<DivBackward0>)\n",
      "tensor(1.2750, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 45\n",
      "Epoch: 45 loss:  1.2607256174087524\n",
      "tensor(1.2606, grad_fn=<DivBackward0>)\n",
      "tensor(1.2759, grad_fn=<DivBackward0>)\n",
      "Epoch: 46 loss:  1.2605983018875122\n",
      "tensor(1.2613, grad_fn=<DivBackward0>)\n",
      "tensor(1.2775, grad_fn=<DivBackward0>)\n",
      "Epoch: 47 loss:  1.261344075202942\n",
      "tensor(1.2590, grad_fn=<DivBackward0>)\n",
      "tensor(1.2752, grad_fn=<DivBackward0>)\n",
      "Epoch: 48 loss:  1.2590110301971436\n",
      "tensor(1.2571, grad_fn=<DivBackward0>)\n",
      "tensor(1.2730, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 49\n",
      "Epoch: 49 loss:  1.2570549249649048\n",
      "tensor(1.2590, grad_fn=<DivBackward0>)\n",
      "tensor(1.2767, grad_fn=<DivBackward0>)\n",
      "Epoch: 50 loss:  1.2589695453643799\n",
      "tensor(1.2583, grad_fn=<DivBackward0>)\n",
      "tensor(1.2760, grad_fn=<DivBackward0>)\n",
      "Epoch: 51 loss:  1.2582792043685913\n",
      "tensor(1.2545, grad_fn=<DivBackward0>)\n",
      "tensor(1.2726, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 52\n",
      "Epoch: 52 loss:  1.2545393705368042\n",
      "tensor(1.2589, grad_fn=<DivBackward0>)\n",
      "tensor(1.2786, grad_fn=<DivBackward0>)\n",
      "Epoch: 53 loss:  1.258923888206482\n",
      "tensor(1.2553, grad_fn=<DivBackward0>)\n",
      "tensor(1.2714, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 54\n",
      "Epoch: 54 loss:  1.25528883934021\n",
      "tensor(1.2531, grad_fn=<DivBackward0>)\n",
      "tensor(1.2726, grad_fn=<DivBackward0>)\n",
      "Epoch: 55 loss:  1.2531310319900513\n",
      "tensor(1.2560, grad_fn=<DivBackward0>)\n",
      "tensor(1.2753, grad_fn=<DivBackward0>)\n",
      "Epoch: 56 loss:  1.25603187084198\n",
      "tensor(1.2527, grad_fn=<DivBackward0>)\n",
      "tensor(1.2689, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 57\n",
      "Epoch: 57 loss:  1.2527278661727905\n",
      "tensor(1.2507, grad_fn=<DivBackward0>)\n",
      "tensor(1.2701, grad_fn=<DivBackward0>)\n",
      "Epoch: 58 loss:  1.2506945133209229\n",
      "tensor(1.2542, grad_fn=<DivBackward0>)\n",
      "tensor(1.2723, grad_fn=<DivBackward0>)\n",
      "Epoch: 59 loss:  1.254236102104187\n",
      "tensor(1.2491, grad_fn=<DivBackward0>)\n",
      "tensor(1.2686, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 60\n",
      "Epoch: 60 loss:  1.2490761280059814\n",
      "tensor(1.2489, grad_fn=<DivBackward0>)\n",
      "tensor(1.2668, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 61\n",
      "Epoch: 61 loss:  1.2489392757415771\n",
      "tensor(1.2493, grad_fn=<DivBackward0>)\n",
      "tensor(1.2707, grad_fn=<DivBackward0>)\n",
      "Epoch: 62 loss:  1.2492895126342773\n",
      "tensor(1.2474, grad_fn=<DivBackward0>)\n",
      "tensor(1.2659, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 63\n",
      "Epoch: 63 loss:  1.2473796606063843\n",
      "tensor(1.2475, grad_fn=<DivBackward0>)\n",
      "tensor(1.2685, grad_fn=<DivBackward0>)\n",
      "Epoch: 64 loss:  1.2475104331970215\n",
      "tensor(1.2463, grad_fn=<DivBackward0>)\n",
      "tensor(1.2721, grad_fn=<DivBackward0>)\n",
      "Epoch: 65 loss:  1.2463371753692627\n",
      "tensor(1.2455, grad_fn=<DivBackward0>)\n",
      "tensor(1.2680, grad_fn=<DivBackward0>)\n",
      "Epoch: 66 loss:  1.2455118894577026\n",
      "tensor(1.2450, grad_fn=<DivBackward0>)\n",
      "tensor(1.2661, grad_fn=<DivBackward0>)\n",
      "Epoch: 67 loss:  1.2450376749038696\n",
      "tensor(1.2427, grad_fn=<DivBackward0>)\n",
      "tensor(1.2641, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 68\n",
      "Epoch: 68 loss:  1.2427117824554443\n",
      "tensor(1.2420, grad_fn=<DivBackward0>)\n",
      "tensor(1.2651, grad_fn=<DivBackward0>)\n",
      "Epoch: 69 loss:  1.2420238256454468\n",
      "tensor(1.2412, grad_fn=<DivBackward0>)\n",
      "tensor(1.2640, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 70\n",
      "Epoch: 70 loss:  1.2411837577819824\n",
      "tensor(1.2399, grad_fn=<DivBackward0>)\n",
      "tensor(1.2657, grad_fn=<DivBackward0>)\n",
      "Epoch: 71 loss:  1.2399399280548096\n",
      "tensor(1.2415, grad_fn=<DivBackward0>)\n",
      "tensor(1.2654, grad_fn=<DivBackward0>)\n",
      "Epoch: 72 loss:  1.2414946556091309\n",
      "tensor(1.2399, grad_fn=<DivBackward0>)\n",
      "tensor(1.2632, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 73\n",
      "Epoch: 73 loss:  1.239905834197998\n",
      "tensor(1.2379, grad_fn=<DivBackward0>)\n",
      "tensor(1.2617, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 74\n",
      "Epoch: 74 loss:  1.237929344177246\n",
      "tensor(1.2383, grad_fn=<DivBackward0>)\n",
      "tensor(1.2652, grad_fn=<DivBackward0>)\n",
      "Epoch: 75 loss:  1.2383108139038086\n",
      "tensor(1.2381, grad_fn=<DivBackward0>)\n",
      "tensor(1.2631, grad_fn=<DivBackward0>)\n",
      "Epoch: 76 loss:  1.2381203174591064\n",
      "tensor(1.2361, grad_fn=<DivBackward0>)\n",
      "tensor(1.2637, grad_fn=<DivBackward0>)\n",
      "Epoch: 77 loss:  1.2360562086105347\n",
      "tensor(1.2359, grad_fn=<DivBackward0>)\n",
      "tensor(1.2619, grad_fn=<DivBackward0>)\n",
      "Epoch: 78 loss:  1.235917091369629\n",
      "tensor(1.2350, grad_fn=<DivBackward0>)\n",
      "tensor(1.2599, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 79\n",
      "Epoch: 79 loss:  1.2350372076034546\n",
      "tensor(1.2365, grad_fn=<DivBackward0>)\n",
      "tensor(1.2669, grad_fn=<DivBackward0>)\n",
      "Epoch: 80 loss:  1.2364944219589233\n",
      "tensor(1.2347, grad_fn=<DivBackward0>)\n",
      "tensor(1.2602, grad_fn=<DivBackward0>)\n",
      "Epoch: 81 loss:  1.2347431182861328\n",
      "tensor(1.2338, grad_fn=<DivBackward0>)\n",
      "tensor(1.2574, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 82\n",
      "Epoch: 82 loss:  1.233790397644043\n",
      "tensor(1.2323, grad_fn=<DivBackward0>)\n",
      "tensor(1.2602, grad_fn=<DivBackward0>)\n",
      "Epoch: 83 loss:  1.2322936058044434\n",
      "tensor(1.2322, grad_fn=<DivBackward0>)\n",
      "tensor(1.2605, grad_fn=<DivBackward0>)\n",
      "Epoch: 84 loss:  1.2322025299072266\n",
      "tensor(1.2332, grad_fn=<DivBackward0>)\n",
      "tensor(1.2597, grad_fn=<DivBackward0>)\n",
      "Epoch: 85 loss:  1.233168125152588\n",
      "tensor(1.2302, grad_fn=<DivBackward0>)\n",
      "tensor(1.2557, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 86\n",
      "Epoch: 86 loss:  1.2301677465438843\n",
      "tensor(1.2296, grad_fn=<DivBackward0>)\n",
      "tensor(1.2561, grad_fn=<DivBackward0>)\n",
      "Epoch: 87 loss:  1.2295808792114258\n",
      "tensor(1.2335, grad_fn=<DivBackward0>)\n",
      "tensor(1.2672, grad_fn=<DivBackward0>)\n",
      "Epoch: 88 loss:  1.2335063219070435\n",
      "tensor(1.2354, grad_fn=<DivBackward0>)\n",
      "tensor(1.2567, grad_fn=<DivBackward0>)\n",
      "Epoch: 89 loss:  1.2353545427322388\n",
      "tensor(1.2304, grad_fn=<DivBackward0>)\n",
      "tensor(1.2612, grad_fn=<DivBackward0>)\n",
      "Epoch: 90 loss:  1.2304052114486694\n",
      "tensor(1.2308, grad_fn=<DivBackward0>)\n",
      "tensor(1.2631, grad_fn=<DivBackward0>)\n",
      "Epoch: 91 loss:  1.2307724952697754\n",
      "tensor(1.2316, grad_fn=<DivBackward0>)\n",
      "tensor(1.2540, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 92\n",
      "Epoch: 92 loss:  1.2316105365753174\n",
      "tensor(1.2277, grad_fn=<DivBackward0>)\n",
      "tensor(1.2572, grad_fn=<DivBackward0>)\n",
      "Epoch: 93 loss:  1.2276779413223267\n",
      "tensor(1.2355, grad_fn=<DivBackward0>)\n",
      "tensor(1.2684, grad_fn=<DivBackward0>)\n",
      "Epoch: 94 loss:  1.2355434894561768\n",
      "tensor(1.2351, grad_fn=<DivBackward0>)\n",
      "tensor(1.2562, grad_fn=<DivBackward0>)\n",
      "Epoch: 95 loss:  1.235084056854248\n",
      "tensor(1.2295, grad_fn=<DivBackward0>)\n",
      "tensor(1.2519, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 96\n",
      "Epoch: 96 loss:  1.2294567823410034\n",
      "tensor(1.2447, grad_fn=<DivBackward0>)\n",
      "tensor(1.2796, grad_fn=<DivBackward0>)\n",
      "Epoch: 97 loss:  1.2447279691696167\n",
      "tensor(1.2292, grad_fn=<DivBackward0>)\n",
      "tensor(1.2516, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 98\n",
      "Epoch: 98 loss:  1.2291895151138306\n",
      "tensor(1.2331, grad_fn=<DivBackward0>)\n",
      "tensor(1.2536, grad_fn=<DivBackward0>)\n",
      "Epoch: 99 loss:  1.2330905199050903\n",
      "[0.9976628 0.9991154 0.9998613 ... 0.9581187 0.8140912 0.8178087]\n",
      "[0.99987715 0.9991503  0.9958815  ... 0.9714274  0.8477437  0.8431574 ]\n",
      "[0.9990647  0.997935   0.9944784  ... 0.9783295  0.86429656 0.8602062 ]\n",
      "wl_10_5_10_1_1\n",
      "GCN_edgeweight(\n",
      "  (conv1): GCNConv(1555, 128)\n",
      "  (conv2): GCNConv(128, 16)\n",
      "  (linear): Linear(in_features=16, out_features=10, bias=True)\n",
      ")\n",
      "tensor(1.4082, grad_fn=<DivBackward0>)\n",
      "tensor(1.4083, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 0\n",
      "Epoch: 0 loss:  1.4081816673278809\n",
      "tensor(1.3958, grad_fn=<DivBackward0>)\n",
      "tensor(1.3962, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 1\n",
      "Epoch: 1 loss:  1.395841360092163\n",
      "tensor(1.3805, grad_fn=<DivBackward0>)\n",
      "tensor(1.3810, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 2\n",
      "Epoch: 2 loss:  1.3805489540100098\n",
      "tensor(1.3703, grad_fn=<DivBackward0>)\n",
      "tensor(1.3712, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 3\n",
      "Epoch: 3 loss:  1.3703052997589111\n",
      "tensor(1.3547, grad_fn=<DivBackward0>)\n",
      "tensor(1.3555, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 4\n",
      "Epoch: 4 loss:  1.354736566543579\n",
      "tensor(1.3464, grad_fn=<DivBackward0>)\n",
      "tensor(1.3471, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 5\n",
      "Epoch: 5 loss:  1.3464314937591553\n",
      "tensor(1.3326, grad_fn=<DivBackward0>)\n",
      "tensor(1.3333, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 6\n",
      "Epoch: 6 loss:  1.3325573205947876\n",
      "tensor(1.3252, grad_fn=<DivBackward0>)\n",
      "tensor(1.3270, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 7\n",
      "Epoch: 7 loss:  1.325163722038269\n",
      "tensor(1.3163, grad_fn=<DivBackward0>)\n",
      "tensor(1.3185, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 8\n",
      "Epoch: 8 loss:  1.3163102865219116\n",
      "tensor(1.3109, grad_fn=<DivBackward0>)\n",
      "tensor(1.3123, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 9\n",
      "Epoch: 9 loss:  1.3108667135238647\n",
      "tensor(1.3081, grad_fn=<DivBackward0>)\n",
      "tensor(1.3091, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 10\n",
      "Epoch: 10 loss:  1.3080629110336304\n",
      "tensor(1.3056, grad_fn=<DivBackward0>)\n",
      "tensor(1.3076, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 11\n",
      "Epoch: 11 loss:  1.3055946826934814\n",
      "tensor(1.3032, grad_fn=<DivBackward0>)\n",
      "tensor(1.3061, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 12\n",
      "Epoch: 12 loss:  1.3031885623931885\n",
      "tensor(1.2983, grad_fn=<DivBackward0>)\n",
      "tensor(1.3013, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 13\n",
      "Epoch: 13 loss:  1.2983193397521973\n",
      "tensor(1.2938, grad_fn=<DivBackward0>)\n",
      "tensor(1.2977, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 14\n",
      "Epoch: 14 loss:  1.2938176393508911\n",
      "tensor(1.2907, grad_fn=<DivBackward0>)\n",
      "tensor(1.2960, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 15\n",
      "Epoch: 15 loss:  1.2906506061553955\n",
      "tensor(1.2874, grad_fn=<DivBackward0>)\n",
      "tensor(1.2943, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 16\n",
      "Epoch: 16 loss:  1.2874270677566528\n",
      "tensor(1.2840, grad_fn=<DivBackward0>)\n",
      "tensor(1.2912, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 17\n",
      "Epoch: 17 loss:  1.2840285301208496\n",
      "tensor(1.2816, grad_fn=<DivBackward0>)\n",
      "tensor(1.2903, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 18\n",
      "Epoch: 18 loss:  1.2815660238265991\n",
      "tensor(1.2798, grad_fn=<DivBackward0>)\n",
      "tensor(1.2889, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 19\n",
      "Epoch: 19 loss:  1.2797614336013794\n",
      "tensor(1.2767, grad_fn=<DivBackward0>)\n",
      "tensor(1.2868, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 20\n",
      "Epoch: 20 loss:  1.276652216911316\n",
      "tensor(1.2755, grad_fn=<DivBackward0>)\n",
      "tensor(1.2847, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 21\n",
      "Epoch: 21 loss:  1.2755126953125\n",
      "tensor(1.2733, grad_fn=<DivBackward0>)\n",
      "tensor(1.2847, grad_fn=<DivBackward0>)\n",
      "Epoch: 22 loss:  1.2732744216918945\n",
      "tensor(1.2725, grad_fn=<DivBackward0>)\n",
      "tensor(1.2847, grad_fn=<DivBackward0>)\n",
      "Epoch: 23 loss:  1.2725186347961426\n",
      "tensor(1.2709, grad_fn=<DivBackward0>)\n",
      "tensor(1.2844, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 24\n",
      "Epoch: 24 loss:  1.2708730697631836\n",
      "tensor(1.2694, grad_fn=<DivBackward0>)\n",
      "tensor(1.2848, grad_fn=<DivBackward0>)\n",
      "Epoch: 25 loss:  1.2693969011306763\n",
      "tensor(1.2670, grad_fn=<DivBackward0>)\n",
      "tensor(1.2823, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 26\n",
      "Epoch: 26 loss:  1.267029047012329\n",
      "tensor(1.2666, grad_fn=<DivBackward0>)\n",
      "tensor(1.2830, grad_fn=<DivBackward0>)\n",
      "Epoch: 27 loss:  1.266568899154663\n",
      "tensor(1.2650, grad_fn=<DivBackward0>)\n",
      "tensor(1.2813, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 28\n",
      "Epoch: 28 loss:  1.2649989128112793\n",
      "tensor(1.2645, grad_fn=<DivBackward0>)\n",
      "tensor(1.2817, grad_fn=<DivBackward0>)\n",
      "Epoch: 29 loss:  1.2645028829574585\n",
      "tensor(1.2634, grad_fn=<DivBackward0>)\n",
      "tensor(1.2817, grad_fn=<DivBackward0>)\n",
      "Epoch: 30 loss:  1.2634233236312866\n",
      "tensor(1.2616, grad_fn=<DivBackward0>)\n",
      "tensor(1.2797, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 31\n",
      "Epoch: 31 loss:  1.2615561485290527\n",
      "tensor(1.2606, grad_fn=<DivBackward0>)\n",
      "tensor(1.2821, grad_fn=<DivBackward0>)\n",
      "Epoch: 32 loss:  1.2605863809585571\n",
      "tensor(1.2596, grad_fn=<DivBackward0>)\n",
      "tensor(1.2784, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 33\n",
      "Epoch: 33 loss:  1.2596380710601807\n",
      "tensor(1.2595, grad_fn=<DivBackward0>)\n",
      "tensor(1.2791, grad_fn=<DivBackward0>)\n",
      "Epoch: 34 loss:  1.2594908475875854\n",
      "tensor(1.2577, grad_fn=<DivBackward0>)\n",
      "tensor(1.2785, grad_fn=<DivBackward0>)\n",
      "Epoch: 35 loss:  1.257667064666748\n",
      "tensor(1.2569, grad_fn=<DivBackward0>)\n",
      "tensor(1.2773, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 36\n",
      "Epoch: 36 loss:  1.2569067478179932\n",
      "tensor(1.2566, grad_fn=<DivBackward0>)\n",
      "tensor(1.2776, grad_fn=<DivBackward0>)\n",
      "Epoch: 37 loss:  1.2565878629684448\n",
      "tensor(1.2561, grad_fn=<DivBackward0>)\n",
      "tensor(1.2787, grad_fn=<DivBackward0>)\n",
      "Epoch: 38 loss:  1.2561275959014893\n",
      "tensor(1.2569, grad_fn=<DivBackward0>)\n",
      "tensor(1.2788, grad_fn=<DivBackward0>)\n",
      "Epoch: 39 loss:  1.2569221258163452\n",
      "tensor(1.2600, grad_fn=<DivBackward0>)\n",
      "tensor(1.2803, grad_fn=<DivBackward0>)\n",
      "Epoch: 40 loss:  1.2599761486053467\n",
      "tensor(1.2531, grad_fn=<DivBackward0>)\n",
      "tensor(1.2728, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 41\n",
      "Epoch: 41 loss:  1.2530752420425415\n",
      "tensor(1.2552, grad_fn=<DivBackward0>)\n",
      "tensor(1.2749, grad_fn=<DivBackward0>)\n",
      "Epoch: 42 loss:  1.255191445350647\n",
      "tensor(1.2579, grad_fn=<DivBackward0>)\n",
      "tensor(1.2768, grad_fn=<DivBackward0>)\n",
      "Epoch: 43 loss:  1.2578973770141602\n",
      "tensor(1.2496, grad_fn=<DivBackward0>)\n",
      "tensor(1.2716, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 44\n",
      "Epoch: 44 loss:  1.2496269941329956\n",
      "tensor(1.2604, grad_fn=<DivBackward0>)\n",
      "tensor(1.2839, grad_fn=<DivBackward0>)\n",
      "Epoch: 45 loss:  1.260377287864685\n",
      "tensor(1.2516, grad_fn=<DivBackward0>)\n",
      "tensor(1.2706, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 46\n",
      "Epoch: 46 loss:  1.2516456842422485\n",
      "tensor(1.2538, grad_fn=<DivBackward0>)\n",
      "tensor(1.2733, grad_fn=<DivBackward0>)\n",
      "Epoch: 47 loss:  1.2538496255874634\n",
      "tensor(1.2477, grad_fn=<DivBackward0>)\n",
      "tensor(1.2647, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 48\n",
      "Epoch: 48 loss:  1.2477039098739624\n",
      "tensor(1.2516, grad_fn=<DivBackward0>)\n",
      "tensor(1.2718, grad_fn=<DivBackward0>)\n",
      "Epoch: 49 loss:  1.2516210079193115\n",
      "tensor(1.2455, grad_fn=<DivBackward0>)\n",
      "tensor(1.2666, grad_fn=<DivBackward0>)\n",
      "Epoch: 50 loss:  1.2454813718795776\n",
      "tensor(1.2483, grad_fn=<DivBackward0>)\n",
      "tensor(1.2676, grad_fn=<DivBackward0>)\n",
      "Epoch: 51 loss:  1.2483421564102173\n",
      "tensor(1.2446, grad_fn=<DivBackward0>)\n",
      "tensor(1.2702, grad_fn=<DivBackward0>)\n",
      "Epoch: 52 loss:  1.244616985321045\n",
      "tensor(1.2485, grad_fn=<DivBackward0>)\n",
      "tensor(1.2733, grad_fn=<DivBackward0>)\n",
      "Epoch: 53 loss:  1.2484532594680786\n",
      "tensor(1.2430, grad_fn=<DivBackward0>)\n",
      "tensor(1.2650, grad_fn=<DivBackward0>)\n",
      "Epoch: 54 loss:  1.2429572343826294\n",
      "tensor(1.2435, grad_fn=<DivBackward0>)\n",
      "tensor(1.2665, grad_fn=<DivBackward0>)\n",
      "Epoch: 55 loss:  1.243510365486145\n",
      "tensor(1.2416, grad_fn=<DivBackward0>)\n",
      "tensor(1.2619, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 56\n",
      "Epoch: 56 loss:  1.2415685653686523\n",
      "tensor(1.2430, grad_fn=<DivBackward0>)\n",
      "tensor(1.2661, grad_fn=<DivBackward0>)\n",
      "Epoch: 57 loss:  1.2430204153060913\n",
      "tensor(1.2397, grad_fn=<DivBackward0>)\n",
      "tensor(1.2602, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 58\n",
      "Epoch: 58 loss:  1.2396644353866577\n",
      "tensor(1.2393, grad_fn=<DivBackward0>)\n",
      "tensor(1.2629, grad_fn=<DivBackward0>)\n",
      "Epoch: 59 loss:  1.2393208742141724\n",
      "tensor(1.2389, grad_fn=<DivBackward0>)\n",
      "tensor(1.2616, grad_fn=<DivBackward0>)\n",
      "Epoch: 60 loss:  1.238940715789795\n",
      "tensor(1.2366, grad_fn=<DivBackward0>)\n",
      "tensor(1.2662, grad_fn=<DivBackward0>)\n",
      "Epoch: 61 loss:  1.236556887626648\n",
      "tensor(1.2360, grad_fn=<DivBackward0>)\n",
      "tensor(1.2615, grad_fn=<DivBackward0>)\n",
      "Epoch: 62 loss:  1.236034870147705\n",
      "tensor(1.2356, grad_fn=<DivBackward0>)\n",
      "tensor(1.2592, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 63\n",
      "Epoch: 63 loss:  1.2356468439102173\n",
      "tensor(1.2360, grad_fn=<DivBackward0>)\n",
      "tensor(1.2599, grad_fn=<DivBackward0>)\n",
      "Epoch: 64 loss:  1.2360239028930664\n",
      "tensor(1.2345, grad_fn=<DivBackward0>)\n",
      "tensor(1.2601, grad_fn=<DivBackward0>)\n",
      "Epoch: 65 loss:  1.2345386743545532\n",
      "tensor(1.2342, grad_fn=<DivBackward0>)\n",
      "tensor(1.2538, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 66\n",
      "Epoch: 66 loss:  1.234214186668396\n",
      "tensor(1.2333, grad_fn=<DivBackward0>)\n",
      "tensor(1.2559, grad_fn=<DivBackward0>)\n",
      "Epoch: 67 loss:  1.2333306074142456\n",
      "tensor(1.2316, grad_fn=<DivBackward0>)\n",
      "tensor(1.2563, grad_fn=<DivBackward0>)\n",
      "Epoch: 68 loss:  1.231641173362732\n",
      "tensor(1.2311, grad_fn=<DivBackward0>)\n",
      "tensor(1.2538, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 69\n",
      "Epoch: 69 loss:  1.2310740947723389\n",
      "tensor(1.2297, grad_fn=<DivBackward0>)\n",
      "tensor(1.2530, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 70\n",
      "Epoch: 70 loss:  1.2296760082244873\n",
      "tensor(1.2295, grad_fn=<DivBackward0>)\n",
      "tensor(1.2566, grad_fn=<DivBackward0>)\n",
      "Epoch: 71 loss:  1.2294989824295044\n",
      "tensor(1.2282, grad_fn=<DivBackward0>)\n",
      "tensor(1.2546, grad_fn=<DivBackward0>)\n",
      "Epoch: 72 loss:  1.2282241582870483\n",
      "tensor(1.2276, grad_fn=<DivBackward0>)\n",
      "tensor(1.2522, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 73\n",
      "Epoch: 73 loss:  1.2276116609573364\n",
      "tensor(1.2274, grad_fn=<DivBackward0>)\n",
      "tensor(1.2496, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 74\n",
      "Epoch: 74 loss:  1.2274229526519775\n",
      "tensor(1.2262, grad_fn=<DivBackward0>)\n",
      "tensor(1.2506, grad_fn=<DivBackward0>)\n",
      "Epoch: 75 loss:  1.2262400388717651\n",
      "tensor(1.2258, grad_fn=<DivBackward0>)\n",
      "tensor(1.2529, grad_fn=<DivBackward0>)\n",
      "Epoch: 76 loss:  1.2258108854293823\n",
      "tensor(1.2249, grad_fn=<DivBackward0>)\n",
      "tensor(1.2507, grad_fn=<DivBackward0>)\n",
      "Epoch: 77 loss:  1.2249408960342407\n",
      "tensor(1.2243, grad_fn=<DivBackward0>)\n",
      "tensor(1.2505, grad_fn=<DivBackward0>)\n",
      "Epoch: 78 loss:  1.224327802658081\n",
      "tensor(1.2224, grad_fn=<DivBackward0>)\n",
      "tensor(1.2482, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 79\n",
      "Epoch: 79 loss:  1.2224438190460205\n",
      "tensor(1.2244, grad_fn=<DivBackward0>)\n",
      "tensor(1.2491, grad_fn=<DivBackward0>)\n",
      "Epoch: 80 loss:  1.2243884801864624\n",
      "tensor(1.2223, grad_fn=<DivBackward0>)\n",
      "tensor(1.2435, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 81\n",
      "Epoch: 81 loss:  1.2222840785980225\n",
      "tensor(1.2210, grad_fn=<DivBackward0>)\n",
      "tensor(1.2471, grad_fn=<DivBackward0>)\n",
      "Epoch: 82 loss:  1.221025824546814\n",
      "tensor(1.2212, grad_fn=<DivBackward0>)\n",
      "tensor(1.2460, grad_fn=<DivBackward0>)\n",
      "Epoch: 83 loss:  1.2212413549423218\n",
      "tensor(1.2203, grad_fn=<DivBackward0>)\n",
      "tensor(1.2462, grad_fn=<DivBackward0>)\n",
      "Epoch: 84 loss:  1.2203328609466553\n",
      "tensor(1.2194, grad_fn=<DivBackward0>)\n",
      "tensor(1.2440, grad_fn=<DivBackward0>)\n",
      "Epoch: 85 loss:  1.2194443941116333\n",
      "tensor(1.2192, grad_fn=<DivBackward0>)\n",
      "tensor(1.2481, grad_fn=<DivBackward0>)\n",
      "Epoch: 86 loss:  1.2191767692565918\n",
      "tensor(1.2182, grad_fn=<DivBackward0>)\n",
      "tensor(1.2441, grad_fn=<DivBackward0>)\n",
      "Epoch: 87 loss:  1.2181507349014282\n",
      "tensor(1.2202, grad_fn=<DivBackward0>)\n",
      "tensor(1.2470, grad_fn=<DivBackward0>)\n",
      "Epoch: 88 loss:  1.2201552391052246\n",
      "tensor(1.2172, grad_fn=<DivBackward0>)\n",
      "tensor(1.2406, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 89\n",
      "Epoch: 89 loss:  1.2171828746795654\n",
      "tensor(1.2159, grad_fn=<DivBackward0>)\n",
      "tensor(1.2423, grad_fn=<DivBackward0>)\n",
      "Epoch: 90 loss:  1.2158687114715576\n",
      "tensor(1.2176, grad_fn=<DivBackward0>)\n",
      "tensor(1.2461, grad_fn=<DivBackward0>)\n",
      "Epoch: 91 loss:  1.2175909280776978\n",
      "tensor(1.2168, grad_fn=<DivBackward0>)\n",
      "tensor(1.2454, grad_fn=<DivBackward0>)\n",
      "Epoch: 92 loss:  1.2167917490005493\n",
      "tensor(1.2151, grad_fn=<DivBackward0>)\n",
      "tensor(1.2485, grad_fn=<DivBackward0>)\n",
      "Epoch: 93 loss:  1.2150893211364746\n",
      "tensor(1.2152, grad_fn=<DivBackward0>)\n",
      "tensor(1.2409, grad_fn=<DivBackward0>)\n",
      "Epoch: 94 loss:  1.2152000665664673\n",
      "tensor(1.2141, grad_fn=<DivBackward0>)\n",
      "tensor(1.2378, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 95\n",
      "Epoch: 95 loss:  1.2140984535217285\n",
      "tensor(1.2132, grad_fn=<DivBackward0>)\n",
      "tensor(1.2413, grad_fn=<DivBackward0>)\n",
      "Epoch: 96 loss:  1.213181495666504\n",
      "tensor(1.2135, grad_fn=<DivBackward0>)\n",
      "tensor(1.2378, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 97\n",
      "Epoch: 97 loss:  1.2134960889816284\n",
      "tensor(1.2132, grad_fn=<DivBackward0>)\n",
      "tensor(1.2364, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 98\n",
      "Epoch: 98 loss:  1.2132339477539062\n",
      "tensor(1.2139, grad_fn=<DivBackward0>)\n",
      "tensor(1.2393, grad_fn=<DivBackward0>)\n",
      "Epoch: 99 loss:  1.2138867378234863\n",
      "[0.9973642  0.9998921  0.99988216 ... 0.9290901  0.81196916 0.87255716]\n",
      "[0.99989545 0.99744934 0.99575514 ... 0.94619733 0.84542763 0.8945185 ]\n",
      "[0.9995051  0.99658495 0.99477875 ... 0.95226973 0.8562155  0.9032113 ]\n",
      "wl_10_5_10_5_10\n",
      "GCN_edgeweight(\n",
      "  (conv1): GCNConv(1555, 128)\n",
      "  (conv2): GCNConv(128, 16)\n",
      "  (linear): Linear(in_features=16, out_features=10, bias=True)\n",
      ")\n",
      "tensor(1.4082, grad_fn=<DivBackward0>)\n",
      "tensor(1.4083, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 0\n",
      "Epoch: 0 loss:  1.4081918001174927\n",
      "tensor(1.3960, grad_fn=<DivBackward0>)\n",
      "tensor(1.3963, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 1\n",
      "Epoch: 1 loss:  1.396026372909546\n",
      "tensor(1.3810, grad_fn=<DivBackward0>)\n",
      "tensor(1.3814, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 2\n",
      "Epoch: 2 loss:  1.3809527158737183\n",
      "tensor(1.3709, grad_fn=<DivBackward0>)\n",
      "tensor(1.3715, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 3\n",
      "Epoch: 3 loss:  1.3709275722503662\n",
      "tensor(1.3552, grad_fn=<DivBackward0>)\n",
      "tensor(1.3558, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 4\n",
      "Epoch: 4 loss:  1.3551621437072754\n",
      "tensor(1.3469, grad_fn=<DivBackward0>)\n",
      "tensor(1.3473, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 5\n",
      "Epoch: 5 loss:  1.3469157218933105\n",
      "tensor(1.3331, grad_fn=<DivBackward0>)\n",
      "tensor(1.3337, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 6\n",
      "Epoch: 6 loss:  1.3330711126327515\n",
      "tensor(1.3258, grad_fn=<DivBackward0>)\n",
      "tensor(1.3276, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 7\n",
      "Epoch: 7 loss:  1.3258062601089478\n",
      "tensor(1.3166, grad_fn=<DivBackward0>)\n",
      "tensor(1.3179, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 8\n",
      "Epoch: 8 loss:  1.3166011571884155\n",
      "tensor(1.3114, grad_fn=<DivBackward0>)\n",
      "tensor(1.3123, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 9\n",
      "Epoch: 9 loss:  1.3113503456115723\n",
      "tensor(1.3080, grad_fn=<DivBackward0>)\n",
      "tensor(1.3091, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 10\n",
      "Epoch: 10 loss:  1.308003544807434\n",
      "tensor(1.3059, grad_fn=<DivBackward0>)\n",
      "tensor(1.3077, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 11\n",
      "Epoch: 11 loss:  1.305869698524475\n",
      "tensor(1.3032, grad_fn=<DivBackward0>)\n",
      "tensor(1.3055, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 12\n",
      "Epoch: 12 loss:  1.3031939268112183\n",
      "tensor(1.2987, grad_fn=<DivBackward0>)\n",
      "tensor(1.3004, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 13\n",
      "Epoch: 13 loss:  1.2986605167388916\n",
      "tensor(1.2941, grad_fn=<DivBackward0>)\n",
      "tensor(1.2957, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 14\n",
      "Epoch: 14 loss:  1.294069528579712\n",
      "tensor(1.2907, grad_fn=<DivBackward0>)\n",
      "tensor(1.2946, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 15\n",
      "Epoch: 15 loss:  1.2907183170318604\n",
      "tensor(1.2873, grad_fn=<DivBackward0>)\n",
      "tensor(1.2917, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 16\n",
      "Epoch: 16 loss:  1.287296175956726\n",
      "tensor(1.2841, grad_fn=<DivBackward0>)\n",
      "tensor(1.2885, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 17\n",
      "Epoch: 17 loss:  1.2840816974639893\n",
      "tensor(1.2815, grad_fn=<DivBackward0>)\n",
      "tensor(1.2864, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 18\n",
      "Epoch: 18 loss:  1.2814600467681885\n",
      "tensor(1.2799, grad_fn=<DivBackward0>)\n",
      "tensor(1.2850, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 19\n",
      "Epoch: 19 loss:  1.279886245727539\n",
      "tensor(1.2766, grad_fn=<DivBackward0>)\n",
      "tensor(1.2829, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 20\n",
      "Epoch: 20 loss:  1.2766132354736328\n",
      "tensor(1.2756, grad_fn=<DivBackward0>)\n",
      "tensor(1.2812, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 21\n",
      "Epoch: 21 loss:  1.275586724281311\n",
      "tensor(1.2736, grad_fn=<DivBackward0>)\n",
      "tensor(1.2788, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 22\n",
      "Epoch: 22 loss:  1.2736436128616333\n",
      "tensor(1.2724, grad_fn=<DivBackward0>)\n",
      "tensor(1.2801, grad_fn=<DivBackward0>)\n",
      "Epoch: 23 loss:  1.272437572479248\n",
      "tensor(1.2711, grad_fn=<DivBackward0>)\n",
      "tensor(1.2795, grad_fn=<DivBackward0>)\n",
      "Epoch: 24 loss:  1.271080732345581\n",
      "tensor(1.2691, grad_fn=<DivBackward0>)\n",
      "tensor(1.2776, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 25\n",
      "Epoch: 25 loss:  1.2691441774368286\n",
      "tensor(1.2670, grad_fn=<DivBackward0>)\n",
      "tensor(1.2760, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 26\n",
      "Epoch: 26 loss:  1.266961932182312\n",
      "tensor(1.2664, grad_fn=<DivBackward0>)\n",
      "tensor(1.2773, grad_fn=<DivBackward0>)\n",
      "Epoch: 27 loss:  1.2663670778274536\n",
      "tensor(1.2645, grad_fn=<DivBackward0>)\n",
      "tensor(1.2759, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 28\n",
      "Epoch: 28 loss:  1.2645093202590942\n",
      "tensor(1.2647, grad_fn=<DivBackward0>)\n",
      "tensor(1.2763, grad_fn=<DivBackward0>)\n",
      "Epoch: 29 loss:  1.264696717262268\n",
      "tensor(1.2636, grad_fn=<DivBackward0>)\n",
      "tensor(1.2777, grad_fn=<DivBackward0>)\n",
      "Epoch: 30 loss:  1.2636216878890991\n",
      "tensor(1.2613, grad_fn=<DivBackward0>)\n",
      "tensor(1.2747, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 31\n",
      "Epoch: 31 loss:  1.2613005638122559\n",
      "tensor(1.2600, grad_fn=<DivBackward0>)\n",
      "tensor(1.2736, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 32\n",
      "Epoch: 32 loss:  1.2599852085113525\n",
      "tensor(1.2593, grad_fn=<DivBackward0>)\n",
      "tensor(1.2734, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 33\n",
      "Epoch: 33 loss:  1.2592813968658447\n",
      "tensor(1.2594, grad_fn=<DivBackward0>)\n",
      "tensor(1.2731, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 34\n",
      "Epoch: 34 loss:  1.2594157457351685\n",
      "tensor(1.2578, grad_fn=<DivBackward0>)\n",
      "tensor(1.2716, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 35\n",
      "Epoch: 35 loss:  1.2577868700027466\n",
      "tensor(1.2567, grad_fn=<DivBackward0>)\n",
      "tensor(1.2717, grad_fn=<DivBackward0>)\n",
      "Epoch: 36 loss:  1.256650686264038\n",
      "tensor(1.2567, grad_fn=<DivBackward0>)\n",
      "tensor(1.2723, grad_fn=<DivBackward0>)\n",
      "Epoch: 37 loss:  1.256656289100647\n",
      "tensor(1.2559, grad_fn=<DivBackward0>)\n",
      "tensor(1.2740, grad_fn=<DivBackward0>)\n",
      "Epoch: 38 loss:  1.2559148073196411\n",
      "tensor(1.2548, grad_fn=<DivBackward0>)\n",
      "tensor(1.2699, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 39\n",
      "Epoch: 39 loss:  1.2547662258148193\n",
      "tensor(1.2530, grad_fn=<DivBackward0>)\n",
      "tensor(1.2684, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 40\n",
      "Epoch: 40 loss:  1.2530206441879272\n",
      "tensor(1.2514, grad_fn=<DivBackward0>)\n",
      "tensor(1.2682, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 41\n",
      "Epoch: 41 loss:  1.251389741897583\n",
      "tensor(1.2505, grad_fn=<DivBackward0>)\n",
      "tensor(1.2689, grad_fn=<DivBackward0>)\n",
      "Epoch: 42 loss:  1.250502347946167\n",
      "tensor(1.2516, grad_fn=<DivBackward0>)\n",
      "tensor(1.2705, grad_fn=<DivBackward0>)\n",
      "Epoch: 43 loss:  1.2516233921051025\n",
      "tensor(1.2512, grad_fn=<DivBackward0>)\n",
      "tensor(1.2702, grad_fn=<DivBackward0>)\n",
      "Epoch: 44 loss:  1.2511796951293945\n",
      "tensor(1.2492, grad_fn=<DivBackward0>)\n",
      "tensor(1.2682, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 45\n",
      "Epoch: 45 loss:  1.2491894960403442\n",
      "tensor(1.2475, grad_fn=<DivBackward0>)\n",
      "tensor(1.2682, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 46\n",
      "Epoch: 46 loss:  1.2475366592407227\n",
      "tensor(1.2464, grad_fn=<DivBackward0>)\n",
      "tensor(1.2707, grad_fn=<DivBackward0>)\n",
      "Epoch: 47 loss:  1.2463902235031128\n",
      "tensor(1.2455, grad_fn=<DivBackward0>)\n",
      "tensor(1.2672, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 48\n",
      "Epoch: 48 loss:  1.2454742193222046\n",
      "tensor(1.2441, grad_fn=<DivBackward0>)\n",
      "tensor(1.2671, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 49\n",
      "Epoch: 49 loss:  1.2440820932388306\n",
      "tensor(1.2422, grad_fn=<DivBackward0>)\n",
      "tensor(1.2651, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 50\n",
      "Epoch: 50 loss:  1.242173671722412\n",
      "tensor(1.2429, grad_fn=<DivBackward0>)\n",
      "tensor(1.2685, grad_fn=<DivBackward0>)\n",
      "Epoch: 51 loss:  1.2428934574127197\n",
      "tensor(1.2435, grad_fn=<DivBackward0>)\n",
      "tensor(1.2698, grad_fn=<DivBackward0>)\n",
      "Epoch: 52 loss:  1.243483543395996\n",
      "tensor(1.2428, grad_fn=<DivBackward0>)\n",
      "tensor(1.2663, grad_fn=<DivBackward0>)\n",
      "Epoch: 53 loss:  1.2428191900253296\n",
      "tensor(1.2387, grad_fn=<DivBackward0>)\n",
      "tensor(1.2636, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 54\n",
      "Epoch: 54 loss:  1.238745093345642\n",
      "tensor(1.2403, grad_fn=<DivBackward0>)\n",
      "tensor(1.2653, grad_fn=<DivBackward0>)\n",
      "Epoch: 55 loss:  1.240334153175354\n",
      "tensor(1.2412, grad_fn=<DivBackward0>)\n",
      "tensor(1.2679, grad_fn=<DivBackward0>)\n",
      "Epoch: 56 loss:  1.2411905527114868\n",
      "tensor(1.2373, grad_fn=<DivBackward0>)\n",
      "tensor(1.2631, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 57\n",
      "Epoch: 57 loss:  1.2373493909835815\n",
      "tensor(1.2381, grad_fn=<DivBackward0>)\n",
      "tensor(1.2666, grad_fn=<DivBackward0>)\n",
      "Epoch: 58 loss:  1.2381179332733154\n",
      "tensor(1.2406, grad_fn=<DivBackward0>)\n",
      "tensor(1.2669, grad_fn=<DivBackward0>)\n",
      "Epoch: 59 loss:  1.2405836582183838\n",
      "tensor(1.2336, grad_fn=<DivBackward0>)\n",
      "tensor(1.2620, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 60\n",
      "Epoch: 60 loss:  1.2335981130599976\n",
      "tensor(1.2393, grad_fn=<DivBackward0>)\n",
      "tensor(1.2678, grad_fn=<DivBackward0>)\n",
      "Epoch: 61 loss:  1.2393405437469482\n",
      "tensor(1.2385, grad_fn=<DivBackward0>)\n",
      "tensor(1.2657, grad_fn=<DivBackward0>)\n",
      "Epoch: 62 loss:  1.238547921180725\n",
      "tensor(1.2329, grad_fn=<DivBackward0>)\n",
      "tensor(1.2604, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 63\n",
      "Epoch: 63 loss:  1.2328691482543945\n",
      "tensor(1.2460, grad_fn=<DivBackward0>)\n",
      "tensor(1.2724, grad_fn=<DivBackward0>)\n",
      "Epoch: 64 loss:  1.246043086051941\n",
      "tensor(1.2349, grad_fn=<DivBackward0>)\n",
      "tensor(1.2607, grad_fn=<DivBackward0>)\n",
      "Epoch: 65 loss:  1.2348861694335938\n",
      "tensor(1.2371, grad_fn=<DivBackward0>)\n",
      "tensor(1.2614, grad_fn=<DivBackward0>)\n",
      "Epoch: 66 loss:  1.2371065616607666\n",
      "tensor(1.2320, grad_fn=<DivBackward0>)\n",
      "tensor(1.2589, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 67\n",
      "Epoch: 67 loss:  1.2320380210876465\n",
      "tensor(1.2366, grad_fn=<DivBackward0>)\n",
      "tensor(1.2648, grad_fn=<DivBackward0>)\n",
      "Epoch: 68 loss:  1.2366498708724976\n",
      "tensor(1.2329, grad_fn=<DivBackward0>)\n",
      "tensor(1.2611, grad_fn=<DivBackward0>)\n",
      "Epoch: 69 loss:  1.2328989505767822\n",
      "tensor(1.2334, grad_fn=<DivBackward0>)\n",
      "tensor(1.2572, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 70\n",
      "Epoch: 70 loss:  1.2334039211273193\n",
      "tensor(1.2303, grad_fn=<DivBackward0>)\n",
      "tensor(1.2605, grad_fn=<DivBackward0>)\n",
      "Epoch: 71 loss:  1.230303168296814\n",
      "tensor(1.2335, grad_fn=<DivBackward0>)\n",
      "tensor(1.2622, grad_fn=<DivBackward0>)\n",
      "Epoch: 72 loss:  1.2334527969360352\n",
      "tensor(1.2291, grad_fn=<DivBackward0>)\n",
      "tensor(1.2584, grad_fn=<DivBackward0>)\n",
      "Epoch: 73 loss:  1.229058861732483\n",
      "tensor(1.2300, grad_fn=<DivBackward0>)\n",
      "tensor(1.2569, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 74\n",
      "Epoch: 74 loss:  1.2299981117248535\n",
      "tensor(1.2260, grad_fn=<DivBackward0>)\n",
      "tensor(1.2533, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 75\n",
      "Epoch: 75 loss:  1.2259546518325806\n",
      "tensor(1.2359, grad_fn=<DivBackward0>)\n",
      "tensor(1.2688, grad_fn=<DivBackward0>)\n",
      "Epoch: 76 loss:  1.235860824584961\n",
      "tensor(1.2254, grad_fn=<DivBackward0>)\n",
      "tensor(1.2542, grad_fn=<DivBackward0>)\n",
      "Epoch: 77 loss:  1.2253565788269043\n",
      "tensor(1.2304, grad_fn=<DivBackward0>)\n",
      "tensor(1.2596, grad_fn=<DivBackward0>)\n",
      "Epoch: 78 loss:  1.230394959449768\n",
      "tensor(1.2236, grad_fn=<DivBackward0>)\n",
      "tensor(1.2524, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 79\n",
      "Epoch: 79 loss:  1.2235615253448486\n",
      "tensor(1.2351, grad_fn=<DivBackward0>)\n",
      "tensor(1.2740, grad_fn=<DivBackward0>)\n",
      "Epoch: 80 loss:  1.23513925075531\n",
      "tensor(1.2233, grad_fn=<DivBackward0>)\n",
      "tensor(1.2528, grad_fn=<DivBackward0>)\n",
      "Epoch: 81 loss:  1.2233009338378906\n",
      "tensor(1.2280, grad_fn=<DivBackward0>)\n",
      "tensor(1.2566, grad_fn=<DivBackward0>)\n",
      "Epoch: 82 loss:  1.2279601097106934\n",
      "tensor(1.2206, grad_fn=<DivBackward0>)\n",
      "tensor(1.2536, grad_fn=<DivBackward0>)\n",
      "Epoch: 83 loss:  1.2205557823181152\n",
      "tensor(1.2317, grad_fn=<DivBackward0>)\n",
      "tensor(1.2647, grad_fn=<DivBackward0>)\n",
      "Epoch: 84 loss:  1.2317360639572144\n",
      "tensor(1.2193, grad_fn=<DivBackward0>)\n",
      "tensor(1.2511, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 85\n",
      "Epoch: 85 loss:  1.219336748123169\n",
      "tensor(1.2246, grad_fn=<DivBackward0>)\n",
      "tensor(1.2530, grad_fn=<DivBackward0>)\n",
      "Epoch: 86 loss:  1.2245975732803345\n",
      "tensor(1.2195, grad_fn=<DivBackward0>)\n",
      "tensor(1.2487, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 87\n",
      "Epoch: 87 loss:  1.2195497751235962\n",
      "tensor(1.2290, grad_fn=<DivBackward0>)\n",
      "tensor(1.2664, grad_fn=<DivBackward0>)\n",
      "Epoch: 88 loss:  1.2289916276931763\n",
      "tensor(1.2174, grad_fn=<DivBackward0>)\n",
      "tensor(1.2577, grad_fn=<DivBackward0>)\n",
      "Epoch: 89 loss:  1.2173501253128052\n",
      "tensor(1.2200, grad_fn=<DivBackward0>)\n",
      "tensor(1.2506, grad_fn=<DivBackward0>)\n",
      "Epoch: 90 loss:  1.2200309038162231\n",
      "tensor(1.2162, grad_fn=<DivBackward0>)\n",
      "tensor(1.2508, grad_fn=<DivBackward0>)\n",
      "Epoch: 91 loss:  1.2161957025527954\n",
      "tensor(1.2204, grad_fn=<DivBackward0>)\n",
      "tensor(1.2558, grad_fn=<DivBackward0>)\n",
      "Epoch: 92 loss:  1.2203640937805176\n",
      "tensor(1.2179, grad_fn=<DivBackward0>)\n",
      "tensor(1.2483, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 93\n",
      "Epoch: 93 loss:  1.2178559303283691\n",
      "tensor(1.2173, grad_fn=<DivBackward0>)\n",
      "tensor(1.2512, grad_fn=<DivBackward0>)\n",
      "Epoch: 94 loss:  1.217285394668579\n",
      "tensor(1.2138, grad_fn=<DivBackward0>)\n",
      "tensor(1.2508, grad_fn=<DivBackward0>)\n",
      "Epoch: 95 loss:  1.2138484716415405\n",
      "tensor(1.2158, grad_fn=<DivBackward0>)\n",
      "tensor(1.2540, grad_fn=<DivBackward0>)\n",
      "Epoch: 96 loss:  1.2158372402191162\n",
      "tensor(1.2140, grad_fn=<DivBackward0>)\n",
      "tensor(1.2535, grad_fn=<DivBackward0>)\n",
      "Epoch: 97 loss:  1.2140414714813232\n",
      "tensor(1.2139, grad_fn=<DivBackward0>)\n",
      "tensor(1.2498, grad_fn=<DivBackward0>)\n",
      "Epoch: 98 loss:  1.213889479637146\n",
      "tensor(1.2116, grad_fn=<DivBackward0>)\n",
      "tensor(1.2562, grad_fn=<DivBackward0>)\n",
      "Epoch: 99 loss:  1.2115813493728638\n",
      "[0.9982114  0.9999015  0.99993575 ... 0.9099186  0.79614884 0.8629593 ]\n",
      "[0.9997778  0.9978796  0.9974306  ... 0.9289278  0.82789826 0.8846045 ]\n",
      "[0.99833083 0.9954175  0.9947508  ... 0.94279647 0.85023004 0.9033266 ]\n",
      "wl_10_10_10_1_1\n",
      "GCN_edgeweight(\n",
      "  (conv1): GCNConv(1555, 128)\n",
      "  (conv2): GCNConv(128, 16)\n",
      "  (linear): Linear(in_features=16, out_features=10, bias=True)\n",
      ")\n",
      "tensor(1.4082, grad_fn=<DivBackward0>)\n",
      "tensor(1.4083, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 0\n",
      "Epoch: 0 loss:  1.4082167148590088\n",
      "tensor(1.3963, grad_fn=<DivBackward0>)\n",
      "tensor(1.3958, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 1\n",
      "Epoch: 1 loss:  1.3963102102279663\n",
      "tensor(1.3814, grad_fn=<DivBackward0>)\n",
      "tensor(1.3798, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 2\n",
      "Epoch: 2 loss:  1.3814369440078735\n",
      "tensor(1.3710, grad_fn=<DivBackward0>)\n",
      "tensor(1.3690, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 3\n",
      "Epoch: 3 loss:  1.3709700107574463\n",
      "tensor(1.3558, grad_fn=<DivBackward0>)\n",
      "tensor(1.3536, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 4\n",
      "Epoch: 4 loss:  1.3558363914489746\n",
      "tensor(1.3470, grad_fn=<DivBackward0>)\n",
      "tensor(1.3447, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 5\n",
      "Epoch: 5 loss:  1.3470075130462646\n",
      "tensor(1.3330, grad_fn=<DivBackward0>)\n",
      "tensor(1.3305, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 6\n",
      "Epoch: 6 loss:  1.332990288734436\n",
      "tensor(1.3271, grad_fn=<DivBackward0>)\n",
      "tensor(1.3260, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 7\n",
      "Epoch: 7 loss:  1.327113389968872\n",
      "tensor(1.3166, grad_fn=<DivBackward0>)\n",
      "tensor(1.3146, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 8\n",
      "Epoch: 8 loss:  1.3165748119354248\n",
      "tensor(1.3128, grad_fn=<DivBackward0>)\n",
      "tensor(1.3103, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 9\n",
      "Epoch: 9 loss:  1.3127518892288208\n",
      "tensor(1.3091, grad_fn=<DivBackward0>)\n",
      "tensor(1.3069, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 10\n",
      "Epoch: 10 loss:  1.3090506792068481\n",
      "tensor(1.3077, grad_fn=<DivBackward0>)\n",
      "tensor(1.3067, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 11\n",
      "Epoch: 11 loss:  1.3076852560043335\n",
      "tensor(1.3037, grad_fn=<DivBackward0>)\n",
      "tensor(1.3032, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 12\n",
      "Epoch: 12 loss:  1.3036761283874512\n",
      "tensor(1.2996, grad_fn=<DivBackward0>)\n",
      "tensor(1.2989, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 13\n",
      "Epoch: 13 loss:  1.2995569705963135\n",
      "tensor(1.2946, grad_fn=<DivBackward0>)\n",
      "tensor(1.2946, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 14\n",
      "Epoch: 14 loss:  1.2945793867111206\n",
      "tensor(1.2922, grad_fn=<DivBackward0>)\n",
      "tensor(1.2931, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 15\n",
      "Epoch: 15 loss:  1.2921544313430786\n",
      "tensor(1.2882, grad_fn=<DivBackward0>)\n",
      "tensor(1.2898, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 16\n",
      "Epoch: 16 loss:  1.2881817817687988\n",
      "tensor(1.2855, grad_fn=<DivBackward0>)\n",
      "tensor(1.2864, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 17\n",
      "Epoch: 17 loss:  1.2855478525161743\n",
      "tensor(1.2824, grad_fn=<DivBackward0>)\n",
      "tensor(1.2845, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 18\n",
      "Epoch: 18 loss:  1.2824386358261108\n",
      "tensor(1.2814, grad_fn=<DivBackward0>)\n",
      "tensor(1.2837, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 19\n",
      "Epoch: 19 loss:  1.2814310789108276\n",
      "tensor(1.2784, grad_fn=<DivBackward0>)\n",
      "tensor(1.2803, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 20\n",
      "Epoch: 20 loss:  1.2783598899841309\n",
      "tensor(1.2772, grad_fn=<DivBackward0>)\n",
      "tensor(1.2784, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 21\n",
      "Epoch: 21 loss:  1.2771722078323364\n",
      "tensor(1.2753, grad_fn=<DivBackward0>)\n",
      "tensor(1.2797, grad_fn=<DivBackward0>)\n",
      "Epoch: 22 loss:  1.2753372192382812\n",
      "tensor(1.2740, grad_fn=<DivBackward0>)\n",
      "tensor(1.2777, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 23\n",
      "Epoch: 23 loss:  1.2740415334701538\n",
      "tensor(1.2722, grad_fn=<DivBackward0>)\n",
      "tensor(1.2774, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 24\n",
      "Epoch: 24 loss:  1.2722011804580688\n",
      "tensor(1.2709, grad_fn=<DivBackward0>)\n",
      "tensor(1.2780, grad_fn=<DivBackward0>)\n",
      "Epoch: 25 loss:  1.2708920240402222\n",
      "tensor(1.2687, grad_fn=<DivBackward0>)\n",
      "tensor(1.2770, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 26\n",
      "Epoch: 26 loss:  1.2686800956726074\n",
      "tensor(1.2683, grad_fn=<DivBackward0>)\n",
      "tensor(1.2756, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 27\n",
      "Epoch: 27 loss:  1.2683323621749878\n",
      "tensor(1.2663, grad_fn=<DivBackward0>)\n",
      "tensor(1.2763, grad_fn=<DivBackward0>)\n",
      "Epoch: 28 loss:  1.2662981748580933\n",
      "tensor(1.2668, grad_fn=<DivBackward0>)\n",
      "tensor(1.2750, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 29\n",
      "Epoch: 29 loss:  1.2668429613113403\n",
      "tensor(1.2658, grad_fn=<DivBackward0>)\n",
      "tensor(1.2758, grad_fn=<DivBackward0>)\n",
      "Epoch: 30 loss:  1.265798807144165\n",
      "tensor(1.2634, grad_fn=<DivBackward0>)\n",
      "tensor(1.2718, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 31\n",
      "Epoch: 31 loss:  1.263437271118164\n",
      "tensor(1.2621, grad_fn=<DivBackward0>)\n",
      "tensor(1.2731, grad_fn=<DivBackward0>)\n",
      "Epoch: 32 loss:  1.2621138095855713\n",
      "tensor(1.2619, grad_fn=<DivBackward0>)\n",
      "tensor(1.2728, grad_fn=<DivBackward0>)\n",
      "Epoch: 33 loss:  1.2618796825408936\n",
      "tensor(1.2614, grad_fn=<DivBackward0>)\n",
      "tensor(1.2711, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 34\n",
      "Epoch: 34 loss:  1.2614448070526123\n",
      "tensor(1.2596, grad_fn=<DivBackward0>)\n",
      "tensor(1.2713, grad_fn=<DivBackward0>)\n",
      "Epoch: 35 loss:  1.2596416473388672\n",
      "tensor(1.2583, grad_fn=<DivBackward0>)\n",
      "tensor(1.2695, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 36\n",
      "Epoch: 36 loss:  1.2583197355270386\n",
      "tensor(1.2579, grad_fn=<DivBackward0>)\n",
      "tensor(1.2724, grad_fn=<DivBackward0>)\n",
      "Epoch: 37 loss:  1.2579375505447388\n",
      "tensor(1.2574, grad_fn=<DivBackward0>)\n",
      "tensor(1.2702, grad_fn=<DivBackward0>)\n",
      "Epoch: 38 loss:  1.2574025392532349\n",
      "tensor(1.2561, grad_fn=<DivBackward0>)\n",
      "tensor(1.2693, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 39\n",
      "Epoch: 39 loss:  1.2561296224594116\n",
      "tensor(1.2546, grad_fn=<DivBackward0>)\n",
      "tensor(1.2677, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 40\n",
      "Epoch: 40 loss:  1.2545530796051025\n",
      "tensor(1.2530, grad_fn=<DivBackward0>)\n",
      "tensor(1.2657, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 41\n",
      "Epoch: 41 loss:  1.2530306577682495\n",
      "tensor(1.2520, grad_fn=<DivBackward0>)\n",
      "tensor(1.2660, grad_fn=<DivBackward0>)\n",
      "Epoch: 42 loss:  1.2520440816879272\n",
      "tensor(1.2521, grad_fn=<DivBackward0>)\n",
      "tensor(1.2674, grad_fn=<DivBackward0>)\n",
      "Epoch: 43 loss:  1.252079725265503\n",
      "tensor(1.2508, grad_fn=<DivBackward0>)\n",
      "tensor(1.2689, grad_fn=<DivBackward0>)\n",
      "Epoch: 44 loss:  1.2507538795471191\n",
      "tensor(1.2514, grad_fn=<DivBackward0>)\n",
      "tensor(1.2669, grad_fn=<DivBackward0>)\n",
      "Epoch: 45 loss:  1.2513874769210815\n",
      "tensor(1.2576, grad_fn=<DivBackward0>)\n",
      "tensor(1.2774, grad_fn=<DivBackward0>)\n",
      "Epoch: 46 loss:  1.2576204538345337\n",
      "tensor(1.2570, grad_fn=<DivBackward0>)\n",
      "tensor(1.2704, grad_fn=<DivBackward0>)\n",
      "Epoch: 47 loss:  1.2570303678512573\n",
      "tensor(1.2479, grad_fn=<DivBackward0>)\n",
      "tensor(1.2626, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 48\n",
      "Epoch: 48 loss:  1.24785578250885\n",
      "tensor(1.2570, grad_fn=<DivBackward0>)\n",
      "tensor(1.2744, grad_fn=<DivBackward0>)\n",
      "Epoch: 49 loss:  1.2569584846496582\n",
      "tensor(1.2449, grad_fn=<DivBackward0>)\n",
      "tensor(1.2585, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 50\n",
      "Epoch: 50 loss:  1.244861125946045\n",
      "tensor(1.2503, grad_fn=<DivBackward0>)\n",
      "tensor(1.2658, grad_fn=<DivBackward0>)\n",
      "Epoch: 51 loss:  1.2502529621124268\n",
      "tensor(1.2430, grad_fn=<DivBackward0>)\n",
      "tensor(1.2606, grad_fn=<DivBackward0>)\n",
      "Epoch: 52 loss:  1.2430365085601807\n",
      "tensor(1.2515, grad_fn=<DivBackward0>)\n",
      "tensor(1.2713, grad_fn=<DivBackward0>)\n",
      "Epoch: 53 loss:  1.251474380493164\n",
      "tensor(1.2440, grad_fn=<DivBackward0>)\n",
      "tensor(1.2615, grad_fn=<DivBackward0>)\n",
      "Epoch: 54 loss:  1.2439824342727661\n",
      "tensor(1.2446, grad_fn=<DivBackward0>)\n",
      "tensor(1.2608, grad_fn=<DivBackward0>)\n",
      "Epoch: 55 loss:  1.2445764541625977\n",
      "tensor(1.2412, grad_fn=<DivBackward0>)\n",
      "tensor(1.2631, grad_fn=<DivBackward0>)\n",
      "Epoch: 56 loss:  1.2412467002868652\n",
      "tensor(1.2446, grad_fn=<DivBackward0>)\n",
      "tensor(1.2670, grad_fn=<DivBackward0>)\n",
      "Epoch: 57 loss:  1.2446093559265137\n",
      "tensor(1.2401, grad_fn=<DivBackward0>)\n",
      "tensor(1.2556, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 58\n",
      "Epoch: 58 loss:  1.240145206451416\n",
      "tensor(1.2409, grad_fn=<DivBackward0>)\n",
      "tensor(1.2567, grad_fn=<DivBackward0>)\n",
      "Epoch: 59 loss:  1.2409255504608154\n",
      "tensor(1.2384, grad_fn=<DivBackward0>)\n",
      "tensor(1.2608, grad_fn=<DivBackward0>)\n",
      "Epoch: 60 loss:  1.2383779287338257\n",
      "tensor(1.2393, grad_fn=<DivBackward0>)\n",
      "tensor(1.2625, grad_fn=<DivBackward0>)\n",
      "Epoch: 61 loss:  1.239319086074829\n",
      "tensor(1.2368, grad_fn=<DivBackward0>)\n",
      "tensor(1.2571, grad_fn=<DivBackward0>)\n",
      "Epoch: 62 loss:  1.2368004322052002\n",
      "tensor(1.2365, grad_fn=<DivBackward0>)\n",
      "tensor(1.2572, grad_fn=<DivBackward0>)\n",
      "Epoch: 63 loss:  1.2364742755889893\n",
      "tensor(1.2363, grad_fn=<DivBackward0>)\n",
      "tensor(1.2594, grad_fn=<DivBackward0>)\n",
      "Epoch: 64 loss:  1.2363098859786987\n",
      "tensor(1.2348, grad_fn=<DivBackward0>)\n",
      "tensor(1.2605, grad_fn=<DivBackward0>)\n",
      "Epoch: 65 loss:  1.2348332405090332\n",
      "tensor(1.2339, grad_fn=<DivBackward0>)\n",
      "tensor(1.2538, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 66\n",
      "Epoch: 66 loss:  1.233940601348877\n",
      "tensor(1.2330, grad_fn=<DivBackward0>)\n",
      "tensor(1.2543, grad_fn=<DivBackward0>)\n",
      "Epoch: 67 loss:  1.2329779863357544\n",
      "tensor(1.2355, grad_fn=<DivBackward0>)\n",
      "tensor(1.2592, grad_fn=<DivBackward0>)\n",
      "Epoch: 68 loss:  1.2354557514190674\n",
      "tensor(1.2307, grad_fn=<DivBackward0>)\n",
      "tensor(1.2537, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 69\n",
      "Epoch: 69 loss:  1.2306804656982422\n",
      "tensor(1.2304, grad_fn=<DivBackward0>)\n",
      "tensor(1.2519, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 70\n",
      "Epoch: 70 loss:  1.2303674221038818\n",
      "tensor(1.2309, grad_fn=<DivBackward0>)\n",
      "tensor(1.2560, grad_fn=<DivBackward0>)\n",
      "Epoch: 71 loss:  1.2309134006500244\n",
      "tensor(1.2291, grad_fn=<DivBackward0>)\n",
      "tensor(1.2514, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 72\n",
      "Epoch: 72 loss:  1.2291316986083984\n",
      "tensor(1.2297, grad_fn=<DivBackward0>)\n",
      "tensor(1.2527, grad_fn=<DivBackward0>)\n",
      "Epoch: 73 loss:  1.2297041416168213\n",
      "tensor(1.2255, grad_fn=<DivBackward0>)\n",
      "tensor(1.2521, grad_fn=<DivBackward0>)\n",
      "Epoch: 74 loss:  1.2255446910858154\n",
      "tensor(1.2294, grad_fn=<DivBackward0>)\n",
      "tensor(1.2549, grad_fn=<DivBackward0>)\n",
      "Epoch: 75 loss:  1.2293747663497925\n",
      "tensor(1.2259, grad_fn=<DivBackward0>)\n",
      "tensor(1.2483, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 76\n",
      "Epoch: 76 loss:  1.2258788347244263\n",
      "tensor(1.2241, grad_fn=<DivBackward0>)\n",
      "tensor(1.2472, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 77\n",
      "Epoch: 77 loss:  1.2241297960281372\n",
      "tensor(1.2250, grad_fn=<DivBackward0>)\n",
      "tensor(1.2505, grad_fn=<DivBackward0>)\n",
      "Epoch: 78 loss:  1.2250245809555054\n",
      "tensor(1.2234, grad_fn=<DivBackward0>)\n",
      "tensor(1.2462, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 79\n",
      "Epoch: 79 loss:  1.223379135131836\n",
      "tensor(1.2230, grad_fn=<DivBackward0>)\n",
      "tensor(1.2476, grad_fn=<DivBackward0>)\n",
      "Epoch: 80 loss:  1.2229845523834229\n",
      "tensor(1.2218, grad_fn=<DivBackward0>)\n",
      "tensor(1.2500, grad_fn=<DivBackward0>)\n",
      "Epoch: 81 loss:  1.2218433618545532\n",
      "tensor(1.2217, grad_fn=<DivBackward0>)\n",
      "tensor(1.2501, grad_fn=<DivBackward0>)\n",
      "Epoch: 82 loss:  1.2216627597808838\n",
      "tensor(1.2204, grad_fn=<DivBackward0>)\n",
      "tensor(1.2426, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 83\n",
      "Epoch: 83 loss:  1.2203601598739624\n",
      "tensor(1.2196, grad_fn=<DivBackward0>)\n",
      "tensor(1.2448, grad_fn=<DivBackward0>)\n",
      "Epoch: 84 loss:  1.219619870185852\n",
      "tensor(1.2202, grad_fn=<DivBackward0>)\n",
      "tensor(1.2468, grad_fn=<DivBackward0>)\n",
      "Epoch: 85 loss:  1.2201578617095947\n",
      "tensor(1.2188, grad_fn=<DivBackward0>)\n",
      "tensor(1.2429, grad_fn=<DivBackward0>)\n",
      "Epoch: 86 loss:  1.2187837362289429\n",
      "tensor(1.2184, grad_fn=<DivBackward0>)\n",
      "tensor(1.2421, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 87\n",
      "Epoch: 87 loss:  1.218415379524231\n",
      "tensor(1.2197, grad_fn=<DivBackward0>)\n",
      "tensor(1.2492, grad_fn=<DivBackward0>)\n",
      "Epoch: 88 loss:  1.2196685075759888\n",
      "tensor(1.2194, grad_fn=<DivBackward0>)\n",
      "tensor(1.2416, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 89\n",
      "Epoch: 89 loss:  1.219437599182129\n",
      "tensor(1.2171, grad_fn=<DivBackward0>)\n",
      "tensor(1.2436, grad_fn=<DivBackward0>)\n",
      "Epoch: 90 loss:  1.2171084880828857\n",
      "tensor(1.2178, grad_fn=<DivBackward0>)\n",
      "tensor(1.2461, grad_fn=<DivBackward0>)\n",
      "Epoch: 91 loss:  1.2177729606628418\n",
      "tensor(1.2186, grad_fn=<DivBackward0>)\n",
      "tensor(1.2417, grad_fn=<DivBackward0>)\n",
      "Epoch: 92 loss:  1.2185821533203125\n",
      "tensor(1.2169, grad_fn=<DivBackward0>)\n",
      "tensor(1.2429, grad_fn=<DivBackward0>)\n",
      "Epoch: 93 loss:  1.2168697118759155\n",
      "tensor(1.2165, grad_fn=<DivBackward0>)\n",
      "tensor(1.2473, grad_fn=<DivBackward0>)\n",
      "Epoch: 94 loss:  1.2165156602859497\n",
      "tensor(1.2150, grad_fn=<DivBackward0>)\n",
      "tensor(1.2428, grad_fn=<DivBackward0>)\n",
      "Epoch: 95 loss:  1.2149693965911865\n",
      "tensor(1.2138, grad_fn=<DivBackward0>)\n",
      "tensor(1.2431, grad_fn=<DivBackward0>)\n",
      "Epoch: 96 loss:  1.2137919664382935\n",
      "tensor(1.2177, grad_fn=<DivBackward0>)\n",
      "tensor(1.2481, grad_fn=<DivBackward0>)\n",
      "Epoch: 97 loss:  1.2176687717437744\n",
      "tensor(1.2166, grad_fn=<DivBackward0>)\n",
      "tensor(1.2415, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 98\n",
      "Epoch: 98 loss:  1.2165822982788086\n",
      "tensor(1.2117, grad_fn=<DivBackward0>)\n",
      "tensor(1.2368, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 99\n",
      "Epoch: 99 loss:  1.2117178440093994\n",
      "[0.9976982  0.9999124  0.99990714 ... 0.9141062  0.7981816  0.8396429 ]\n",
      "[0.99989736 0.99731714 0.9963061  ... 0.93219936 0.8301112  0.8631159 ]\n",
      "[0.9990775  0.99547917 0.9943853  ... 0.9426037  0.8471618  0.8787972 ]\n",
      "wl_10_1_1_1_1\n",
      "GCN_edgeweight(\n",
      "  (conv1): GCNConv(1555, 128)\n",
      "  (conv2): GCNConv(128, 16)\n",
      "  (linear): Linear(in_features=16, out_features=10, bias=True)\n",
      ")\n",
      "tensor(1.4084, grad_fn=<DivBackward0>)\n",
      "tensor(1.4085, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 0\n",
      "Epoch: 0 loss:  1.4083601236343384\n",
      "tensor(1.3977, grad_fn=<DivBackward0>)\n",
      "tensor(1.3980, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 1\n",
      "Epoch: 1 loss:  1.3977470397949219\n",
      "tensor(1.3845, grad_fn=<DivBackward0>)\n",
      "tensor(1.3849, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 2\n",
      "Epoch: 2 loss:  1.3844974040985107\n",
      "tensor(1.3729, grad_fn=<DivBackward0>)\n",
      "tensor(1.3737, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 3\n",
      "Epoch: 3 loss:  1.372879981994629\n",
      "tensor(1.3596, grad_fn=<DivBackward0>)\n",
      "tensor(1.3604, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 4\n",
      "Epoch: 4 loss:  1.3595623970031738\n",
      "tensor(1.3481, grad_fn=<DivBackward0>)\n",
      "tensor(1.3490, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 5\n",
      "Epoch: 5 loss:  1.34806227684021\n",
      "tensor(1.3361, grad_fn=<DivBackward0>)\n",
      "tensor(1.3381, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 6\n",
      "Epoch: 6 loss:  1.3360648155212402\n",
      "tensor(1.3285, grad_fn=<DivBackward0>)\n",
      "tensor(1.3316, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 7\n",
      "Epoch: 7 loss:  1.328513503074646\n",
      "tensor(1.3214, grad_fn=<DivBackward0>)\n",
      "tensor(1.3238, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 8\n",
      "Epoch: 8 loss:  1.321371078491211\n",
      "tensor(1.3189, grad_fn=<DivBackward0>)\n",
      "tensor(1.3214, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 9\n",
      "Epoch: 9 loss:  1.3189163208007812\n",
      "tensor(1.3176, grad_fn=<DivBackward0>)\n",
      "tensor(1.3207, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 10\n",
      "Epoch: 10 loss:  1.3176045417785645\n",
      "tensor(1.3128, grad_fn=<DivBackward0>)\n",
      "tensor(1.3158, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 11\n",
      "Epoch: 11 loss:  1.312819242477417\n",
      "tensor(1.3082, grad_fn=<DivBackward0>)\n",
      "tensor(1.3126, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 12\n",
      "Epoch: 12 loss:  1.3082478046417236\n",
      "tensor(1.3059, grad_fn=<DivBackward0>)\n",
      "tensor(1.3114, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 13\n",
      "Epoch: 13 loss:  1.3059418201446533\n",
      "tensor(1.3015, grad_fn=<DivBackward0>)\n",
      "tensor(1.3069, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 14\n",
      "Epoch: 14 loss:  1.301450252532959\n",
      "tensor(1.2997, grad_fn=<DivBackward0>)\n",
      "tensor(1.3057, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 15\n",
      "Epoch: 15 loss:  1.2996916770935059\n",
      "tensor(1.2973, grad_fn=<DivBackward0>)\n",
      "tensor(1.3047, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 16\n",
      "Epoch: 16 loss:  1.2972626686096191\n",
      "tensor(1.2943, grad_fn=<DivBackward0>)\n",
      "tensor(1.3012, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 17\n",
      "Epoch: 17 loss:  1.294326663017273\n",
      "tensor(1.2927, grad_fn=<DivBackward0>)\n",
      "tensor(1.2997, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 18\n",
      "Epoch: 18 loss:  1.2927014827728271\n",
      "tensor(1.2909, grad_fn=<DivBackward0>)\n",
      "tensor(1.2980, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 19\n",
      "Epoch: 19 loss:  1.290887475013733\n",
      "tensor(1.2890, grad_fn=<DivBackward0>)\n",
      "tensor(1.2972, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 20\n",
      "Epoch: 20 loss:  1.2889864444732666\n",
      "tensor(1.2873, grad_fn=<DivBackward0>)\n",
      "tensor(1.2970, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 21\n",
      "Epoch: 21 loss:  1.2873308658599854\n",
      "tensor(1.2856, grad_fn=<DivBackward0>)\n",
      "tensor(1.2945, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 22\n",
      "Epoch: 22 loss:  1.2856218814849854\n",
      "tensor(1.2855, grad_fn=<DivBackward0>)\n",
      "tensor(1.2955, grad_fn=<DivBackward0>)\n",
      "Epoch: 23 loss:  1.2854984998703003\n",
      "tensor(1.2854, grad_fn=<DivBackward0>)\n",
      "tensor(1.2947, grad_fn=<DivBackward0>)\n",
      "Epoch: 24 loss:  1.2853542566299438\n",
      "tensor(1.2844, grad_fn=<DivBackward0>)\n",
      "tensor(1.2972, grad_fn=<DivBackward0>)\n",
      "Epoch: 25 loss:  1.284358263015747\n",
      "tensor(1.2813, grad_fn=<DivBackward0>)\n",
      "tensor(1.2917, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 26\n",
      "Epoch: 26 loss:  1.281313180923462\n",
      "tensor(1.2805, grad_fn=<DivBackward0>)\n",
      "tensor(1.2921, grad_fn=<DivBackward0>)\n",
      "Epoch: 27 loss:  1.2805086374282837\n",
      "tensor(1.2807, grad_fn=<DivBackward0>)\n",
      "tensor(1.2922, grad_fn=<DivBackward0>)\n",
      "Epoch: 28 loss:  1.2806575298309326\n",
      "tensor(1.2792, grad_fn=<DivBackward0>)\n",
      "tensor(1.2901, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 29\n",
      "Epoch: 29 loss:  1.2791892290115356\n",
      "tensor(1.2778, grad_fn=<DivBackward0>)\n",
      "tensor(1.2898, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 30\n",
      "Epoch: 30 loss:  1.2778229713439941\n",
      "tensor(1.2779, grad_fn=<DivBackward0>)\n",
      "tensor(1.2928, grad_fn=<DivBackward0>)\n",
      "Epoch: 31 loss:  1.2779327630996704\n",
      "tensor(1.2759, grad_fn=<DivBackward0>)\n",
      "tensor(1.2915, grad_fn=<DivBackward0>)\n",
      "Epoch: 32 loss:  1.2758713960647583\n",
      "tensor(1.2745, grad_fn=<DivBackward0>)\n",
      "tensor(1.2910, grad_fn=<DivBackward0>)\n",
      "Epoch: 33 loss:  1.2745145559310913\n",
      "tensor(1.2764, grad_fn=<DivBackward0>)\n",
      "tensor(1.2925, grad_fn=<DivBackward0>)\n",
      "Epoch: 34 loss:  1.2764391899108887\n",
      "tensor(1.2735, grad_fn=<DivBackward0>)\n",
      "tensor(1.2901, grad_fn=<DivBackward0>)\n",
      "Epoch: 35 loss:  1.2735005617141724\n",
      "tensor(1.2727, grad_fn=<DivBackward0>)\n",
      "tensor(1.2887, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 36\n",
      "Epoch: 36 loss:  1.2726794481277466\n",
      "tensor(1.2736, grad_fn=<DivBackward0>)\n",
      "tensor(1.2912, grad_fn=<DivBackward0>)\n",
      "Epoch: 37 loss:  1.2736027240753174\n",
      "tensor(1.2723, grad_fn=<DivBackward0>)\n",
      "tensor(1.2906, grad_fn=<DivBackward0>)\n",
      "Epoch: 38 loss:  1.272312879562378\n",
      "tensor(1.2712, grad_fn=<DivBackward0>)\n",
      "tensor(1.2886, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 39\n",
      "Epoch: 39 loss:  1.2711964845657349\n",
      "tensor(1.2716, grad_fn=<DivBackward0>)\n",
      "tensor(1.2906, grad_fn=<DivBackward0>)\n",
      "Epoch: 40 loss:  1.2715632915496826\n",
      "tensor(1.2690, grad_fn=<DivBackward0>)\n",
      "tensor(1.2867, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 41\n",
      "Epoch: 41 loss:  1.2689586877822876\n",
      "tensor(1.2684, grad_fn=<DivBackward0>)\n",
      "tensor(1.2865, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 42\n",
      "Epoch: 42 loss:  1.268411636352539\n",
      "tensor(1.2692, grad_fn=<DivBackward0>)\n",
      "tensor(1.2878, grad_fn=<DivBackward0>)\n",
      "Epoch: 43 loss:  1.2692025899887085\n",
      "tensor(1.2672, grad_fn=<DivBackward0>)\n",
      "tensor(1.2862, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 44\n",
      "Epoch: 44 loss:  1.267189860343933\n",
      "tensor(1.2661, grad_fn=<DivBackward0>)\n",
      "tensor(1.2860, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 45\n",
      "Epoch: 45 loss:  1.2660852670669556\n",
      "tensor(1.2671, grad_fn=<DivBackward0>)\n",
      "tensor(1.2880, grad_fn=<DivBackward0>)\n",
      "Epoch: 46 loss:  1.2670769691467285\n",
      "tensor(1.2654, grad_fn=<DivBackward0>)\n",
      "tensor(1.2836, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 47\n",
      "Epoch: 47 loss:  1.265372633934021\n",
      "tensor(1.2642, grad_fn=<DivBackward0>)\n",
      "tensor(1.2841, grad_fn=<DivBackward0>)\n",
      "Epoch: 48 loss:  1.2642418146133423\n",
      "tensor(1.2633, grad_fn=<DivBackward0>)\n",
      "tensor(1.2845, grad_fn=<DivBackward0>)\n",
      "Epoch: 49 loss:  1.2632949352264404\n",
      "tensor(1.2617, grad_fn=<DivBackward0>)\n",
      "tensor(1.2844, grad_fn=<DivBackward0>)\n",
      "Epoch: 50 loss:  1.2616746425628662\n",
      "tensor(1.2615, grad_fn=<DivBackward0>)\n",
      "tensor(1.2835, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 51\n",
      "Epoch: 51 loss:  1.261479139328003\n",
      "tensor(1.2616, grad_fn=<DivBackward0>)\n",
      "tensor(1.2843, grad_fn=<DivBackward0>)\n",
      "Epoch: 52 loss:  1.2616453170776367\n",
      "tensor(1.2611, grad_fn=<DivBackward0>)\n",
      "tensor(1.2833, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 53\n",
      "Epoch: 53 loss:  1.2610690593719482\n",
      "tensor(1.2595, grad_fn=<DivBackward0>)\n",
      "tensor(1.2829, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 54\n",
      "Epoch: 54 loss:  1.259522557258606\n",
      "tensor(1.2592, grad_fn=<DivBackward0>)\n",
      "tensor(1.2823, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 55\n",
      "Epoch: 55 loss:  1.2591588497161865\n",
      "tensor(1.2576, grad_fn=<DivBackward0>)\n",
      "tensor(1.2791, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 56\n",
      "Epoch: 56 loss:  1.257559895515442\n",
      "tensor(1.2586, grad_fn=<DivBackward0>)\n",
      "tensor(1.2839, grad_fn=<DivBackward0>)\n",
      "Epoch: 57 loss:  1.2586407661437988\n",
      "tensor(1.2579, grad_fn=<DivBackward0>)\n",
      "tensor(1.2812, grad_fn=<DivBackward0>)\n",
      "Epoch: 58 loss:  1.2579002380371094\n",
      "tensor(1.2579, grad_fn=<DivBackward0>)\n",
      "tensor(1.2819, grad_fn=<DivBackward0>)\n",
      "Epoch: 59 loss:  1.2578723430633545\n",
      "tensor(1.2561, grad_fn=<DivBackward0>)\n",
      "tensor(1.2794, grad_fn=<DivBackward0>)\n",
      "Epoch: 60 loss:  1.2561299800872803\n",
      "tensor(1.2533, grad_fn=<DivBackward0>)\n",
      "tensor(1.2799, grad_fn=<DivBackward0>)\n",
      "Epoch: 61 loss:  1.2533060312271118\n",
      "tensor(1.2550, grad_fn=<DivBackward0>)\n",
      "tensor(1.2808, grad_fn=<DivBackward0>)\n",
      "Epoch: 62 loss:  1.2550013065338135\n",
      "tensor(1.2583, grad_fn=<DivBackward0>)\n",
      "tensor(1.2785, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 63\n",
      "Epoch: 63 loss:  1.2582522630691528\n",
      "tensor(1.2549, grad_fn=<DivBackward0>)\n",
      "tensor(1.2793, grad_fn=<DivBackward0>)\n",
      "Epoch: 64 loss:  1.2549443244934082\n",
      "tensor(1.2532, grad_fn=<DivBackward0>)\n",
      "tensor(1.2794, grad_fn=<DivBackward0>)\n",
      "Epoch: 65 loss:  1.2532488107681274\n",
      "tensor(1.2525, grad_fn=<DivBackward0>)\n",
      "tensor(1.2757, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 66\n",
      "Epoch: 66 loss:  1.25254487991333\n",
      "tensor(1.2525, grad_fn=<DivBackward0>)\n",
      "tensor(1.2789, grad_fn=<DivBackward0>)\n",
      "Epoch: 67 loss:  1.252475619316101\n",
      "tensor(1.2495, grad_fn=<DivBackward0>)\n",
      "tensor(1.2744, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 68\n",
      "Epoch: 68 loss:  1.2495108842849731\n",
      "tensor(1.2496, grad_fn=<DivBackward0>)\n",
      "tensor(1.2744, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 69\n",
      "Epoch: 69 loss:  1.2495903968811035\n",
      "tensor(1.2514, grad_fn=<DivBackward0>)\n",
      "tensor(1.2765, grad_fn=<DivBackward0>)\n",
      "Epoch: 70 loss:  1.2513748407363892\n",
      "tensor(1.2480, grad_fn=<DivBackward0>)\n",
      "tensor(1.2706, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 71\n",
      "Epoch: 71 loss:  1.2480472326278687\n",
      "tensor(1.2469, grad_fn=<DivBackward0>)\n",
      "tensor(1.2729, grad_fn=<DivBackward0>)\n",
      "Epoch: 72 loss:  1.246928334236145\n",
      "tensor(1.2483, grad_fn=<DivBackward0>)\n",
      "tensor(1.2775, grad_fn=<DivBackward0>)\n",
      "Epoch: 73 loss:  1.2483294010162354\n",
      "tensor(1.2462, grad_fn=<DivBackward0>)\n",
      "tensor(1.2696, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 74\n",
      "Epoch: 74 loss:  1.2462220191955566\n",
      "tensor(1.2451, grad_fn=<DivBackward0>)\n",
      "tensor(1.2723, grad_fn=<DivBackward0>)\n",
      "Epoch: 75 loss:  1.24514901638031\n",
      "tensor(1.2466, grad_fn=<DivBackward0>)\n",
      "tensor(1.2774, grad_fn=<DivBackward0>)\n",
      "Epoch: 76 loss:  1.2466099262237549\n",
      "tensor(1.2448, grad_fn=<DivBackward0>)\n",
      "tensor(1.2668, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 77\n",
      "Epoch: 77 loss:  1.244769811630249\n",
      "tensor(1.2440, grad_fn=<DivBackward0>)\n",
      "tensor(1.2703, grad_fn=<DivBackward0>)\n",
      "Epoch: 78 loss:  1.244042158126831\n",
      "tensor(1.2441, grad_fn=<DivBackward0>)\n",
      "tensor(1.2735, grad_fn=<DivBackward0>)\n",
      "Epoch: 79 loss:  1.2441350221633911\n",
      "tensor(1.2451, grad_fn=<DivBackward0>)\n",
      "tensor(1.2673, grad_fn=<DivBackward0>)\n",
      "Epoch: 80 loss:  1.245087742805481\n",
      "tensor(1.2417, grad_fn=<DivBackward0>)\n",
      "tensor(1.2694, grad_fn=<DivBackward0>)\n",
      "Epoch: 81 loss:  1.241710901260376\n",
      "tensor(1.2431, grad_fn=<DivBackward0>)\n",
      "tensor(1.2733, grad_fn=<DivBackward0>)\n",
      "Epoch: 82 loss:  1.2430826425552368\n",
      "tensor(1.2441, grad_fn=<DivBackward0>)\n",
      "tensor(1.2664, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 83\n",
      "Epoch: 83 loss:  1.2441232204437256\n",
      "tensor(1.2409, grad_fn=<DivBackward0>)\n",
      "tensor(1.2672, grad_fn=<DivBackward0>)\n",
      "Epoch: 84 loss:  1.2408534288406372\n",
      "tensor(1.2436, grad_fn=<DivBackward0>)\n",
      "tensor(1.2711, grad_fn=<DivBackward0>)\n",
      "Epoch: 85 loss:  1.2436450719833374\n",
      "tensor(1.2443, grad_fn=<DivBackward0>)\n",
      "tensor(1.2661, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 86\n",
      "Epoch: 86 loss:  1.2442710399627686\n",
      "tensor(1.2386, grad_fn=<DivBackward0>)\n",
      "tensor(1.2633, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 87\n",
      "Epoch: 87 loss:  1.2385753393173218\n",
      "tensor(1.2511, grad_fn=<DivBackward0>)\n",
      "tensor(1.2818, grad_fn=<DivBackward0>)\n",
      "Epoch: 88 loss:  1.2511180639266968\n",
      "tensor(1.2497, grad_fn=<DivBackward0>)\n",
      "tensor(1.2699, grad_fn=<DivBackward0>)\n",
      "Epoch: 89 loss:  1.249700665473938\n",
      "tensor(1.2442, grad_fn=<DivBackward0>)\n",
      "tensor(1.2650, grad_fn=<DivBackward0>)\n",
      "Epoch: 90 loss:  1.2441761493682861\n",
      "tensor(1.2513, grad_fn=<DivBackward0>)\n",
      "tensor(1.2825, grad_fn=<DivBackward0>)\n",
      "Epoch: 91 loss:  1.2513083219528198\n",
      "tensor(1.2370, grad_fn=<DivBackward0>)\n",
      "tensor(1.2645, grad_fn=<DivBackward0>)\n",
      "Epoch: 92 loss:  1.237037181854248\n",
      "tensor(1.2440, grad_fn=<DivBackward0>)\n",
      "tensor(1.2679, grad_fn=<DivBackward0>)\n",
      "Epoch: 93 loss:  1.2440464496612549\n",
      "tensor(1.2378, grad_fn=<DivBackward0>)\n",
      "tensor(1.2624, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 94\n",
      "Epoch: 94 loss:  1.2377865314483643\n",
      "tensor(1.2457, grad_fn=<DivBackward0>)\n",
      "tensor(1.2870, grad_fn=<DivBackward0>)\n",
      "Epoch: 95 loss:  1.245710015296936\n",
      "tensor(1.2367, grad_fn=<DivBackward0>)\n",
      "tensor(1.2648, grad_fn=<DivBackward0>)\n",
      "Epoch: 96 loss:  1.2367068529129028\n",
      "tensor(1.2375, grad_fn=<DivBackward0>)\n",
      "tensor(1.2625, grad_fn=<DivBackward0>)\n",
      "Epoch: 97 loss:  1.2375301122665405\n",
      "tensor(1.2354, grad_fn=<DivBackward0>)\n",
      "tensor(1.2658, grad_fn=<DivBackward0>)\n",
      "Epoch: 98 loss:  1.2354282140731812\n",
      "tensor(1.2409, grad_fn=<DivBackward0>)\n",
      "tensor(1.2756, grad_fn=<DivBackward0>)\n",
      "Epoch: 99 loss:  1.240945816040039\n",
      "[0.99705404 0.9997813  0.99992543 ... 0.9300695  0.82818407 0.78282917]\n",
      "[0.9998543  0.99705374 0.9964817  ... 0.9504388  0.8624404  0.8143821 ]\n",
      "[0.99872893 0.9953992  0.99465775 ... 0.96078736 0.8797986  0.8359455 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dict = dict()\n",
    "\n",
    "dir_path = \"/content/gdrive/MyDrive/GDS/walks/noextrawalks\"\n",
    "hidden_channels = [128, 16]\n",
    "encoding_dim = 10\n",
    "\n",
    "seed_pages_used = [    \n",
    "    '/find-a-job',\n",
    "    '/universal-credit',\n",
    "    '/government/collections/financial-support-for-businesses-during-coronavirus-covid-19']\n",
    "\n",
    "\n",
    "walk_length_list = [5, 10]\n",
    "walk_params_list = [[2, 2, 2, 2],\n",
    "                [5, 10, 1, 1],\n",
    "               [5, 10, 5, 10],\n",
    "               [10, 10, 1 ,1],\n",
    "               [1, 1, 1, 1]]\n",
    "\n",
    "# walk_length_list = [10, 3]\n",
    "# walk_params_list = [[5, 10, 1, 1],\n",
    "#                [5, 10, 5, 10],\n",
    "#                [10, 10, 1 ,1],\n",
    "#                [1, 1, 1, 1]]\n",
    "\n",
    "for walk_length in walk_length_list:\n",
    "\n",
    "  for walk_params in walk_params_list:\n",
    "\n",
    "    name_extension = \"wl_\" + str(walk_length)\n",
    "    for x in walk_params:\n",
    "      name_extension += \"_\" + str(x)\n",
    "\n",
    "    pyg_graph = load_pickle_file(\"pyg_graph_\" + name_extension, dir_path=dir_path)\n",
    "    # dump_pickle_file(failed_nodes, 'failed_nodes', dir_path=\"/content/gdrive/MyDrive/GDS/pickles/\")\n",
    "    neigh_matrix = load_pickle_file(\"neigh_matrix_\" + name_extension, dir_path=dir_path)\n",
    "    negative_array = load_pickle_file(\"negative_array_\" + name_extension, dir_path=dir_path)\n",
    "\n",
    "    print(name_extension)\n",
    "\n",
    "    model = GCN_edgeweight(hidden_channels, encoding_dim)\n",
    "    model_name = \"/model_\" + name_extension\n",
    "    print(model)\n",
    "    criterion = MSELoss3()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # Define optimizer.\n",
    "\n",
    "    history = train(model, pyg_graph, neigh_matrix, negative_array, num_epochs = 100, \n",
    "                    dir_path = \"/content/gdrive/MyDrive/GDS/walks/noextrawalks/models\", model_name = model_name) \n",
    "\n",
    "\n",
    "    # model = GCN_edgeweight(hidden_channels, encoding_dim)\n",
    "    val_loss, scores_df_rankings, score = scores(model, pyg_graph, neigh_matrix, negative_array, seed_pages_used, edge_weight = True)\n",
    "    \n",
    "    results_dict[model_name] = dict()\n",
    "    results_dict[model_name][\"loss\"] = val_loss\n",
    "    results_dict[model_name][\"score\"] = score\n",
    "    results_dict[model_name][\"ranking\"] = scores_df_rankings  \n",
    "\n",
    "dump_pickle_file(results_dict, \"results_dict\", dir_path = \"/content/gdrive/MyDrive/GDS/walks/noextrawalks/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XVQ6nbGX1Jmt"
   },
   "outputs": [],
   "source": [
    "results_dict = load_pickle_file(\"results_dict\", dir_path = \"/content/gdrive/MyDrive/GDS/walks/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T2uBDDAf1V5n",
    "outputId": "a1dfb5b5-7795-4932-c47a-df3016587971"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/model_wl_5_5_10_1_1\n",
      "0.0009262080202366932\n",
      "/model_wl_5_5_10_5_10\n",
      "0.021190765317603357\n",
      "/model_wl_5_10_10_1_1\n",
      "0.011980431581707736\n",
      "/model_wl_5_1_1_1_1\n",
      "0.04681616606615298\n"
     ]
    }
   ],
   "source": [
    "for key in results_dict:\n",
    "  print(key)\n",
    "  print(results_dict[key][\"score\"])\n",
    "  results_dict[key][\"ranking\"][\"page\"].iloc[:50].to_csv(\"/content/gdrive/MyDrive/GDS/walks/models\" + str(key) + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vo7Ul9vt3Hr5",
    "outputId": "191812ed-2b8d-4bd8-abe2-1f1adea23e51"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03412083478744377"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dict[key][\"score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TOG-V2Vx1kGz",
    "outputId": "8c921d93-e011-40b1-c064-835b9cd38b21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                           /find-a-job\n",
       "1                                     /universal-credit\n",
       "2     /government/collections/financial-support-for-...\n",
       "3                                 /apply-apprenticeship\n",
       "4                                      /browse/benefits\n",
       "5                              /apply-to-come-to-the-uk\n",
       "6                             /browse/visas-immigration\n",
       "7                                  /browse/disabilities\n",
       "8                                 /jobseekers-allowance\n",
       "9                             /sign-in-universal-credit\n",
       "10                                      /browse/working\n",
       "11                                  /browse/citizenship\n",
       "12                   /check-job-applicant-right-to-work\n",
       "13                        /browse/working/state-pension\n",
       "14    /government/organisations/disclosure-and-barri...\n",
       "15                        /request-copy-criminal-record\n",
       "16                             /contact-pension-service\n",
       "17                 /settled-status-eu-citizens-families\n",
       "18                       /view-prove-immigration-status\n",
       "19                                      /access-to-work\n",
       "Name: page, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dict[\"/model_wl_3_1_1_1_1\"][\"ranking\"][\"page\"].iloc[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZM3Bgjf2XGqI",
    "outputId": "e8aa8a4f-8854-448e-9259-0a34da3ec582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wl_5_2_2_2_2\n",
      "GCN_edgeweight(\n",
      "  (conv1): GCNConv(6206, 128)\n",
      "  (conv2): GCNConv(128, 16)\n",
      "  (linear): Linear(in_features=16, out_features=10, bias=True)\n",
      ")\n",
      "tensor(1.4009, grad_fn=<DivBackward0>)\n",
      "tensor(1.4010, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 0\n",
      "Epoch: 0 loss:  1.4009419679641724\n",
      "tensor(1.3928, grad_fn=<DivBackward0>)\n",
      "tensor(1.3929, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 1\n",
      "Epoch: 1 loss:  1.392831802368164\n",
      "tensor(1.3797, grad_fn=<DivBackward0>)\n",
      "tensor(1.3798, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 2\n",
      "Epoch: 2 loss:  1.3796508312225342\n",
      "tensor(1.3613, grad_fn=<DivBackward0>)\n",
      "tensor(1.3616, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 3\n",
      "Epoch: 3 loss:  1.3613295555114746\n",
      "tensor(1.3428, grad_fn=<DivBackward0>)\n",
      "tensor(1.3428, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 4\n",
      "Epoch: 4 loss:  1.3427547216415405\n",
      "tensor(1.3338, grad_fn=<DivBackward0>)\n",
      "tensor(1.3334, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 5\n",
      "Epoch: 5 loss:  1.3338338136672974\n",
      "tensor(1.3160, grad_fn=<DivBackward0>)\n",
      "tensor(1.3162, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 6\n",
      "Epoch: 6 loss:  1.3159656524658203\n",
      "tensor(1.3051, grad_fn=<DivBackward0>)\n",
      "tensor(1.3062, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 7\n",
      "Epoch: 7 loss:  1.3050506114959717\n",
      "tensor(1.2993, grad_fn=<DivBackward0>)\n",
      "tensor(1.3011, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 8\n",
      "Epoch: 8 loss:  1.2993457317352295\n",
      "tensor(1.2918, grad_fn=<DivBackward0>)\n",
      "tensor(1.2939, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 9\n",
      "Epoch: 9 loss:  1.291761875152588\n",
      "tensor(1.2881, grad_fn=<DivBackward0>)\n",
      "tensor(1.2914, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 10\n",
      "Epoch: 10 loss:  1.288112759590149\n",
      "tensor(1.2829, grad_fn=<DivBackward0>)\n",
      "tensor(1.2880, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 11\n",
      "Epoch: 11 loss:  1.2829416990280151\n",
      "tensor(1.2773, grad_fn=<DivBackward0>)\n",
      "tensor(1.2860, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 12\n",
      "Epoch: 12 loss:  1.277292013168335\n",
      "tensor(1.2749, grad_fn=<DivBackward0>)\n",
      "tensor(1.2871, grad_fn=<DivBackward0>)\n",
      "Epoch: 13 loss:  1.2749428749084473\n",
      "tensor(1.2712, grad_fn=<DivBackward0>)\n",
      "tensor(1.2879, grad_fn=<DivBackward0>)\n",
      "Epoch: 14 loss:  1.2712010145187378\n",
      "tensor(1.2684, grad_fn=<DivBackward0>)\n",
      "tensor(1.2902, grad_fn=<DivBackward0>)\n",
      "Epoch: 15 loss:  1.2684351205825806\n",
      "tensor(1.2648, grad_fn=<DivBackward0>)\n",
      "tensor(1.2897, grad_fn=<DivBackward0>)\n",
      "Epoch: 16 loss:  1.264829158782959\n",
      "tensor(1.2612, grad_fn=<DivBackward0>)\n",
      "tensor(1.2894, grad_fn=<DivBackward0>)\n",
      "Epoch: 17 loss:  1.261232852935791\n",
      "tensor(1.2583, grad_fn=<DivBackward0>)\n",
      "tensor(1.2888, grad_fn=<DivBackward0>)\n",
      "Epoch: 18 loss:  1.258306622505188\n",
      "tensor(1.2556, grad_fn=<DivBackward0>)\n",
      "tensor(1.2892, grad_fn=<DivBackward0>)\n",
      "Epoch: 19 loss:  1.2556217908859253\n",
      "tensor(1.2537, grad_fn=<DivBackward0>)\n",
      "tensor(1.2899, grad_fn=<DivBackward0>)\n",
      "Epoch: 20 loss:  1.2536884546279907\n",
      "tensor(1.2508, grad_fn=<DivBackward0>)\n",
      "tensor(1.2895, grad_fn=<DivBackward0>)\n",
      "Epoch: 21 loss:  1.250778079032898\n",
      "tensor(1.2488, grad_fn=<DivBackward0>)\n",
      "tensor(1.2920, grad_fn=<DivBackward0>)\n",
      "Epoch: 22 loss:  1.2487668991088867\n",
      "tensor(1.2460, grad_fn=<DivBackward0>)\n",
      "tensor(1.2917, grad_fn=<DivBackward0>)\n",
      "Epoch: 23 loss:  1.2459715604782104\n",
      "tensor(1.2435, grad_fn=<DivBackward0>)\n",
      "tensor(1.2936, grad_fn=<DivBackward0>)\n",
      "Epoch: 24 loss:  1.243472695350647\n",
      "tensor(1.2405, grad_fn=<DivBackward0>)\n",
      "tensor(1.2953, grad_fn=<DivBackward0>)\n",
      "Epoch: 25 loss:  1.2404536008834839\n",
      "tensor(1.2384, grad_fn=<DivBackward0>)\n",
      "tensor(1.2993, grad_fn=<DivBackward0>)\n",
      "Epoch: 26 loss:  1.2383543252944946\n",
      "tensor(1.2356, grad_fn=<DivBackward0>)\n",
      "tensor(1.3010, grad_fn=<DivBackward0>)\n",
      "Epoch: 27 loss:  1.235607624053955\n",
      "tensor(1.2326, grad_fn=<DivBackward0>)\n",
      "tensor(1.3040, grad_fn=<DivBackward0>)\n",
      "Epoch: 28 loss:  1.2326099872589111\n",
      "tensor(1.2293, grad_fn=<DivBackward0>)\n",
      "tensor(1.3050, grad_fn=<DivBackward0>)\n",
      "Epoch: 29 loss:  1.2293232679367065\n",
      "tensor(1.2263, grad_fn=<DivBackward0>)\n",
      "tensor(1.3088, grad_fn=<DivBackward0>)\n",
      "Epoch: 30 loss:  1.226282000541687\n",
      "tensor(1.2232, grad_fn=<DivBackward0>)\n",
      "tensor(1.3117, grad_fn=<DivBackward0>)\n",
      "Epoch: 31 loss:  1.2232398986816406\n",
      "tensor(1.2202, grad_fn=<DivBackward0>)\n",
      "tensor(1.3169, grad_fn=<DivBackward0>)\n",
      "Early stopping!\n",
      "[ 0.98365825  0.99244773  0.9995747  ...  0.9153196   0.66095793\n",
      " -0.00968942]\n",
      "[ 0.9939264   0.98591095  0.9647443  ...  0.8953651   0.45636624\n",
      " -0.2867941 ]\n",
      "[ 0.97138673  0.96685994  0.9517605  ...  0.7928698   0.56733465\n",
      " -0.25028068]\n",
      "wl_5_5_10_1_1\n",
      "GCN_edgeweight(\n",
      "  (conv1): GCNConv(6206, 128)\n",
      "  (conv2): GCNConv(128, 16)\n",
      "  (linear): Linear(in_features=16, out_features=10, bias=True)\n",
      ")\n",
      "tensor(1.4009, grad_fn=<DivBackward0>)\n",
      "tensor(1.4010, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 0\n",
      "Epoch: 0 loss:  1.4009490013122559\n",
      "tensor(1.3926, grad_fn=<DivBackward0>)\n",
      "tensor(1.3925, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 1\n",
      "Epoch: 1 loss:  1.392615556716919\n",
      "tensor(1.3788, grad_fn=<DivBackward0>)\n",
      "tensor(1.3782, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 2\n",
      "Epoch: 2 loss:  1.3787792921066284\n",
      "tensor(1.3596, grad_fn=<DivBackward0>)\n",
      "tensor(1.3583, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 3\n",
      "Epoch: 3 loss:  1.3596209287643433\n",
      "tensor(1.3406, grad_fn=<DivBackward0>)\n",
      "tensor(1.3380, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 4\n",
      "Epoch: 4 loss:  1.3406105041503906\n",
      "tensor(1.3320, grad_fn=<DivBackward0>)\n",
      "tensor(1.3276, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 5\n",
      "Epoch: 5 loss:  1.331970453262329\n",
      "tensor(1.3136, grad_fn=<DivBackward0>)\n",
      "tensor(1.3098, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 6\n",
      "Epoch: 6 loss:  1.313552737236023\n",
      "tensor(1.3019, grad_fn=<DivBackward0>)\n",
      "tensor(1.2991, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 7\n",
      "Epoch: 7 loss:  1.3019214868545532\n",
      "tensor(1.2964, grad_fn=<DivBackward0>)\n",
      "tensor(1.2941, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 8\n",
      "Epoch: 8 loss:  1.2963685989379883\n",
      "tensor(1.2885, grad_fn=<DivBackward0>)\n",
      "tensor(1.2864, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 9\n",
      "Epoch: 9 loss:  1.2884891033172607\n",
      "tensor(1.2845, grad_fn=<DivBackward0>)\n",
      "tensor(1.2823, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 10\n",
      "Epoch: 10 loss:  1.2845228910446167\n",
      "tensor(1.2801, grad_fn=<DivBackward0>)\n",
      "tensor(1.2792, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 11\n",
      "Epoch: 11 loss:  1.280133843421936\n",
      "tensor(1.2737, grad_fn=<DivBackward0>)\n",
      "tensor(1.2760, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 12\n",
      "Epoch: 12 loss:  1.2737343311309814\n",
      "tensor(1.2713, grad_fn=<DivBackward0>)\n",
      "tensor(1.2769, grad_fn=<DivBackward0>)\n",
      "Epoch: 13 loss:  1.2712610960006714\n",
      "tensor(1.2678, grad_fn=<DivBackward0>)\n",
      "tensor(1.2770, grad_fn=<DivBackward0>)\n",
      "Epoch: 14 loss:  1.267846941947937\n",
      "tensor(1.2645, grad_fn=<DivBackward0>)\n",
      "tensor(1.2775, grad_fn=<DivBackward0>)\n",
      "Epoch: 15 loss:  1.2645355463027954\n",
      "tensor(1.2617, grad_fn=<DivBackward0>)\n",
      "tensor(1.2792, grad_fn=<DivBackward0>)\n",
      "Epoch: 16 loss:  1.2617043256759644\n",
      "tensor(1.2574, grad_fn=<DivBackward0>)\n",
      "tensor(1.2784, grad_fn=<DivBackward0>)\n",
      "Epoch: 17 loss:  1.2574067115783691\n",
      "tensor(1.2545, grad_fn=<DivBackward0>)\n",
      "tensor(1.2783, grad_fn=<DivBackward0>)\n",
      "Epoch: 18 loss:  1.2545115947723389\n",
      "tensor(1.2514, grad_fn=<DivBackward0>)\n",
      "tensor(1.2788, grad_fn=<DivBackward0>)\n",
      "Epoch: 19 loss:  1.2513772249221802\n",
      "tensor(1.2494, grad_fn=<DivBackward0>)\n",
      "tensor(1.2794, grad_fn=<DivBackward0>)\n",
      "Epoch: 20 loss:  1.2493654489517212\n",
      "tensor(1.2469, grad_fn=<DivBackward0>)\n",
      "tensor(1.2800, grad_fn=<DivBackward0>)\n",
      "Epoch: 21 loss:  1.24687922000885\n",
      "tensor(1.2441, grad_fn=<DivBackward0>)\n",
      "tensor(1.2821, grad_fn=<DivBackward0>)\n",
      "Epoch: 22 loss:  1.2441023588180542\n",
      "tensor(1.2418, grad_fn=<DivBackward0>)\n",
      "tensor(1.2825, grad_fn=<DivBackward0>)\n",
      "Epoch: 23 loss:  1.2418174743652344\n",
      "tensor(1.2387, grad_fn=<DivBackward0>)\n",
      "tensor(1.2835, grad_fn=<DivBackward0>)\n",
      "Epoch: 24 loss:  1.2387274503707886\n",
      "tensor(1.2362, grad_fn=<DivBackward0>)\n",
      "tensor(1.2854, grad_fn=<DivBackward0>)\n",
      "Epoch: 25 loss:  1.236209511756897\n",
      "tensor(1.2334, grad_fn=<DivBackward0>)\n",
      "tensor(1.2893, grad_fn=<DivBackward0>)\n",
      "Epoch: 26 loss:  1.2333672046661377\n",
      "tensor(1.2309, grad_fn=<DivBackward0>)\n",
      "tensor(1.2922, grad_fn=<DivBackward0>)\n",
      "Epoch: 27 loss:  1.2308794260025024\n",
      "tensor(1.2273, grad_fn=<DivBackward0>)\n",
      "tensor(1.2937, grad_fn=<DivBackward0>)\n",
      "Epoch: 28 loss:  1.2273321151733398\n",
      "tensor(1.2245, grad_fn=<DivBackward0>)\n",
      "tensor(1.2990, grad_fn=<DivBackward0>)\n",
      "Epoch: 29 loss:  1.224489688873291\n",
      "tensor(1.2206, grad_fn=<DivBackward0>)\n",
      "tensor(1.2996, grad_fn=<DivBackward0>)\n",
      "Epoch: 30 loss:  1.2205928564071655\n",
      "tensor(1.2180, grad_fn=<DivBackward0>)\n",
      "tensor(1.3040, grad_fn=<DivBackward0>)\n",
      "Epoch: 31 loss:  1.217968463897705\n",
      "tensor(1.2142, grad_fn=<DivBackward0>)\n",
      "tensor(1.3072, grad_fn=<DivBackward0>)\n",
      "Early stopping!\n",
      "[ 0.9820576  0.9929271  0.9977104 ...  0.8542547 -0.858114   0.8561619]\n",
      "[ 0.99992335  0.99755603  0.992572   ...  0.7866365  -0.7611756\n",
      "  0.9364406 ]\n",
      "[ 0.9297112   0.93979645  0.9423015  ...  0.68483025 -0.8431665\n",
      "  0.8207492 ]\n",
      "wl_5_5_10_5_10\n",
      "GCN_edgeweight(\n",
      "  (conv1): GCNConv(6206, 128)\n",
      "  (conv2): GCNConv(128, 16)\n",
      "  (linear): Linear(in_features=16, out_features=10, bias=True)\n",
      ")\n",
      "tensor(1.4010, grad_fn=<DivBackward0>)\n",
      "tensor(1.4010, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 0\n",
      "Epoch: 0 loss:  1.4009522199630737\n",
      "tensor(1.3926, grad_fn=<DivBackward0>)\n",
      "tensor(1.3926, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 1\n",
      "Epoch: 1 loss:  1.3925820589065552\n",
      "tensor(1.3788, grad_fn=<DivBackward0>)\n",
      "tensor(1.3789, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 2\n",
      "Epoch: 2 loss:  1.3787894248962402\n",
      "tensor(1.3595, grad_fn=<DivBackward0>)\n",
      "tensor(1.3598, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 3\n",
      "Epoch: 3 loss:  1.3595188856124878\n",
      "tensor(1.3404, grad_fn=<DivBackward0>)\n",
      "tensor(1.3417, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 4\n",
      "Epoch: 4 loss:  1.340364694595337\n",
      "tensor(1.3319, grad_fn=<DivBackward0>)\n",
      "tensor(1.3356, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 5\n",
      "Epoch: 5 loss:  1.3319344520568848\n",
      "tensor(1.3137, grad_fn=<DivBackward0>)\n",
      "tensor(1.3176, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 6\n",
      "Epoch: 6 loss:  1.3136825561523438\n",
      "tensor(1.3014, grad_fn=<DivBackward0>)\n",
      "tensor(1.3046, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 7\n",
      "Epoch: 7 loss:  1.3014479875564575\n",
      "tensor(1.2959, grad_fn=<DivBackward0>)\n",
      "tensor(1.2989, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 8\n",
      "Epoch: 8 loss:  1.2958767414093018\n",
      "tensor(1.2881, grad_fn=<DivBackward0>)\n",
      "tensor(1.2929, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 9\n",
      "Epoch: 9 loss:  1.2880929708480835\n",
      "tensor(1.2840, grad_fn=<DivBackward0>)\n",
      "tensor(1.2919, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 10\n",
      "Epoch: 10 loss:  1.2840380668640137\n",
      "tensor(1.2798, grad_fn=<DivBackward0>)\n",
      "tensor(1.2904, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 11\n",
      "Epoch: 11 loss:  1.2797712087631226\n",
      "tensor(1.2729, grad_fn=<DivBackward0>)\n",
      "tensor(1.2859, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 12\n",
      "Epoch: 12 loss:  1.2729192972183228\n",
      "tensor(1.2706, grad_fn=<DivBackward0>)\n",
      "tensor(1.2849, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 13\n",
      "Epoch: 13 loss:  1.2706327438354492\n",
      "tensor(1.2676, grad_fn=<DivBackward0>)\n",
      "tensor(1.2856, grad_fn=<DivBackward0>)\n",
      "Epoch: 14 loss:  1.2675508260726929\n",
      "tensor(1.2642, grad_fn=<DivBackward0>)\n",
      "tensor(1.2894, grad_fn=<DivBackward0>)\n",
      "Epoch: 15 loss:  1.2641868591308594\n",
      "tensor(1.2616, grad_fn=<DivBackward0>)\n",
      "tensor(1.2919, grad_fn=<DivBackward0>)\n",
      "Epoch: 16 loss:  1.2615537643432617\n",
      "tensor(1.2569, grad_fn=<DivBackward0>)\n",
      "tensor(1.2897, grad_fn=<DivBackward0>)\n",
      "Epoch: 17 loss:  1.2568646669387817\n",
      "tensor(1.2539, grad_fn=<DivBackward0>)\n",
      "tensor(1.2890, grad_fn=<DivBackward0>)\n",
      "Epoch: 18 loss:  1.2539092302322388\n",
      "tensor(1.2511, grad_fn=<DivBackward0>)\n",
      "tensor(1.2888, grad_fn=<DivBackward0>)\n",
      "Epoch: 19 loss:  1.25111722946167\n",
      "tensor(1.2487, grad_fn=<DivBackward0>)\n",
      "tensor(1.2897, grad_fn=<DivBackward0>)\n",
      "Epoch: 20 loss:  1.248727798461914\n",
      "tensor(1.2467, grad_fn=<DivBackward0>)\n",
      "tensor(1.2919, grad_fn=<DivBackward0>)\n",
      "Epoch: 21 loss:  1.2466799020767212\n",
      "tensor(1.2436, grad_fn=<DivBackward0>)\n",
      "tensor(1.2908, grad_fn=<DivBackward0>)\n",
      "Epoch: 22 loss:  1.2435725927352905\n",
      "tensor(1.2414, grad_fn=<DivBackward0>)\n",
      "tensor(1.2933, grad_fn=<DivBackward0>)\n",
      "Epoch: 23 loss:  1.241361141204834\n",
      "tensor(1.2385, grad_fn=<DivBackward0>)\n",
      "tensor(1.2949, grad_fn=<DivBackward0>)\n",
      "Epoch: 24 loss:  1.2384756803512573\n",
      "tensor(1.2360, grad_fn=<DivBackward0>)\n",
      "tensor(1.2998, grad_fn=<DivBackward0>)\n",
      "Epoch: 25 loss:  1.235965609550476\n",
      "tensor(1.2331, grad_fn=<DivBackward0>)\n",
      "tensor(1.3013, grad_fn=<DivBackward0>)\n",
      "Epoch: 26 loss:  1.2330763339996338\n",
      "tensor(1.2305, grad_fn=<DivBackward0>)\n",
      "tensor(1.3035, grad_fn=<DivBackward0>)\n",
      "Epoch: 27 loss:  1.2305080890655518\n",
      "tensor(1.2272, grad_fn=<DivBackward0>)\n",
      "tensor(1.3074, grad_fn=<DivBackward0>)\n",
      "Epoch: 28 loss:  1.2271922826766968\n",
      "tensor(1.2241, grad_fn=<DivBackward0>)\n",
      "tensor(1.3132, grad_fn=<DivBackward0>)\n",
      "Epoch: 29 loss:  1.224136471748352\n",
      "tensor(1.2204, grad_fn=<DivBackward0>)\n",
      "tensor(1.3149, grad_fn=<DivBackward0>)\n",
      "Epoch: 30 loss:  1.2204082012176514\n",
      "tensor(1.2179, grad_fn=<DivBackward0>)\n",
      "tensor(1.3162, grad_fn=<DivBackward0>)\n",
      "Epoch: 31 loss:  1.2179275751113892\n",
      "tensor(1.2138, grad_fn=<DivBackward0>)\n",
      "tensor(1.3230, grad_fn=<DivBackward0>)\n",
      "Epoch: 32 loss:  1.2137680053710938\n",
      "tensor(1.2118, grad_fn=<DivBackward0>)\n",
      "tensor(1.3304, grad_fn=<DivBackward0>)\n",
      "Early stopping!\n",
      "[ 0.9773894   0.99073327  0.9976487  ...  0.96688545  0.48160547\n",
      " -0.7386561 ]\n",
      "[ 0.9992693   0.9940909   0.9842957  ...  0.94240963  0.28239763\n",
      " -0.80112994]\n",
      "[ 0.97842187  0.9867451   0.9876159  ...  0.93587106  0.4982172\n",
      " -0.8250673 ]\n",
      "wl_5_10_10_1_1\n",
      "GCN_edgeweight(\n",
      "  (conv1): GCNConv(6206, 128)\n",
      "  (conv2): GCNConv(128, 16)\n",
      "  (linear): Linear(in_features=16, out_features=10, bias=True)\n",
      ")\n",
      "tensor(1.4009, grad_fn=<DivBackward0>)\n",
      "tensor(1.4010, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 0\n",
      "Epoch: 0 loss:  1.4009476900100708\n",
      "tensor(1.3926, grad_fn=<DivBackward0>)\n",
      "tensor(1.3928, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 1\n",
      "Epoch: 1 loss:  1.392624855041504\n",
      "tensor(1.3788, grad_fn=<DivBackward0>)\n",
      "tensor(1.3796, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 2\n",
      "Epoch: 2 loss:  1.3788388967514038\n",
      "tensor(1.3597, grad_fn=<DivBackward0>)\n",
      "tensor(1.3614, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 3\n",
      "Epoch: 3 loss:  1.35968017578125\n",
      "tensor(1.3405, grad_fn=<DivBackward0>)\n",
      "tensor(1.3431, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 4\n",
      "Epoch: 4 loss:  1.3405044078826904\n",
      "tensor(1.3321, grad_fn=<DivBackward0>)\n",
      "tensor(1.3351, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 5\n",
      "Epoch: 5 loss:  1.3320969343185425\n",
      "tensor(1.3135, grad_fn=<DivBackward0>)\n",
      "tensor(1.3177, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 6\n",
      "Epoch: 6 loss:  1.3135404586791992\n",
      "tensor(1.3018, grad_fn=<DivBackward0>)\n",
      "tensor(1.3061, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 7\n",
      "Epoch: 7 loss:  1.3018114566802979\n",
      "tensor(1.2963, grad_fn=<DivBackward0>)\n",
      "tensor(1.3008, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 8\n",
      "Epoch: 8 loss:  1.2963312864303589\n",
      "tensor(1.2884, grad_fn=<DivBackward0>)\n",
      "tensor(1.2933, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 9\n",
      "Epoch: 9 loss:  1.2883907556533813\n",
      "tensor(1.2843, grad_fn=<DivBackward0>)\n",
      "tensor(1.2905, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 10\n",
      "Epoch: 10 loss:  1.2842531204223633\n",
      "tensor(1.2799, grad_fn=<DivBackward0>)\n",
      "tensor(1.2883, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 11\n",
      "Epoch: 11 loss:  1.2798885107040405\n",
      "tensor(1.2734, grad_fn=<DivBackward0>)\n",
      "tensor(1.2840, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 12\n",
      "Epoch: 12 loss:  1.2734254598617554\n",
      "tensor(1.2707, grad_fn=<DivBackward0>)\n",
      "tensor(1.2845, grad_fn=<DivBackward0>)\n",
      "Epoch: 13 loss:  1.2707198858261108\n",
      "tensor(1.2676, grad_fn=<DivBackward0>)\n",
      "tensor(1.2841, grad_fn=<DivBackward0>)\n",
      "Epoch: 14 loss:  1.267555594444275\n",
      "tensor(1.2642, grad_fn=<DivBackward0>)\n",
      "tensor(1.2870, grad_fn=<DivBackward0>)\n",
      "Epoch: 15 loss:  1.2641791105270386\n",
      "tensor(1.2616, grad_fn=<DivBackward0>)\n",
      "tensor(1.2882, grad_fn=<DivBackward0>)\n",
      "Epoch: 16 loss:  1.261606216430664\n",
      "tensor(1.2570, grad_fn=<DivBackward0>)\n",
      "tensor(1.2880, grad_fn=<DivBackward0>)\n",
      "Epoch: 17 loss:  1.2569999694824219\n",
      "tensor(1.2541, grad_fn=<DivBackward0>)\n",
      "tensor(1.2872, grad_fn=<DivBackward0>)\n",
      "Epoch: 18 loss:  1.2541240453720093\n",
      "tensor(1.2513, grad_fn=<DivBackward0>)\n",
      "tensor(1.2867, grad_fn=<DivBackward0>)\n",
      "Epoch: 19 loss:  1.251292109489441\n",
      "tensor(1.2492, grad_fn=<DivBackward0>)\n",
      "tensor(1.2885, grad_fn=<DivBackward0>)\n",
      "Epoch: 20 loss:  1.2491854429244995\n",
      "tensor(1.2468, grad_fn=<DivBackward0>)\n",
      "tensor(1.2886, grad_fn=<DivBackward0>)\n",
      "Epoch: 21 loss:  1.246781826019287\n",
      "tensor(1.2442, grad_fn=<DivBackward0>)\n",
      "tensor(1.2894, grad_fn=<DivBackward0>)\n",
      "Epoch: 22 loss:  1.2441751956939697\n",
      "tensor(1.2418, grad_fn=<DivBackward0>)\n",
      "tensor(1.2923, grad_fn=<DivBackward0>)\n",
      "Epoch: 23 loss:  1.2418144941329956\n",
      "tensor(1.2388, grad_fn=<DivBackward0>)\n",
      "tensor(1.2938, grad_fn=<DivBackward0>)\n",
      "Epoch: 24 loss:  1.23878014087677\n",
      "tensor(1.2366, grad_fn=<DivBackward0>)\n",
      "tensor(1.2952, grad_fn=<DivBackward0>)\n",
      "Epoch: 25 loss:  1.2366467714309692\n",
      "tensor(1.2338, grad_fn=<DivBackward0>)\n",
      "tensor(1.2990, grad_fn=<DivBackward0>)\n",
      "Epoch: 26 loss:  1.2337955236434937\n",
      "tensor(1.2314, grad_fn=<DivBackward0>)\n",
      "tensor(1.3003, grad_fn=<DivBackward0>)\n",
      "Epoch: 27 loss:  1.2313929796218872\n",
      "tensor(1.2281, grad_fn=<DivBackward0>)\n",
      "tensor(1.3026, grad_fn=<DivBackward0>)\n",
      "Epoch: 28 loss:  1.2280802726745605\n",
      "tensor(1.2256, grad_fn=<DivBackward0>)\n",
      "tensor(1.3049, grad_fn=<DivBackward0>)\n",
      "Epoch: 29 loss:  1.2256383895874023\n",
      "tensor(1.2216, grad_fn=<DivBackward0>)\n",
      "tensor(1.3070, grad_fn=<DivBackward0>)\n",
      "Epoch: 30 loss:  1.2215718030929565\n",
      "tensor(1.2190, grad_fn=<DivBackward0>)\n",
      "tensor(1.3092, grad_fn=<DivBackward0>)\n",
      "Epoch: 31 loss:  1.2190089225769043\n",
      "tensor(1.2154, grad_fn=<DivBackward0>)\n",
      "tensor(1.3122, grad_fn=<DivBackward0>)\n",
      "Early stopping!\n",
      "[0.9785281  0.9957948  0.9994297  ... 0.3907505  0.23875603 0.84445167]\n",
      "[0.99978757 0.99118346 0.9813069  ... 0.26864168 0.02834746 0.7142464 ]\n",
      "[0.93669605 0.96386373 0.97127354 ... 0.27285016 0.34427795 0.8867874 ]\n",
      "wl_5_1_1_1_1\n",
      "GCN_edgeweight(\n",
      "  (conv1): GCNConv(6206, 128)\n",
      "  (conv2): GCNConv(128, 16)\n",
      "  (linear): Linear(in_features=16, out_features=10, bias=True)\n",
      ")\n",
      "tensor(1.4009, grad_fn=<DivBackward0>)\n",
      "tensor(1.4009, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 0\n",
      "Epoch: 0 loss:  1.400943636894226\n",
      "tensor(1.3929, grad_fn=<DivBackward0>)\n",
      "tensor(1.3931, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 1\n",
      "Epoch: 1 loss:  1.3929303884506226\n",
      "tensor(1.3800, grad_fn=<DivBackward0>)\n",
      "tensor(1.3807, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 2\n",
      "Epoch: 2 loss:  1.379965901374817\n",
      "tensor(1.3620, grad_fn=<DivBackward0>)\n",
      "tensor(1.3636, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 3\n",
      "Epoch: 3 loss:  1.3619985580444336\n",
      "tensor(1.3436, grad_fn=<DivBackward0>)\n",
      "tensor(1.3461, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 4\n",
      "Epoch: 4 loss:  1.3435968160629272\n",
      "tensor(1.3343, grad_fn=<DivBackward0>)\n",
      "tensor(1.3375, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 5\n",
      "Epoch: 5 loss:  1.3343183994293213\n",
      "tensor(1.3170, grad_fn=<DivBackward0>)\n",
      "tensor(1.3207, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 6\n",
      "Epoch: 6 loss:  1.3169913291931152\n",
      "tensor(1.3063, grad_fn=<DivBackward0>)\n",
      "tensor(1.3104, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 7\n",
      "Epoch: 7 loss:  1.3063082695007324\n",
      "tensor(1.3006, grad_fn=<DivBackward0>)\n",
      "tensor(1.3050, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 8\n",
      "Epoch: 8 loss:  1.300573468208313\n",
      "tensor(1.2932, grad_fn=<DivBackward0>)\n",
      "tensor(1.2989, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 9\n",
      "Epoch: 9 loss:  1.2932087182998657\n",
      "tensor(1.2898, grad_fn=<DivBackward0>)\n",
      "tensor(1.2980, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 10\n",
      "Epoch: 10 loss:  1.2898485660552979\n",
      "tensor(1.2846, grad_fn=<DivBackward0>)\n",
      "tensor(1.2948, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 11\n",
      "Epoch: 11 loss:  1.284555196762085\n",
      "tensor(1.2794, grad_fn=<DivBackward0>)\n",
      "tensor(1.2925, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 12\n",
      "Epoch: 12 loss:  1.2794148921966553\n",
      "tensor(1.2771, grad_fn=<DivBackward0>)\n",
      "tensor(1.2927, grad_fn=<DivBackward0>)\n",
      "Epoch: 13 loss:  1.277101993560791\n",
      "tensor(1.2735, grad_fn=<DivBackward0>)\n",
      "tensor(1.2938, grad_fn=<DivBackward0>)\n",
      "Epoch: 14 loss:  1.2735064029693604\n",
      "tensor(1.2710, grad_fn=<DivBackward0>)\n",
      "tensor(1.2963, grad_fn=<DivBackward0>)\n",
      "Epoch: 15 loss:  1.2709935903549194\n",
      "tensor(1.2668, grad_fn=<DivBackward0>)\n",
      "tensor(1.2960, grad_fn=<DivBackward0>)\n",
      "Epoch: 16 loss:  1.2667936086654663\n",
      "tensor(1.2636, grad_fn=<DivBackward0>)\n",
      "tensor(1.2947, grad_fn=<DivBackward0>)\n",
      "Epoch: 17 loss:  1.2636144161224365\n",
      "tensor(1.2605, grad_fn=<DivBackward0>)\n",
      "tensor(1.2947, grad_fn=<DivBackward0>)\n",
      "Epoch: 18 loss:  1.2604786157608032\n",
      "tensor(1.2582, grad_fn=<DivBackward0>)\n",
      "tensor(1.2957, grad_fn=<DivBackward0>)\n",
      "Epoch: 19 loss:  1.2581607103347778\n",
      "tensor(1.2561, grad_fn=<DivBackward0>)\n",
      "tensor(1.2982, grad_fn=<DivBackward0>)\n",
      "Epoch: 20 loss:  1.256115436553955\n",
      "tensor(1.2532, grad_fn=<DivBackward0>)\n",
      "tensor(1.2974, grad_fn=<DivBackward0>)\n",
      "Epoch: 21 loss:  1.2532286643981934\n",
      "tensor(1.2510, grad_fn=<DivBackward0>)\n",
      "tensor(1.2993, grad_fn=<DivBackward0>)\n",
      "Epoch: 22 loss:  1.2510099411010742\n",
      "tensor(1.2484, grad_fn=<DivBackward0>)\n",
      "tensor(1.3024, grad_fn=<DivBackward0>)\n",
      "Epoch: 23 loss:  1.2483644485473633\n",
      "tensor(1.2461, grad_fn=<DivBackward0>)\n",
      "tensor(1.3055, grad_fn=<DivBackward0>)\n",
      "Epoch: 24 loss:  1.2461395263671875\n",
      "tensor(1.2433, grad_fn=<DivBackward0>)\n",
      "tensor(1.3051, grad_fn=<DivBackward0>)\n",
      "Epoch: 25 loss:  1.2432575225830078\n",
      "tensor(1.2412, grad_fn=<DivBackward0>)\n",
      "tensor(1.3095, grad_fn=<DivBackward0>)\n",
      "Epoch: 26 loss:  1.2412441968917847\n",
      "tensor(1.2383, grad_fn=<DivBackward0>)\n",
      "tensor(1.3126, grad_fn=<DivBackward0>)\n",
      "Epoch: 27 loss:  1.2382913827896118\n",
      "tensor(1.2358, grad_fn=<DivBackward0>)\n",
      "tensor(1.3147, grad_fn=<DivBackward0>)\n",
      "Epoch: 28 loss:  1.2357536554336548\n",
      "tensor(1.2325, grad_fn=<DivBackward0>)\n",
      "tensor(1.3165, grad_fn=<DivBackward0>)\n",
      "Epoch: 29 loss:  1.2325389385223389\n",
      "tensor(1.2294, grad_fn=<DivBackward0>)\n",
      "tensor(1.3187, grad_fn=<DivBackward0>)\n",
      "Epoch: 30 loss:  1.2294007539749146\n",
      "tensor(1.2262, grad_fn=<DivBackward0>)\n",
      "tensor(1.3240, grad_fn=<DivBackward0>)\n",
      "Epoch: 31 loss:  1.226210594177246\n",
      "tensor(1.2233, grad_fn=<DivBackward0>)\n",
      "tensor(1.3266, grad_fn=<DivBackward0>)\n",
      "Early stopping!\n",
      "[ 0.97499216  0.9931352   0.99940306 ...  0.83153075 -0.21020108\n",
      "  0.9672595 ]\n",
      "[ 0.999851    0.9928881   0.97933173 ...  0.7060371  -0.29483214\n",
      "  0.92394   ]\n",
      "[ 0.9896271   0.9752929   0.9552777  ...  0.6186783  -0.22891437\n",
      "  0.90091664]\n",
      "wl_10_2_2_2_2\n",
      "GCN_edgeweight(\n",
      "  (conv1): GCNConv(6206, 128)\n",
      "  (conv2): GCNConv(128, 16)\n",
      "  (linear): Linear(in_features=16, out_features=10, bias=True)\n",
      ")\n",
      "tensor(1.4008, grad_fn=<DivBackward0>)\n",
      "tensor(1.4009, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 0\n",
      "Epoch: 0 loss:  1.4007774591445923\n",
      "tensor(1.3932, grad_fn=<DivBackward0>)\n",
      "tensor(1.3934, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 1\n",
      "Epoch: 1 loss:  1.3932428359985352\n",
      "tensor(1.3815, grad_fn=<DivBackward0>)\n",
      "tensor(1.3815, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 2\n",
      "Epoch: 2 loss:  1.3815308809280396\n",
      "tensor(1.3651, grad_fn=<DivBackward0>)\n",
      "tensor(1.3649, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 3\n",
      "Epoch: 3 loss:  1.3651026487350464\n",
      "tensor(1.3468, grad_fn=<DivBackward0>)\n",
      "tensor(1.3469, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 4\n",
      "Epoch: 4 loss:  1.3467857837677002\n",
      "tensor(1.3321, grad_fn=<DivBackward0>)\n",
      "tensor(1.3331, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 5\n",
      "Epoch: 5 loss:  1.3321267366409302\n",
      "tensor(1.3173, grad_fn=<DivBackward0>)\n",
      "tensor(1.3191, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 6\n",
      "Epoch: 6 loss:  1.317344307899475\n",
      "tensor(1.3067, grad_fn=<DivBackward0>)\n",
      "tensor(1.3088, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 7\n",
      "Epoch: 7 loss:  1.3067286014556885\n",
      "tensor(1.3002, grad_fn=<DivBackward0>)\n",
      "tensor(1.3035, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 8\n",
      "Epoch: 8 loss:  1.3001842498779297\n",
      "tensor(1.2948, grad_fn=<DivBackward0>)\n",
      "tensor(1.3011, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 9\n",
      "Epoch: 9 loss:  1.2947763204574585\n",
      "tensor(1.2909, grad_fn=<DivBackward0>)\n",
      "tensor(1.3006, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 10\n",
      "Epoch: 10 loss:  1.2908958196640015\n",
      "tensor(1.2861, grad_fn=<DivBackward0>)\n",
      "tensor(1.2997, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 11\n",
      "Epoch: 11 loss:  1.2860898971557617\n",
      "tensor(1.2817, grad_fn=<DivBackward0>)\n",
      "tensor(1.3005, grad_fn=<DivBackward0>)\n",
      "Epoch: 12 loss:  1.2817363739013672\n",
      "tensor(1.2767, grad_fn=<DivBackward0>)\n",
      "tensor(1.3023, grad_fn=<DivBackward0>)\n",
      "Epoch: 13 loss:  1.2767454385757446\n",
      "tensor(1.2718, grad_fn=<DivBackward0>)\n",
      "tensor(1.3018, grad_fn=<DivBackward0>)\n",
      "Epoch: 14 loss:  1.2717714309692383\n",
      "tensor(1.2676, grad_fn=<DivBackward0>)\n",
      "tensor(1.3004, grad_fn=<DivBackward0>)\n",
      "Epoch: 15 loss:  1.2676491737365723\n",
      "tensor(1.2642, grad_fn=<DivBackward0>)\n",
      "tensor(1.2996, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 16\n",
      "Epoch: 16 loss:  1.2641806602478027\n",
      "tensor(1.2622, grad_fn=<DivBackward0>)\n",
      "tensor(1.3006, grad_fn=<DivBackward0>)\n",
      "Epoch: 17 loss:  1.2621620893478394\n",
      "tensor(1.2585, grad_fn=<DivBackward0>)\n",
      "tensor(1.3005, grad_fn=<DivBackward0>)\n",
      "Epoch: 18 loss:  1.2585253715515137\n",
      "tensor(1.2564, grad_fn=<DivBackward0>)\n",
      "tensor(1.3011, grad_fn=<DivBackward0>)\n",
      "Epoch: 19 loss:  1.2563832998275757\n",
      "tensor(1.2533, grad_fn=<DivBackward0>)\n",
      "tensor(1.3034, grad_fn=<DivBackward0>)\n",
      "Epoch: 20 loss:  1.2533432245254517\n",
      "tensor(1.2514, grad_fn=<DivBackward0>)\n",
      "tensor(1.3056, grad_fn=<DivBackward0>)\n",
      "Epoch: 21 loss:  1.251447081565857\n",
      "tensor(1.2487, grad_fn=<DivBackward0>)\n",
      "tensor(1.3087, grad_fn=<DivBackward0>)\n",
      "Epoch: 22 loss:  1.248703122138977\n",
      "tensor(1.2470, grad_fn=<DivBackward0>)\n",
      "tensor(1.3122, grad_fn=<DivBackward0>)\n",
      "Epoch: 23 loss:  1.246953010559082\n",
      "tensor(1.2450, grad_fn=<DivBackward0>)\n",
      "tensor(1.3176, grad_fn=<DivBackward0>)\n",
      "Epoch: 24 loss:  1.2450460195541382\n",
      "tensor(1.2429, grad_fn=<DivBackward0>)\n",
      "tensor(1.3218, grad_fn=<DivBackward0>)\n",
      "Epoch: 25 loss:  1.2429020404815674\n",
      "tensor(1.2412, grad_fn=<DivBackward0>)\n",
      "tensor(1.3270, grad_fn=<DivBackward0>)\n",
      "Epoch: 26 loss:  1.2411999702453613\n",
      "tensor(1.2398, grad_fn=<DivBackward0>)\n",
      "tensor(1.3347, grad_fn=<DivBackward0>)\n",
      "Epoch: 27 loss:  1.2397894859313965\n",
      "tensor(1.2382, grad_fn=<DivBackward0>)\n",
      "tensor(1.3403, grad_fn=<DivBackward0>)\n",
      "Epoch: 28 loss:  1.2382124662399292\n",
      "tensor(1.2364, grad_fn=<DivBackward0>)\n",
      "tensor(1.3481, grad_fn=<DivBackward0>)\n",
      "Epoch: 29 loss:  1.2363935708999634\n",
      "tensor(1.2347, grad_fn=<DivBackward0>)\n",
      "tensor(1.3533, grad_fn=<DivBackward0>)\n",
      "Epoch: 30 loss:  1.2346999645233154\n",
      "tensor(1.2335, grad_fn=<DivBackward0>)\n",
      "tensor(1.3629, grad_fn=<DivBackward0>)\n",
      "Epoch: 31 loss:  1.2334766387939453\n",
      "tensor(1.2325, grad_fn=<DivBackward0>)\n",
      "tensor(1.3737, grad_fn=<DivBackward0>)\n",
      "Epoch: 32 loss:  1.232540488243103\n",
      "tensor(1.2315, grad_fn=<DivBackward0>)\n",
      "tensor(1.3755, grad_fn=<DivBackward0>)\n",
      "Epoch: 33 loss:  1.231484293937683\n",
      "tensor(1.2301, grad_fn=<DivBackward0>)\n",
      "tensor(1.3880, grad_fn=<DivBackward0>)\n",
      "Epoch: 34 loss:  1.2301024198532104\n",
      "tensor(1.2278, grad_fn=<DivBackward0>)\n",
      "tensor(1.3930, grad_fn=<DivBackward0>)\n",
      "Epoch: 35 loss:  1.2278088331222534\n",
      "tensor(1.2267, grad_fn=<DivBackward0>)\n",
      "tensor(1.4017, grad_fn=<DivBackward0>)\n",
      "Early stopping!\n",
      "[0.99929893 0.9995982  0.99976254 ... 0.95624745 0.79250944 0.75781006]\n",
      "[0.9989805  0.995701   0.99727356 ... 0.9712932  0.7804075  0.7888314 ]\n",
      "[0.96225744 0.97113127 0.965822   ... 0.8793425  0.9017953  0.62630826]\n",
      "wl_10_5_10_1_1\n",
      "GCN_edgeweight(\n",
      "  (conv1): GCNConv(6206, 128)\n",
      "  (conv2): GCNConv(128, 16)\n",
      "  (linear): Linear(in_features=16, out_features=10, bias=True)\n",
      ")\n",
      "tensor(1.4008, grad_fn=<DivBackward0>)\n",
      "tensor(1.4010, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 0\n",
      "Epoch: 0 loss:  1.400776743888855\n",
      "tensor(1.3928, grad_fn=<DivBackward0>)\n",
      "tensor(1.3931, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 1\n",
      "Epoch: 1 loss:  1.3928381204605103\n",
      "tensor(1.3801, grad_fn=<DivBackward0>)\n",
      "tensor(1.3805, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 2\n",
      "Epoch: 2 loss:  1.3800841569900513\n",
      "tensor(1.3622, grad_fn=<DivBackward0>)\n",
      "tensor(1.3630, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 3\n",
      "Epoch: 3 loss:  1.3622254133224487\n",
      "tensor(1.3428, grad_fn=<DivBackward0>)\n",
      "tensor(1.3440, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 4\n",
      "Epoch: 4 loss:  1.3428406715393066\n",
      "tensor(1.3285, grad_fn=<DivBackward0>)\n",
      "tensor(1.3292, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 5\n",
      "Epoch: 5 loss:  1.328460454940796\n",
      "tensor(1.3123, grad_fn=<DivBackward0>)\n",
      "tensor(1.3140, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 6\n",
      "Epoch: 6 loss:  1.312260627746582\n",
      "tensor(1.3004, grad_fn=<DivBackward0>)\n",
      "tensor(1.3032, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 7\n",
      "Epoch: 7 loss:  1.3004491329193115\n",
      "tensor(1.2936, grad_fn=<DivBackward0>)\n",
      "tensor(1.2985, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 8\n",
      "Epoch: 8 loss:  1.293580174446106\n",
      "tensor(1.2867, grad_fn=<DivBackward0>)\n",
      "tensor(1.2934, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 9\n",
      "Epoch: 9 loss:  1.2867201566696167\n",
      "tensor(1.2833, grad_fn=<DivBackward0>)\n",
      "tensor(1.2930, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 10\n",
      "Epoch: 10 loss:  1.2833002805709839\n",
      "tensor(1.2770, grad_fn=<DivBackward0>)\n",
      "tensor(1.2914, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 11\n",
      "Epoch: 11 loss:  1.2769681215286255\n",
      "tensor(1.2734, grad_fn=<DivBackward0>)\n",
      "tensor(1.2926, grad_fn=<DivBackward0>)\n",
      "Epoch: 12 loss:  1.2734436988830566\n",
      "tensor(1.2680, grad_fn=<DivBackward0>)\n",
      "tensor(1.2917, grad_fn=<DivBackward0>)\n",
      "Epoch: 13 loss:  1.2679966688156128\n",
      "tensor(1.2634, grad_fn=<DivBackward0>)\n",
      "tensor(1.2908, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 14\n",
      "Epoch: 14 loss:  1.2634371519088745\n",
      "tensor(1.2584, grad_fn=<DivBackward0>)\n",
      "tensor(1.2909, grad_fn=<DivBackward0>)\n",
      "Epoch: 15 loss:  1.2583750486373901\n",
      "tensor(1.2548, grad_fn=<DivBackward0>)\n",
      "tensor(1.2899, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 16\n",
      "Epoch: 16 loss:  1.2548311948776245\n",
      "tensor(1.2516, grad_fn=<DivBackward0>)\n",
      "tensor(1.2904, grad_fn=<DivBackward0>)\n",
      "Epoch: 17 loss:  1.251571536064148\n",
      "tensor(1.2489, grad_fn=<DivBackward0>)\n",
      "tensor(1.2905, grad_fn=<DivBackward0>)\n",
      "Epoch: 18 loss:  1.2489031553268433\n",
      "tensor(1.2457, grad_fn=<DivBackward0>)\n",
      "tensor(1.2924, grad_fn=<DivBackward0>)\n",
      "Epoch: 19 loss:  1.2457330226898193\n",
      "tensor(1.2434, grad_fn=<DivBackward0>)\n",
      "tensor(1.2950, grad_fn=<DivBackward0>)\n",
      "Epoch: 20 loss:  1.2433826923370361\n",
      "tensor(1.2405, grad_fn=<DivBackward0>)\n",
      "tensor(1.2957, grad_fn=<DivBackward0>)\n",
      "Epoch: 21 loss:  1.2404558658599854\n",
      "tensor(1.2386, grad_fn=<DivBackward0>)\n",
      "tensor(1.3002, grad_fn=<DivBackward0>)\n",
      "Epoch: 22 loss:  1.2386103868484497\n",
      "tensor(1.2358, grad_fn=<DivBackward0>)\n",
      "tensor(1.3047, grad_fn=<DivBackward0>)\n",
      "Epoch: 23 loss:  1.2357865571975708\n",
      "tensor(1.2336, grad_fn=<DivBackward0>)\n",
      "tensor(1.3072, grad_fn=<DivBackward0>)\n",
      "Epoch: 24 loss:  1.2335641384124756\n",
      "tensor(1.2316, grad_fn=<DivBackward0>)\n",
      "tensor(1.3124, grad_fn=<DivBackward0>)\n",
      "Epoch: 25 loss:  1.231635332107544\n",
      "tensor(1.2297, grad_fn=<DivBackward0>)\n",
      "tensor(1.3168, grad_fn=<DivBackward0>)\n",
      "Epoch: 26 loss:  1.2297191619873047\n",
      "tensor(1.2283, grad_fn=<DivBackward0>)\n",
      "tensor(1.3226, grad_fn=<DivBackward0>)\n",
      "Epoch: 27 loss:  1.2282601594924927\n",
      "tensor(1.2264, grad_fn=<DivBackward0>)\n",
      "tensor(1.3280, grad_fn=<DivBackward0>)\n",
      "Epoch: 28 loss:  1.2264442443847656\n",
      "tensor(1.2244, grad_fn=<DivBackward0>)\n",
      "tensor(1.3341, grad_fn=<DivBackward0>)\n",
      "Epoch: 29 loss:  1.2243642807006836\n",
      "tensor(1.2230, grad_fn=<DivBackward0>)\n",
      "tensor(1.3407, grad_fn=<DivBackward0>)\n",
      "Epoch: 30 loss:  1.222999930381775\n",
      "tensor(1.2210, grad_fn=<DivBackward0>)\n",
      "tensor(1.3464, grad_fn=<DivBackward0>)\n",
      "Epoch: 31 loss:  1.2210196256637573\n",
      "tensor(1.2196, grad_fn=<DivBackward0>)\n",
      "tensor(1.3567, grad_fn=<DivBackward0>)\n",
      "Epoch: 32 loss:  1.2195686101913452\n",
      "tensor(1.2184, grad_fn=<DivBackward0>)\n",
      "tensor(1.3641, grad_fn=<DivBackward0>)\n",
      "Epoch: 33 loss:  1.2183504104614258\n",
      "tensor(1.2175, grad_fn=<DivBackward0>)\n",
      "tensor(1.3742, grad_fn=<DivBackward0>)\n",
      "Epoch: 34 loss:  1.2174885272979736\n",
      "tensor(1.2153, grad_fn=<DivBackward0>)\n",
      "tensor(1.3820, grad_fn=<DivBackward0>)\n",
      "Epoch: 35 loss:  1.215329885482788\n",
      "tensor(1.2137, grad_fn=<DivBackward0>)\n",
      "tensor(1.3881, grad_fn=<DivBackward0>)\n",
      "Early stopping!\n",
      "[ 0.9964747   0.9997219   0.9990922  ...  0.82233155 -0.94617134\n",
      "  0.7619335 ]\n",
      "[ 0.9996447   0.99612963  0.9975977  ...  0.86521894 -0.9362185\n",
      "  0.8038277 ]\n",
      "[ 0.96955997  0.97649604  0.97547597 ...  0.7560009  -0.9689796\n",
      "  0.72431904]\n",
      "wl_10_5_10_5_10\n",
      "GCN_edgeweight(\n",
      "  (conv1): GCNConv(6206, 128)\n",
      "  (conv2): GCNConv(128, 16)\n",
      "  (linear): Linear(in_features=16, out_features=10, bias=True)\n",
      ")\n",
      "tensor(1.4008, grad_fn=<DivBackward0>)\n",
      "tensor(1.4010, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 0\n",
      "Epoch: 0 loss:  1.4007774591445923\n",
      "tensor(1.3928, grad_fn=<DivBackward0>)\n",
      "tensor(1.3931, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 1\n",
      "Epoch: 1 loss:  1.3928403854370117\n",
      "tensor(1.3800, grad_fn=<DivBackward0>)\n",
      "tensor(1.3804, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 2\n",
      "Epoch: 2 loss:  1.3800251483917236\n",
      "tensor(1.3623, grad_fn=<DivBackward0>)\n",
      "tensor(1.3629, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 3\n",
      "Epoch: 3 loss:  1.3622677326202393\n",
      "tensor(1.3428, grad_fn=<DivBackward0>)\n",
      "tensor(1.3436, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 4\n",
      "Epoch: 4 loss:  1.3427740335464478\n",
      "tensor(1.3281, grad_fn=<DivBackward0>)\n",
      "tensor(1.3290, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 5\n",
      "Epoch: 5 loss:  1.3281078338623047\n",
      "tensor(1.3121, grad_fn=<DivBackward0>)\n",
      "tensor(1.3135, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 6\n",
      "Epoch: 6 loss:  1.3120694160461426\n",
      "tensor(1.3003, grad_fn=<DivBackward0>)\n",
      "tensor(1.3014, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 7\n",
      "Epoch: 7 loss:  1.300252914428711\n",
      "tensor(1.2933, grad_fn=<DivBackward0>)\n",
      "tensor(1.2950, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 8\n",
      "Epoch: 8 loss:  1.2933400869369507\n",
      "tensor(1.2867, grad_fn=<DivBackward0>)\n",
      "tensor(1.2900, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 9\n",
      "Epoch: 9 loss:  1.2867127656936646\n",
      "tensor(1.2835, grad_fn=<DivBackward0>)\n",
      "tensor(1.2898, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 10\n",
      "Epoch: 10 loss:  1.2834670543670654\n",
      "tensor(1.2771, grad_fn=<DivBackward0>)\n",
      "tensor(1.2861, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 11\n",
      "Epoch: 11 loss:  1.2770745754241943\n",
      "tensor(1.2737, grad_fn=<DivBackward0>)\n",
      "tensor(1.2868, grad_fn=<DivBackward0>)\n",
      "Epoch: 12 loss:  1.2737107276916504\n",
      "tensor(1.2681, grad_fn=<DivBackward0>)\n",
      "tensor(1.2863, grad_fn=<DivBackward0>)\n",
      "Epoch: 13 loss:  1.2681186199188232\n",
      "tensor(1.2638, grad_fn=<DivBackward0>)\n",
      "tensor(1.2877, grad_fn=<DivBackward0>)\n",
      "Epoch: 14 loss:  1.2637792825698853\n",
      "tensor(1.2585, grad_fn=<DivBackward0>)\n",
      "tensor(1.2860, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 15\n",
      "Epoch: 15 loss:  1.2584692239761353\n",
      "tensor(1.2552, grad_fn=<DivBackward0>)\n",
      "tensor(1.2857, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 16\n",
      "Epoch: 16 loss:  1.2551844120025635\n",
      "tensor(1.2518, grad_fn=<DivBackward0>)\n",
      "tensor(1.2858, grad_fn=<DivBackward0>)\n",
      "Epoch: 17 loss:  1.2518097162246704\n",
      "tensor(1.2493, grad_fn=<DivBackward0>)\n",
      "tensor(1.2863, grad_fn=<DivBackward0>)\n",
      "Epoch: 18 loss:  1.2493126392364502\n",
      "tensor(1.2462, grad_fn=<DivBackward0>)\n",
      "tensor(1.2866, grad_fn=<DivBackward0>)\n",
      "Epoch: 19 loss:  1.2462446689605713\n",
      "tensor(1.2438, grad_fn=<DivBackward0>)\n",
      "tensor(1.2889, grad_fn=<DivBackward0>)\n",
      "Epoch: 20 loss:  1.2437748908996582\n",
      "tensor(1.2408, grad_fn=<DivBackward0>)\n",
      "tensor(1.2907, grad_fn=<DivBackward0>)\n",
      "Epoch: 21 loss:  1.2407917976379395\n",
      "tensor(1.2386, grad_fn=<DivBackward0>)\n",
      "tensor(1.2937, grad_fn=<DivBackward0>)\n",
      "Epoch: 22 loss:  1.2385988235473633\n",
      "tensor(1.2359, grad_fn=<DivBackward0>)\n",
      "tensor(1.2989, grad_fn=<DivBackward0>)\n",
      "Epoch: 23 loss:  1.235854148864746\n",
      "tensor(1.2336, grad_fn=<DivBackward0>)\n",
      "tensor(1.3020, grad_fn=<DivBackward0>)\n",
      "Epoch: 24 loss:  1.2336164712905884\n",
      "tensor(1.2320, grad_fn=<DivBackward0>)\n",
      "tensor(1.3052, grad_fn=<DivBackward0>)\n",
      "Epoch: 25 loss:  1.2320035696029663\n",
      "tensor(1.2299, grad_fn=<DivBackward0>)\n",
      "tensor(1.3121, grad_fn=<DivBackward0>)\n",
      "Epoch: 26 loss:  1.2299319505691528\n",
      "tensor(1.2283, grad_fn=<DivBackward0>)\n",
      "tensor(1.3193, grad_fn=<DivBackward0>)\n",
      "Epoch: 27 loss:  1.2283470630645752\n",
      "tensor(1.2267, grad_fn=<DivBackward0>)\n",
      "tensor(1.3255, grad_fn=<DivBackward0>)\n",
      "Epoch: 28 loss:  1.226726770401001\n",
      "tensor(1.2251, grad_fn=<DivBackward0>)\n",
      "tensor(1.3306, grad_fn=<DivBackward0>)\n",
      "Epoch: 29 loss:  1.2250735759735107\n",
      "tensor(1.2237, grad_fn=<DivBackward0>)\n",
      "tensor(1.3392, grad_fn=<DivBackward0>)\n",
      "Epoch: 30 loss:  1.2236584424972534\n",
      "tensor(1.2217, grad_fn=<DivBackward0>)\n",
      "tensor(1.3467, grad_fn=<DivBackward0>)\n",
      "Epoch: 31 loss:  1.2217117547988892\n",
      "tensor(1.2201, grad_fn=<DivBackward0>)\n",
      "tensor(1.3566, grad_fn=<DivBackward0>)\n",
      "Epoch: 32 loss:  1.2200714349746704\n",
      "tensor(1.2185, grad_fn=<DivBackward0>)\n",
      "tensor(1.3642, grad_fn=<DivBackward0>)\n",
      "Epoch: 33 loss:  1.218500018119812\n",
      "tensor(1.2180, grad_fn=<DivBackward0>)\n",
      "tensor(1.3730, grad_fn=<DivBackward0>)\n",
      "Epoch: 34 loss:  1.2179769277572632\n",
      "tensor(1.2153, grad_fn=<DivBackward0>)\n",
      "tensor(1.3816, grad_fn=<DivBackward0>)\n",
      "Epoch: 35 loss:  1.2153359651565552\n",
      "tensor(1.2138, grad_fn=<DivBackward0>)\n",
      "tensor(1.3869, grad_fn=<DivBackward0>)\n",
      "Early stopping!\n",
      "[0.9959865  0.9995712  0.99957865 ... 0.9352509  0.8526939  0.8647186 ]\n",
      "[0.99960965 0.99682003 0.9963927  ... 0.9232001  0.8415277  0.90049464]\n",
      "[0.9591085  0.9633703  0.96502143 ... 0.82264376 0.95451194 0.85186785]\n",
      "wl_10_10_10_1_1\n",
      "GCN_edgeweight(\n",
      "  (conv1): GCNConv(6206, 128)\n",
      "  (conv2): GCNConv(128, 16)\n",
      "  (linear): Linear(in_features=16, out_features=10, bias=True)\n",
      ")\n",
      "tensor(1.4008, grad_fn=<DivBackward0>)\n",
      "tensor(1.4009, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 0\n",
      "Epoch: 0 loss:  1.4007766246795654\n",
      "tensor(1.3930, grad_fn=<DivBackward0>)\n",
      "tensor(1.3929, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 1\n",
      "Epoch: 1 loss:  1.3929599523544312\n",
      "tensor(1.3804, grad_fn=<DivBackward0>)\n",
      "tensor(1.3797, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 2\n",
      "Epoch: 2 loss:  1.3804219961166382\n",
      "tensor(1.3629, grad_fn=<DivBackward0>)\n",
      "tensor(1.3612, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 3\n",
      "Epoch: 3 loss:  1.362870693206787\n",
      "tensor(1.3434, grad_fn=<DivBackward0>)\n",
      "tensor(1.3413, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 4\n",
      "Epoch: 4 loss:  1.3434475660324097\n",
      "tensor(1.3284, grad_fn=<DivBackward0>)\n",
      "tensor(1.3266, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 5\n",
      "Epoch: 5 loss:  1.3283528089523315\n",
      "tensor(1.3126, grad_fn=<DivBackward0>)\n",
      "tensor(1.3115, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 6\n",
      "Epoch: 6 loss:  1.3126155138015747\n",
      "tensor(1.3007, grad_fn=<DivBackward0>)\n",
      "tensor(1.2997, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 7\n",
      "Epoch: 7 loss:  1.3006973266601562\n",
      "tensor(1.2939, grad_fn=<DivBackward0>)\n",
      "tensor(1.2942, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 8\n",
      "Epoch: 8 loss:  1.2939186096191406\n",
      "tensor(1.2871, grad_fn=<DivBackward0>)\n",
      "tensor(1.2907, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 9\n",
      "Epoch: 9 loss:  1.2870687246322632\n",
      "tensor(1.2840, grad_fn=<DivBackward0>)\n",
      "tensor(1.2931, grad_fn=<DivBackward0>)\n",
      "Epoch: 10 loss:  1.283998966217041\n",
      "tensor(1.2777, grad_fn=<DivBackward0>)\n",
      "tensor(1.2894, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 11\n",
      "Epoch: 11 loss:  1.2776904106140137\n",
      "tensor(1.2743, grad_fn=<DivBackward0>)\n",
      "tensor(1.2907, grad_fn=<DivBackward0>)\n",
      "Epoch: 12 loss:  1.2742847204208374\n",
      "tensor(1.2684, grad_fn=<DivBackward0>)\n",
      "tensor(1.2908, grad_fn=<DivBackward0>)\n",
      "Epoch: 13 loss:  1.2684218883514404\n",
      "tensor(1.2641, grad_fn=<DivBackward0>)\n",
      "tensor(1.2909, grad_fn=<DivBackward0>)\n",
      "Epoch: 14 loss:  1.2640682458877563\n",
      "tensor(1.2587, grad_fn=<DivBackward0>)\n",
      "tensor(1.2887, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 15\n",
      "Epoch: 15 loss:  1.2586594820022583\n",
      "tensor(1.2555, grad_fn=<DivBackward0>)\n",
      "tensor(1.2879, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 16\n",
      "Epoch: 16 loss:  1.255450963973999\n",
      "tensor(1.2522, grad_fn=<DivBackward0>)\n",
      "tensor(1.2878, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 17\n",
      "Epoch: 17 loss:  1.2521800994873047\n",
      "tensor(1.2497, grad_fn=<DivBackward0>)\n",
      "tensor(1.2896, grad_fn=<DivBackward0>)\n",
      "Epoch: 18 loss:  1.2497106790542603\n",
      "tensor(1.2464, grad_fn=<DivBackward0>)\n",
      "tensor(1.2900, grad_fn=<DivBackward0>)\n",
      "Epoch: 19 loss:  1.246442198753357\n",
      "tensor(1.2441, grad_fn=<DivBackward0>)\n",
      "tensor(1.2924, grad_fn=<DivBackward0>)\n",
      "Epoch: 20 loss:  1.2440797090530396\n",
      "tensor(1.2410, grad_fn=<DivBackward0>)\n",
      "tensor(1.2956, grad_fn=<DivBackward0>)\n",
      "Epoch: 21 loss:  1.2410024404525757\n",
      "tensor(1.2394, grad_fn=<DivBackward0>)\n",
      "tensor(1.2997, grad_fn=<DivBackward0>)\n",
      "Epoch: 22 loss:  1.2394424676895142\n",
      "tensor(1.2365, grad_fn=<DivBackward0>)\n",
      "tensor(1.3034, grad_fn=<DivBackward0>)\n",
      "Epoch: 23 loss:  1.236542820930481\n",
      "tensor(1.2346, grad_fn=<DivBackward0>)\n",
      "tensor(1.3077, grad_fn=<DivBackward0>)\n",
      "Epoch: 24 loss:  1.2345985174179077\n",
      "tensor(1.2325, grad_fn=<DivBackward0>)\n",
      "tensor(1.3145, grad_fn=<DivBackward0>)\n",
      "Epoch: 25 loss:  1.2324883937835693\n",
      "tensor(1.2306, grad_fn=<DivBackward0>)\n",
      "tensor(1.3196, grad_fn=<DivBackward0>)\n",
      "Epoch: 26 loss:  1.2306100130081177\n",
      "tensor(1.2290, grad_fn=<DivBackward0>)\n",
      "tensor(1.3260, grad_fn=<DivBackward0>)\n",
      "Epoch: 27 loss:  1.2290366888046265\n",
      "tensor(1.2271, grad_fn=<DivBackward0>)\n",
      "tensor(1.3331, grad_fn=<DivBackward0>)\n",
      "Epoch: 28 loss:  1.2270835638046265\n",
      "tensor(1.2255, grad_fn=<DivBackward0>)\n",
      "tensor(1.3414, grad_fn=<DivBackward0>)\n",
      "Epoch: 29 loss:  1.2255032062530518\n",
      "tensor(1.2238, grad_fn=<DivBackward0>)\n",
      "tensor(1.3466, grad_fn=<DivBackward0>)\n",
      "Epoch: 30 loss:  1.223842978477478\n",
      "tensor(1.2220, grad_fn=<DivBackward0>)\n",
      "tensor(1.3579, grad_fn=<DivBackward0>)\n",
      "Epoch: 31 loss:  1.2219738960266113\n",
      "tensor(1.2201, grad_fn=<DivBackward0>)\n",
      "tensor(1.3672, grad_fn=<DivBackward0>)\n",
      "Epoch: 32 loss:  1.2201058864593506\n",
      "tensor(1.2189, grad_fn=<DivBackward0>)\n",
      "tensor(1.3712, grad_fn=<DivBackward0>)\n",
      "Epoch: 33 loss:  1.2189499139785767\n",
      "tensor(1.2184, grad_fn=<DivBackward0>)\n",
      "tensor(1.3856, grad_fn=<DivBackward0>)\n",
      "Epoch: 34 loss:  1.2183563709259033\n",
      "tensor(1.2163, grad_fn=<DivBackward0>)\n",
      "tensor(1.3927, grad_fn=<DivBackward0>)\n",
      "Epoch: 35 loss:  1.2162891626358032\n",
      "tensor(1.2142, grad_fn=<DivBackward0>)\n",
      "tensor(1.4003, grad_fn=<DivBackward0>)\n",
      "Epoch: 36 loss:  1.21424400806427\n",
      "tensor(1.2137, grad_fn=<DivBackward0>)\n",
      "tensor(1.4159, grad_fn=<DivBackward0>)\n",
      "Early stopping!\n",
      "[0.99751043 0.9997803  0.9992443  ... 0.78600246 0.8462962  0.83352906]\n",
      "[0.99805224 0.99170554 0.99549735 ... 0.79088247 0.81411386 0.88893104]\n",
      "[0.98363054 0.97172827 0.9812454  ... 0.8215106  0.7677267  0.9302307 ]\n",
      "wl_10_1_1_1_1\n",
      "GCN_edgeweight(\n",
      "  (conv1): GCNConv(6206, 128)\n",
      "  (conv2): GCNConv(128, 16)\n",
      "  (linear): Linear(in_features=16, out_features=10, bias=True)\n",
      ")\n",
      "tensor(1.4008, grad_fn=<DivBackward0>)\n",
      "tensor(1.4010, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 0\n",
      "Epoch: 0 loss:  1.4007631540298462\n",
      "tensor(1.3934, grad_fn=<DivBackward0>)\n",
      "tensor(1.3936, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 1\n",
      "Epoch: 1 loss:  1.3933757543563843\n",
      "tensor(1.3820, grad_fn=<DivBackward0>)\n",
      "tensor(1.3824, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 2\n",
      "Epoch: 2 loss:  1.38197922706604\n",
      "tensor(1.3658, grad_fn=<DivBackward0>)\n",
      "tensor(1.3667, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 3\n",
      "Epoch: 3 loss:  1.3658279180526733\n",
      "tensor(1.3479, grad_fn=<DivBackward0>)\n",
      "tensor(1.3496, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 4\n",
      "Epoch: 4 loss:  1.3478937149047852\n",
      "tensor(1.3335, grad_fn=<DivBackward0>)\n",
      "tensor(1.3365, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 5\n",
      "Epoch: 5 loss:  1.3335485458374023\n",
      "tensor(1.3193, grad_fn=<DivBackward0>)\n",
      "tensor(1.3229, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 6\n",
      "Epoch: 6 loss:  1.3192647695541382\n",
      "tensor(1.3091, grad_fn=<DivBackward0>)\n",
      "tensor(1.3133, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 7\n",
      "Epoch: 7 loss:  1.3091154098510742\n",
      "tensor(1.3025, grad_fn=<DivBackward0>)\n",
      "tensor(1.3084, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 8\n",
      "Epoch: 8 loss:  1.3025423288345337\n",
      "tensor(1.2977, grad_fn=<DivBackward0>)\n",
      "tensor(1.3069, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 9\n",
      "Epoch: 9 loss:  1.2976911067962646\n",
      "tensor(1.2937, grad_fn=<DivBackward0>)\n",
      "tensor(1.3072, grad_fn=<DivBackward0>)\n",
      "Epoch: 10 loss:  1.2936526536941528\n",
      "tensor(1.2891, grad_fn=<DivBackward0>)\n",
      "tensor(1.3067, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 11\n",
      "Epoch: 11 loss:  1.289053201675415\n",
      "tensor(1.2849, grad_fn=<DivBackward0>)\n",
      "tensor(1.3076, grad_fn=<DivBackward0>)\n",
      "Epoch: 12 loss:  1.2848994731903076\n",
      "tensor(1.2803, grad_fn=<DivBackward0>)\n",
      "tensor(1.3085, grad_fn=<DivBackward0>)\n",
      "Epoch: 13 loss:  1.280269980430603\n",
      "tensor(1.2753, grad_fn=<DivBackward0>)\n",
      "tensor(1.3065, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 14\n",
      "Epoch: 14 loss:  1.2752565145492554\n",
      "tensor(1.2715, grad_fn=<DivBackward0>)\n",
      "tensor(1.3058, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 15\n",
      "Epoch: 15 loss:  1.2714874744415283\n",
      "tensor(1.2680, grad_fn=<DivBackward0>)\n",
      "tensor(1.3050, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 16\n",
      "Epoch: 16 loss:  1.2680237293243408\n",
      "tensor(1.2665, grad_fn=<DivBackward0>)\n",
      "tensor(1.3073, grad_fn=<DivBackward0>)\n",
      "Epoch: 17 loss:  1.2665226459503174\n",
      "tensor(1.2632, grad_fn=<DivBackward0>)\n",
      "tensor(1.3060, grad_fn=<DivBackward0>)\n",
      "Epoch: 18 loss:  1.263160228729248\n",
      "tensor(1.2611, grad_fn=<DivBackward0>)\n",
      "tensor(1.3083, grad_fn=<DivBackward0>)\n",
      "Epoch: 19 loss:  1.2610974311828613\n",
      "tensor(1.2582, grad_fn=<DivBackward0>)\n",
      "tensor(1.3112, grad_fn=<DivBackward0>)\n",
      "Epoch: 20 loss:  1.2582136392593384\n",
      "tensor(1.2557, grad_fn=<DivBackward0>)\n",
      "tensor(1.3140, grad_fn=<DivBackward0>)\n",
      "Epoch: 21 loss:  1.2557419538497925\n",
      "tensor(1.2536, grad_fn=<DivBackward0>)\n",
      "tensor(1.3178, grad_fn=<DivBackward0>)\n",
      "Epoch: 22 loss:  1.253554105758667\n",
      "tensor(1.2518, grad_fn=<DivBackward0>)\n",
      "tensor(1.3224, grad_fn=<DivBackward0>)\n",
      "Epoch: 23 loss:  1.2517544031143188\n",
      "tensor(1.2499, grad_fn=<DivBackward0>)\n",
      "tensor(1.3264, grad_fn=<DivBackward0>)\n",
      "Epoch: 24 loss:  1.249885082244873\n",
      "tensor(1.2483, grad_fn=<DivBackward0>)\n",
      "tensor(1.3303, grad_fn=<DivBackward0>)\n",
      "Epoch: 25 loss:  1.2482850551605225\n",
      "tensor(1.2464, grad_fn=<DivBackward0>)\n",
      "tensor(1.3380, grad_fn=<DivBackward0>)\n",
      "Epoch: 26 loss:  1.2464052438735962\n",
      "tensor(1.2451, grad_fn=<DivBackward0>)\n",
      "tensor(1.3440, grad_fn=<DivBackward0>)\n",
      "Epoch: 27 loss:  1.245060682296753\n",
      "tensor(1.2439, grad_fn=<DivBackward0>)\n",
      "tensor(1.3483, grad_fn=<DivBackward0>)\n",
      "Epoch: 28 loss:  1.2438600063323975\n",
      "tensor(1.2422, grad_fn=<DivBackward0>)\n",
      "tensor(1.3587, grad_fn=<DivBackward0>)\n",
      "Epoch: 29 loss:  1.2422089576721191\n",
      "tensor(1.2406, grad_fn=<DivBackward0>)\n",
      "tensor(1.3639, grad_fn=<DivBackward0>)\n",
      "Epoch: 30 loss:  1.2406076192855835\n",
      "tensor(1.2388, grad_fn=<DivBackward0>)\n",
      "tensor(1.3711, grad_fn=<DivBackward0>)\n",
      "Epoch: 31 loss:  1.2388386726379395\n",
      "tensor(1.2377, grad_fn=<DivBackward0>)\n",
      "tensor(1.3822, grad_fn=<DivBackward0>)\n",
      "Epoch: 32 loss:  1.2376818656921387\n",
      "tensor(1.2368, grad_fn=<DivBackward0>)\n",
      "tensor(1.3905, grad_fn=<DivBackward0>)\n",
      "Epoch: 33 loss:  1.2367972135543823\n",
      "tensor(1.2365, grad_fn=<DivBackward0>)\n",
      "tensor(1.4026, grad_fn=<DivBackward0>)\n",
      "Epoch: 34 loss:  1.23648202419281\n",
      "tensor(1.2345, grad_fn=<DivBackward0>)\n",
      "tensor(1.4070, grad_fn=<DivBackward0>)\n",
      "Epoch: 35 loss:  1.2345205545425415\n",
      "tensor(1.2322, grad_fn=<DivBackward0>)\n",
      "tensor(1.4152, grad_fn=<DivBackward0>)\n",
      "Early stopping!\n",
      "[ 0.9970265   0.99960613  0.9988854  ... -0.95984596  0.80067694\n",
      "  0.9784188 ]\n",
      "[ 0.99799263  0.9936103   0.9951647  ... -0.9594008   0.7563443\n",
      "  0.9820943 ]\n",
      "[ 0.87363374  0.895422    0.8849712  ... -0.8933099   0.9447387\n",
      "  0.84418255]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dict = dict()\n",
    "\n",
    "dir_path = \"/content/gdrive/MyDrive/GDS/walks/with_onehot\"\n",
    "hidden_channels = [128, 16]\n",
    "encoding_dim = 10\n",
    "\n",
    "seed_pages_used = [    \n",
    "    '/find-a-job',\n",
    "    '/universal-credit',\n",
    "    '/government/collections/financial-support-for-businesses-during-coronavirus-covid-19']\n",
    "\n",
    "\n",
    "walk_length_list = [5, 10]\n",
    "walk_params_list = [[2, 2, 2, 2],\n",
    "                [5, 10, 1, 1],\n",
    "               [5, 10, 5, 10],\n",
    "               [10, 10, 1 ,1],\n",
    "               [1, 1, 1, 1]]\n",
    "\n",
    "# walk_length_list = [10, 3]\n",
    "# walk_params_list = [[5, 10, 1, 1],\n",
    "#                [5, 10, 5, 10],\n",
    "#                [10, 10, 1 ,1],\n",
    "#                [1, 1, 1, 1]]\n",
    "\n",
    "for walk_length in walk_length_list:\n",
    "\n",
    "  for walk_params in walk_params_list:\n",
    "\n",
    "    name_extension = \"wl_\" + str(walk_length)\n",
    "    for x in walk_params:\n",
    "      name_extension += \"_\" + str(x)\n",
    "\n",
    "    pyg_graph = load_pickle_file(\"pyg_graph_\" + name_extension, dir_path=dir_path)\n",
    "    # dump_pickle_file(failed_nodes, 'failed_nodes', dir_path=\"/content/gdrive/MyDrive/GDS/pickles/\")\n",
    "    neigh_matrix = load_pickle_file(\"neigh_matrix_\" + name_extension, dir_path=dir_path)\n",
    "    negative_array = load_pickle_file(\"negative_array_\" + name_extension, dir_path=dir_path)\n",
    "\n",
    "    print(name_extension)\n",
    "\n",
    "    model = GCN_edgeweight(hidden_channels, encoding_dim)\n",
    "    model_name = \"/model_\" + name_extension\n",
    "    print(model)\n",
    "    criterion = MSELoss3()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # Define optimizer.\n",
    "\n",
    "    history = train(model, pyg_graph, neigh_matrix, negative_array, num_epochs = 100, \n",
    "                    dir_path = \"/content/gdrive/MyDrive/GDS/walks/with_onehot/models\", model_name = model_name) \n",
    "\n",
    "\n",
    "    # model = GCN_edgeweight(hidden_channels, encoding_dim)\n",
    "    val_loss, scores_df_rankings, score = scores(model, pyg_graph, neigh_matrix, negative_array, seed_pages_used, edge_weight = True)\n",
    "    \n",
    "    results_dict[model_name] = dict()\n",
    "    results_dict[model_name][\"loss\"] = val_loss\n",
    "    results_dict[model_name][\"score\"] = score\n",
    "    results_dict[model_name][\"ranking\"] = scores_df_rankings  \n",
    "\n",
    "dump_pickle_file(results_dict, \"results_dict\", dir_path = \"/content/gdrive/MyDrive/GDS/walks/with_onehot/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L0Ip_2jPEx1z"
   },
   "outputs": [],
   "source": [
    "results_dict = load_pickle_file(\"results_dict\", dir_path = \"/content/gdrive/MyDrive/GDS/walks/noextrawalks/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5vmhF_IDE3d3",
    "outputId": "3f14fd3e-15a2-4060-e7eb-80e649f68947"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/model_wl_5_2_2_2_2\n",
      "tensor(1.2422, requires_grad=True)\n",
      "/model_wl_5_5_10_1_1\n",
      "tensor(1.2194, requires_grad=True)\n",
      "/model_wl_5_5_10_5_10\n",
      "tensor(1.2648, requires_grad=True)\n",
      "/model_wl_5_10_10_1_1\n",
      "tensor(1.2419, requires_grad=True)\n",
      "/model_wl_5_1_1_1_1\n",
      "tensor(1.2492, requires_grad=True)\n",
      "/model_wl_10_2_2_2_2\n",
      "tensor(1.2536, requires_grad=True)\n",
      "/model_wl_10_5_10_1_1\n",
      "tensor(1.2367, requires_grad=True)\n",
      "/model_wl_10_5_10_5_10\n",
      "tensor(1.2500, requires_grad=True)\n",
      "/model_wl_10_10_10_1_1\n",
      "tensor(1.2523, requires_grad=True)\n",
      "/model_wl_10_1_1_1_1\n",
      "tensor(1.2629, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for model in results_dict.keys():\n",
    "  print(model)\n",
    "  print( results_dict[model][\"loss\"] )\n",
    "  results_dict[model][\"ranking\"].to_csv(\"/content/gdrive/MyDrive/GDS/walks/noextrawalks/models\" + str(model) + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4SZv8p1WE_6q",
    "outputId": "293673ea-6873-4346-d678-0ea8921227cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                     /universal-credit\n",
       "1     /government/collections/financial-support-for-...\n",
       "2                                           /find-a-job\n",
       "3                                       /browse/working\n",
       "4                    /check-job-applicant-right-to-work\n",
       "5                           /browse/working/finding-job\n",
       "6                             /sign-in-universal-credit\n",
       "7                   /guidance/universal-credit-advances\n",
       "8     /government/collections/sponsorship-informatio...\n",
       "9                      /browse/working/tax-minimum-wage\n",
       "10                                   /become-apprentice\n",
       "11                                /apply-apprenticeship\n",
       "12                                /jobseekers-allowance\n",
       "13                                 /prove-right-to-work\n",
       "14                                   /state-pension-age\n",
       "15                             /topic/business-tax/paye\n",
       "16                        /browse/education/find-course\n",
       "17                                /view-driving-licence\n",
       "18    /government/organisations/uk-visas-and-immigra...\n",
       "19                        /request-copy-criminal-record\n",
       "20                    /guidance/pay-apprenticeship-levy\n",
       "21                   /browse/employing-people/contracts\n",
       "22                             /browse/employing-people\n",
       "23                       /view-prove-immigration-status\n",
       "24    /new-state-pension/your-national-insurance-rec...\n",
       "25                          /career-skills-and-training\n",
       "26                            /browse/visas-immigration\n",
       "27                /log-in-register-hmrc-online-services\n",
       "28    /government/organisations/education-and-skills...\n",
       "29                       /browse/tax/national-insurance\n",
       "30               /guidance/apprenticeship-funding-rules\n",
       "31                                /personal-tax-account\n",
       "32                                 /browse/disabilities\n",
       "33      /topic/further-education-skills/apprenticeships\n",
       "34    /government/organisations/department-for-work-...\n",
       "35                                   /1619-bursary-fund\n",
       "36    /guidance/travel-abroad-from-england-during-co...\n",
       "37                                      /access-to-work\n",
       "38                                       /check-uk-visa\n",
       "39    /capital-gains-tax/report-and-pay-capital-gain...\n",
       "40                                 /check-state-pension\n",
       "41    /guidance/brexit-guidance-for-individuals-and-...\n",
       "42    /settled-status-eu-citizens-families/applying-...\n",
       "43                   /criminal-record-checks-apply-role\n",
       "44                 /browse/benefits/manage-your-benefit\n",
       "45                                              /brexit\n",
       "46                      /guidance/free-courses-for-jobs\n",
       "47                       /browse/housing-local-services\n",
       "48                      /browse/births-deaths-marriages\n",
       "49                      /contact-ukvi-inside-outside-uk\n",
       "Name: page, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dict['/model_wl_5_5_10_1_1']['ranking'].sort_values(by = \"max\", ascending = False)[\"page\"].iloc[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WcwduOZLFxkv",
    "outputId": "3b63a2bb-4fda-4739-da93-1571d094f66a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                     /universal-credit\n",
       "1                                           /find-a-job\n",
       "2     /government/collections/financial-support-for-...\n",
       "3                              /browse/working/time-off\n",
       "4                  /browse/visas-immigration/work-visas\n",
       "5                                  /check-state-pension\n",
       "6                                       /access-to-work\n",
       "7                  /browse/benefits/manage-your-benefit\n",
       "8                               /contact-jobcentre-plus\n",
       "9                          /browse/working/armed-forces\n",
       "10                                      /browse/working\n",
       "11    /government/organisations/department-for-work-...\n",
       "12                                         /coronavirus\n",
       "13                       /view-prove-immigration-status\n",
       "14                                     /browse/benefits\n",
       "15                            /browse/visas-immigration\n",
       "16                        /browse/working/state-pension\n",
       "17                         /national-minimum-wage-rates\n",
       "18                                /jobseekers-allowance\n",
       "19                                   /become-apprentice\n",
       "20                        /skilled-worker-visa/your-job\n",
       "21                            /sign-in-universal-credit\n",
       "22    /guidance/travel-abroad-from-england-during-co...\n",
       "23    /guidance/red-amber-and-green-list-rules-for-e...\n",
       "24                   /criminal-record-checks-apply-role\n",
       "25                                        /contact-hmrc\n",
       "26                                  /browse/citizenship\n",
       "27                                /apply-apprenticeship\n",
       "28                     /apply-national-insurance-number\n",
       "29                                          /browse/tax\n",
       "30                                  /dbs-update-service\n",
       "31                                     /browse/business\n",
       "32                  /browse/working/rights-trade-unions\n",
       "33                                 /prove-right-to-work\n",
       "34                            /browse/disabilities/work\n",
       "35                /log-in-register-hmrc-online-services\n",
       "36               /browse/working/contract-working-hours\n",
       "37                      /search/guidance-and-regulation\n",
       "38    /government/organisations/education-and-skills...\n",
       "39                                    /browse/education\n",
       "40                                       /browse/abroad\n",
       "41    /government/organisations/uk-visas-and-immigra...\n",
       "42    /government/organisations/disclosure-and-barri...\n",
       "43                      /contact-ukvi-inside-outside-uk\n",
       "44                                   /state-pension-age\n",
       "45      /topic/further-education-skills/apprenticeships\n",
       "46                        /browse/education/find-course\n",
       "47                   /browse/employing-people/contracts\n",
       "48    /guidance/dbs-check-requests-guidance-for-empl...\n",
       "49                                      /pension-credit\n",
       "Name: page, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dict['/model_wl_5_5_10_5_10']['ranking'].sort_values(by = \"max\", ascending = False)[\"page\"].iloc[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8TMJjLxNccL"
   },
   "source": [
    "### Load objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "713kSXonkica"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGFDnmyCNh3z"
   },
   "source": [
    "### Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fBT8wlDbtAhX",
    "outputId": "6081684c-dce8-41e0-8081-f1203541d07f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding dim 10\n",
      "GCN_edgeweight(\n",
      "  (conv1): GCNConv(1555, 128)\n",
      "  (conv2): GCNConv(128, 16)\n",
      "  (linear): Linear(in_features=16, out_features=10, bias=True)\n",
      ")\n",
      "tensor(1.4082, grad_fn=<DivBackward0>)\n",
      "tensor(1.4084, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 0\n",
      "Epoch: 0 loss:  1.4081979990005493\n",
      "tensor(1.3965, grad_fn=<DivBackward0>)\n",
      "tensor(1.3967, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 1\n",
      "Epoch: 1 loss:  1.3964701890945435\n",
      "tensor(1.3819, grad_fn=<DivBackward0>)\n",
      "tensor(1.3826, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 2\n",
      "Epoch: 2 loss:  1.3819078207015991\n",
      "tensor(1.3711, grad_fn=<DivBackward0>)\n",
      "tensor(1.3735, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 3\n",
      "Epoch: 3 loss:  1.3711469173431396\n",
      "tensor(1.3560, grad_fn=<DivBackward0>)\n",
      "tensor(1.3581, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 4\n",
      "Epoch: 4 loss:  1.3560467958450317\n",
      "tensor(1.3457, grad_fn=<DivBackward0>)\n",
      "tensor(1.3479, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 5\n",
      "Epoch: 5 loss:  1.3457310199737549\n",
      "tensor(1.3318, grad_fn=<DivBackward0>)\n",
      "tensor(1.3360, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 6\n",
      "Epoch: 6 loss:  1.331827163696289\n",
      "tensor(1.3261, grad_fn=<DivBackward0>)\n",
      "tensor(1.3329, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 7\n",
      "Epoch: 7 loss:  1.3260867595672607\n",
      "tensor(1.3147, grad_fn=<DivBackward0>)\n",
      "tensor(1.3219, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 8\n",
      "Epoch: 8 loss:  1.3147101402282715\n",
      "tensor(1.3118, grad_fn=<DivBackward0>)\n",
      "tensor(1.3188, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 9\n",
      "Epoch: 9 loss:  1.311766505241394\n",
      "tensor(1.3071, grad_fn=<DivBackward0>)\n",
      "tensor(1.3159, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 10\n",
      "Epoch: 10 loss:  1.3070768117904663\n",
      "tensor(1.3072, grad_fn=<DivBackward0>)\n",
      "tensor(1.3195, grad_fn=<DivBackward0>)\n",
      "Epoch: 11 loss:  1.3072237968444824\n",
      "tensor(1.3014, grad_fn=<DivBackward0>)\n",
      "tensor(1.3130, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 12\n",
      "Epoch: 12 loss:  1.3013627529144287\n",
      "tensor(1.2980, grad_fn=<DivBackward0>)\n",
      "tensor(1.3103, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 13\n",
      "Epoch: 13 loss:  1.297977328300476\n",
      "tensor(1.2922, grad_fn=<DivBackward0>)\n",
      "tensor(1.3090, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 14\n",
      "Epoch: 14 loss:  1.2921710014343262\n",
      "tensor(1.2917, grad_fn=<DivBackward0>)\n",
      "tensor(1.3143, grad_fn=<DivBackward0>)\n",
      "Epoch: 15 loss:  1.291671633720398\n",
      "tensor(1.2861, grad_fn=<DivBackward0>)\n",
      "tensor(1.3056, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 16\n",
      "Epoch: 16 loss:  1.2861040830612183\n",
      "tensor(1.2838, grad_fn=<DivBackward0>)\n",
      "tensor(1.3031, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 17\n",
      "Epoch: 17 loss:  1.283823847770691\n",
      "tensor(1.2810, grad_fn=<DivBackward0>)\n",
      "tensor(1.3054, grad_fn=<DivBackward0>)\n",
      "Epoch: 18 loss:  1.2810122966766357\n",
      "tensor(1.2799, grad_fn=<DivBackward0>)\n",
      "tensor(1.3059, grad_fn=<DivBackward0>)\n",
      "Epoch: 19 loss:  1.2798992395401\n",
      "tensor(1.2765, grad_fn=<DivBackward0>)\n",
      "tensor(1.3001, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 20\n",
      "Epoch: 20 loss:  1.2765165567398071\n",
      "tensor(1.2755, grad_fn=<DivBackward0>)\n",
      "tensor(1.2989, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 21\n",
      "Epoch: 21 loss:  1.275519847869873\n",
      "tensor(1.2737, grad_fn=<DivBackward0>)\n",
      "tensor(1.3019, grad_fn=<DivBackward0>)\n",
      "Epoch: 22 loss:  1.2737220525741577\n",
      "tensor(1.2721, grad_fn=<DivBackward0>)\n",
      "tensor(1.2991, grad_fn=<DivBackward0>)\n",
      "Epoch: 23 loss:  1.27213716506958\n",
      "tensor(1.2711, grad_fn=<DivBackward0>)\n",
      "tensor(1.2974, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 24\n",
      "Epoch: 24 loss:  1.271122932434082\n",
      "tensor(1.2683, grad_fn=<DivBackward0>)\n",
      "tensor(1.3004, grad_fn=<DivBackward0>)\n",
      "Epoch: 25 loss:  1.2683088779449463\n",
      "tensor(1.2673, grad_fn=<DivBackward0>)\n",
      "tensor(1.3025, grad_fn=<DivBackward0>)\n",
      "Epoch: 26 loss:  1.2673271894454956\n",
      "tensor(1.2673, grad_fn=<DivBackward0>)\n",
      "tensor(1.3011, grad_fn=<DivBackward0>)\n",
      "Epoch: 27 loss:  1.2673008441925049\n",
      "tensor(1.2642, grad_fn=<DivBackward0>)\n",
      "tensor(1.3009, grad_fn=<DivBackward0>)\n",
      "Epoch: 28 loss:  1.2642043828964233\n",
      "tensor(1.2632, grad_fn=<DivBackward0>)\n",
      "tensor(1.3000, grad_fn=<DivBackward0>)\n",
      "Epoch: 29 loss:  1.263225793838501\n",
      "tensor(1.2623, grad_fn=<DivBackward0>)\n",
      "tensor(1.2972, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 30\n",
      "Epoch: 30 loss:  1.2623211145401\n",
      "tensor(1.2616, grad_fn=<DivBackward0>)\n",
      "tensor(1.3011, grad_fn=<DivBackward0>)\n",
      "Epoch: 31 loss:  1.2615690231323242\n",
      "tensor(1.2597, grad_fn=<DivBackward0>)\n",
      "tensor(1.2949, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 32\n",
      "Epoch: 32 loss:  1.2596566677093506\n",
      "tensor(1.2583, grad_fn=<DivBackward0>)\n",
      "tensor(1.2953, grad_fn=<DivBackward0>)\n",
      "Epoch: 33 loss:  1.2583385705947876\n",
      "tensor(1.2579, grad_fn=<DivBackward0>)\n",
      "tensor(1.2958, grad_fn=<DivBackward0>)\n",
      "Epoch: 34 loss:  1.257935643196106\n",
      "tensor(1.2573, grad_fn=<DivBackward0>)\n",
      "tensor(1.2933, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 35\n",
      "Epoch: 35 loss:  1.257257342338562\n",
      "tensor(1.2558, grad_fn=<DivBackward0>)\n",
      "tensor(1.2944, grad_fn=<DivBackward0>)\n",
      "Epoch: 36 loss:  1.255815863609314\n",
      "tensor(1.2551, grad_fn=<DivBackward0>)\n",
      "tensor(1.2992, grad_fn=<DivBackward0>)\n",
      "Epoch: 37 loss:  1.2550790309906006\n",
      "tensor(1.2542, grad_fn=<DivBackward0>)\n",
      "tensor(1.2919, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 38\n",
      "Epoch: 38 loss:  1.254155158996582\n",
      "tensor(1.2544, grad_fn=<DivBackward0>)\n",
      "tensor(1.2984, grad_fn=<DivBackward0>)\n",
      "Epoch: 39 loss:  1.254409670829773\n",
      "tensor(1.2535, grad_fn=<DivBackward0>)\n",
      "tensor(1.2919, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 40\n",
      "Epoch: 40 loss:  1.2535269260406494\n",
      "tensor(1.2519, grad_fn=<DivBackward0>)\n",
      "tensor(1.3036, grad_fn=<DivBackward0>)\n",
      "Epoch: 41 loss:  1.2518681287765503\n",
      "tensor(1.2501, grad_fn=<DivBackward0>)\n",
      "tensor(1.2921, grad_fn=<DivBackward0>)\n",
      "Epoch: 42 loss:  1.2501342296600342\n",
      "tensor(1.2494, grad_fn=<DivBackward0>)\n",
      "tensor(1.2899, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 43\n",
      "Epoch: 43 loss:  1.2494386434555054\n",
      "tensor(1.2475, grad_fn=<DivBackward0>)\n",
      "tensor(1.2991, grad_fn=<DivBackward0>)\n",
      "Epoch: 44 loss:  1.2475413084030151\n",
      "tensor(1.2471, grad_fn=<DivBackward0>)\n",
      "tensor(1.2953, grad_fn=<DivBackward0>)\n",
      "Epoch: 45 loss:  1.2470734119415283\n",
      "tensor(1.2470, grad_fn=<DivBackward0>)\n",
      "tensor(1.3000, grad_fn=<DivBackward0>)\n",
      "Epoch: 46 loss:  1.246989369392395\n",
      "tensor(1.2450, grad_fn=<DivBackward0>)\n",
      "tensor(1.2991, grad_fn=<DivBackward0>)\n",
      "Epoch: 47 loss:  1.2449979782104492\n",
      "tensor(1.2439, grad_fn=<DivBackward0>)\n",
      "tensor(1.2984, grad_fn=<DivBackward0>)\n",
      "Epoch: 48 loss:  1.2438931465148926\n",
      "tensor(1.2429, grad_fn=<DivBackward0>)\n",
      "tensor(1.2870, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 49\n",
      "Epoch: 49 loss:  1.2428994178771973\n",
      "tensor(1.2413, grad_fn=<DivBackward0>)\n",
      "tensor(1.3016, grad_fn=<DivBackward0>)\n",
      "Epoch: 50 loss:  1.2413221597671509\n",
      "tensor(1.2406, grad_fn=<DivBackward0>)\n",
      "tensor(1.2919, grad_fn=<DivBackward0>)\n",
      "Epoch: 51 loss:  1.2406272888183594\n",
      "tensor(1.2392, grad_fn=<DivBackward0>)\n",
      "tensor(1.2921, grad_fn=<DivBackward0>)\n",
      "Epoch: 52 loss:  1.2391786575317383\n",
      "tensor(1.2380, grad_fn=<DivBackward0>)\n",
      "tensor(1.2916, grad_fn=<DivBackward0>)\n",
      "Epoch: 53 loss:  1.2380378246307373\n",
      "tensor(1.2354, grad_fn=<DivBackward0>)\n",
      "tensor(1.3011, grad_fn=<DivBackward0>)\n",
      "Epoch: 54 loss:  1.235366702079773\n",
      "tensor(1.2373, grad_fn=<DivBackward0>)\n",
      "tensor(1.3047, grad_fn=<DivBackward0>)\n",
      "Epoch: 55 loss:  1.2373075485229492\n",
      "tensor(1.2371, grad_fn=<DivBackward0>)\n",
      "tensor(1.2873, grad_fn=<DivBackward0>)\n",
      "Epoch: 56 loss:  1.2370927333831787\n",
      "tensor(1.2392, grad_fn=<DivBackward0>)\n",
      "tensor(1.3074, grad_fn=<DivBackward0>)\n",
      "Epoch: 57 loss:  1.2391529083251953\n",
      "tensor(1.2374, grad_fn=<DivBackward0>)\n",
      "tensor(1.2889, grad_fn=<DivBackward0>)\n",
      "Epoch: 58 loss:  1.237412691116333\n",
      "tensor(1.2324, grad_fn=<DivBackward0>)\n",
      "tensor(1.2935, grad_fn=<DivBackward0>)\n",
      "Epoch: 59 loss:  1.232397437095642\n",
      "tensor(1.2366, grad_fn=<DivBackward0>)\n",
      "tensor(1.3004, grad_fn=<DivBackward0>)\n",
      "Epoch: 60 loss:  1.2365717887878418\n",
      "tensor(1.2403, grad_fn=<DivBackward0>)\n",
      "tensor(1.2897, grad_fn=<DivBackward0>)\n",
      "Epoch: 61 loss:  1.2403420209884644\n",
      "tensor(1.2300, grad_fn=<DivBackward0>)\n",
      "tensor(1.2959, grad_fn=<DivBackward0>)\n",
      "Epoch: 62 loss:  1.230031132698059\n",
      "tensor(1.2413, grad_fn=<DivBackward0>)\n",
      "tensor(1.3210, grad_fn=<DivBackward0>)\n",
      "Epoch: 63 loss:  1.2413201332092285\n",
      "tensor(1.2401, grad_fn=<DivBackward0>)\n",
      "tensor(1.2892, grad_fn=<DivBackward0>)\n",
      "Epoch: 64 loss:  1.240102767944336\n",
      "tensor(1.2383, grad_fn=<DivBackward0>)\n",
      "tensor(1.2859, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 65\n",
      "Epoch: 65 loss:  1.2382525205612183\n",
      "tensor(1.2337, grad_fn=<DivBackward0>)\n",
      "tensor(1.3048, grad_fn=<DivBackward0>)\n",
      "Epoch: 66 loss:  1.2337359189987183\n",
      "tensor(1.2338, grad_fn=<DivBackward0>)\n",
      "tensor(1.3027, grad_fn=<DivBackward0>)\n",
      "Epoch: 67 loss:  1.2337974309921265\n",
      "tensor(1.2317, grad_fn=<DivBackward0>)\n",
      "tensor(1.2823, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 68\n",
      "Epoch: 68 loss:  1.2316794395446777\n",
      "tensor(1.2301, grad_fn=<DivBackward0>)\n",
      "tensor(1.2859, grad_fn=<DivBackward0>)\n",
      "Epoch: 69 loss:  1.2300931215286255\n",
      "tensor(1.2309, grad_fn=<DivBackward0>)\n",
      "tensor(1.3099, grad_fn=<DivBackward0>)\n",
      "Epoch: 70 loss:  1.2308565378189087\n",
      "tensor(1.2245, grad_fn=<DivBackward0>)\n",
      "tensor(1.2997, grad_fn=<DivBackward0>)\n",
      "Epoch: 71 loss:  1.2245378494262695\n",
      "tensor(1.2264, grad_fn=<DivBackward0>)\n",
      "tensor(1.2810, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 72\n",
      "Epoch: 72 loss:  1.2263591289520264\n",
      "tensor(1.2231, grad_fn=<DivBackward0>)\n",
      "tensor(1.2786, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 73\n",
      "Epoch: 73 loss:  1.2231487035751343\n",
      "tensor(1.2242, grad_fn=<DivBackward0>)\n",
      "tensor(1.2937, grad_fn=<DivBackward0>)\n",
      "Epoch: 74 loss:  1.2241519689559937\n",
      "tensor(1.2220, grad_fn=<DivBackward0>)\n",
      "tensor(1.2856, grad_fn=<DivBackward0>)\n",
      "Epoch: 75 loss:  1.2219743728637695\n",
      "tensor(1.2237, grad_fn=<DivBackward0>)\n",
      "tensor(1.2793, grad_fn=<DivBackward0>)\n",
      "Epoch: 76 loss:  1.2236628532409668\n",
      "tensor(1.2189, grad_fn=<DivBackward0>)\n",
      "tensor(1.2872, grad_fn=<DivBackward0>)\n",
      "Epoch: 77 loss:  1.2188962697982788\n",
      "tensor(1.2223, grad_fn=<DivBackward0>)\n",
      "tensor(1.3074, grad_fn=<DivBackward0>)\n",
      "Epoch: 78 loss:  1.222306251525879\n",
      "tensor(1.2184, grad_fn=<DivBackward0>)\n",
      "tensor(1.2797, grad_fn=<DivBackward0>)\n",
      "Epoch: 79 loss:  1.2183852195739746\n",
      "tensor(1.2193, grad_fn=<DivBackward0>)\n",
      "tensor(1.2779, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 80\n",
      "Epoch: 80 loss:  1.2192860841751099\n",
      "tensor(1.2167, grad_fn=<DivBackward0>)\n",
      "tensor(1.2833, grad_fn=<DivBackward0>)\n",
      "Epoch: 81 loss:  1.2166987657546997\n",
      "tensor(1.2197, grad_fn=<DivBackward0>)\n",
      "tensor(1.2994, grad_fn=<DivBackward0>)\n",
      "Epoch: 82 loss:  1.2196558713912964\n",
      "tensor(1.2161, grad_fn=<DivBackward0>)\n",
      "tensor(1.2731, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 83\n",
      "Epoch: 83 loss:  1.216078758239746\n",
      "tensor(1.2157, grad_fn=<DivBackward0>)\n",
      "tensor(1.2729, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 84\n",
      "Epoch: 84 loss:  1.2156946659088135\n",
      "tensor(1.2184, grad_fn=<DivBackward0>)\n",
      "tensor(1.2994, grad_fn=<DivBackward0>)\n",
      "Epoch: 85 loss:  1.2183761596679688\n",
      "tensor(1.2131, grad_fn=<DivBackward0>)\n",
      "tensor(1.2843, grad_fn=<DivBackward0>)\n",
      "Epoch: 86 loss:  1.2130790948867798\n",
      "tensor(1.2139, grad_fn=<DivBackward0>)\n",
      "tensor(1.2816, grad_fn=<DivBackward0>)\n",
      "Epoch: 87 loss:  1.2138712406158447\n",
      "tensor(1.2124, grad_fn=<DivBackward0>)\n",
      "tensor(1.2871, grad_fn=<DivBackward0>)\n",
      "Epoch: 88 loss:  1.2124385833740234\n",
      "tensor(1.2151, grad_fn=<DivBackward0>)\n",
      "tensor(1.2892, grad_fn=<DivBackward0>)\n",
      "Epoch: 89 loss:  1.2151036262512207\n",
      "tensor(1.2115, grad_fn=<DivBackward0>)\n",
      "tensor(1.2792, grad_fn=<DivBackward0>)\n",
      "Epoch: 90 loss:  1.211491346359253\n",
      "tensor(1.2097, grad_fn=<DivBackward0>)\n",
      "tensor(1.2798, grad_fn=<DivBackward0>)\n",
      "Epoch: 91 loss:  1.2097171545028687\n",
      "tensor(1.2124, grad_fn=<DivBackward0>)\n",
      "tensor(1.2794, grad_fn=<DivBackward0>)\n",
      "Epoch: 92 loss:  1.212403416633606\n",
      "tensor(1.2084, grad_fn=<DivBackward0>)\n",
      "tensor(1.2767, grad_fn=<DivBackward0>)\n",
      "Epoch: 93 loss:  1.208385944366455\n",
      "tensor(1.2087, grad_fn=<DivBackward0>)\n",
      "tensor(1.2705, grad_fn=<DivBackward0>)\n",
      "Saving at epoch 94\n",
      "Epoch: 94 loss:  1.2086941003799438\n",
      "tensor(1.2083, grad_fn=<DivBackward0>)\n",
      "tensor(1.2830, grad_fn=<DivBackward0>)\n",
      "Epoch: 95 loss:  1.2083319425582886\n",
      "tensor(1.2068, grad_fn=<DivBackward0>)\n",
      "tensor(1.2852, grad_fn=<DivBackward0>)\n",
      "Epoch: 96 loss:  1.206787347793579\n",
      "tensor(1.2074, grad_fn=<DivBackward0>)\n",
      "tensor(1.2785, grad_fn=<DivBackward0>)\n",
      "Epoch: 97 loss:  1.2074027061462402\n",
      "tensor(1.2061, grad_fn=<DivBackward0>)\n",
      "tensor(1.2771, grad_fn=<DivBackward0>)\n",
      "Epoch: 98 loss:  1.2061314582824707\n",
      "tensor(1.2055, grad_fn=<DivBackward0>)\n",
      "tensor(1.2883, grad_fn=<DivBackward0>)\n",
      "Epoch: 99 loss:  1.2054872512817383\n",
      "Spec: [128, 16]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-56f6cdcf65e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Spec:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mplot_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-99-b2e07dc063f2>\u001b[0m in \u001b[0;36mplot_losses\u001b[0;34m(history)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# hidden_channels_list = [[64, 32], [128, 10],  [128, 16], [32, 16]]\n",
    "# encoding_dims = [5, 10, 15]\n",
    "\n",
    "hidden_channels_list = [[128, 16]]\n",
    "encoding_dims = [10]\n",
    "\n",
    "for encoding_dim in encoding_dims:\n",
    "  print(\"Encoding dim\", encoding_dim)\n",
    "\n",
    "  for hidden_channels in hidden_channels_list:\n",
    "\n",
    "    model = GCN_edgeweight(hidden_channels, encoding_dim)\n",
    "    print(model)\n",
    "    # h = model(pyg_graph.node_feature, pyg_graph.edge_index)\n",
    "\n",
    "\n",
    "    # model = GCN(hidden_channels, encoding_dim)\n",
    "    # criterion = torch.nn.CrossEntropyLoss(weight = weights)  # Define loss criterion.\n",
    "    criterion = MSELoss3()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # Define optimizer.\n",
    "\n",
    "    history = train(model, pyg_graph4, num_epochs = 100, dir_path = dir_path)\n",
    "\n",
    "    print(\"Spec:\", hidden_channels)\n",
    "    plot_losses(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7p2jP7I2NnYc"
   },
   "source": [
    "### Load models and rank pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "syyPM-WRsntz",
    "outputId": "fef0d30b-e9c2-4d59-ae83-c6bb9d046699"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding dim 10\n",
      "[0.99757606 0.999267   0.99918914 ... 0.9028003  0.8064209  0.7877489 ]\n",
      "[0.99988544 0.9989915  0.9990175  ... 0.92512727 0.8382548  0.8209337 ]\n",
      "[0.9976989  0.99569863 0.9956225  ... 0.9421289  0.86560744 0.84992063]\n"
     ]
    }
   ],
   "source": [
    "dir_path = \"/content/gdrive/MyDrive/GDS/pickles/removed_nodes/edge_weight\"\n",
    "\n",
    "seed_pages_used = [    \n",
    "    '/find-a-job',\n",
    "    '/universal-credit',\n",
    "    '/government/collections/financial-support-for-businesses-during-coronavirus-covid-19']\n",
    "\n",
    "\n",
    "hidden_channels_list = [[128, 16]]\n",
    "encoding_dims = [10]\n",
    "\n",
    "\n",
    "# hidden_channels_list = [[64, 32], [128, 10],  [128, 16], [32, 16]]\n",
    "\n",
    "# hidden_channels_list = [[64, 32], [64, 10], [32, 10], [32, 16], [32, 32], [128, 16], [64, 64], [128, 64], [128, 32], [64, 16]]\n",
    "# encoding_dim = 10\n",
    "criterion = MSELoss3()\n",
    "\n",
    "results_dict = dict()\n",
    "\n",
    "# encoding_dims = [5, 10, 15]\n",
    "\n",
    "for encoding_dim in encoding_dims:\n",
    "  print(\"Encoding dim\", encoding_dim)\n",
    "\n",
    "  for hidden_channels in hidden_channels_list:\n",
    "\n",
    "    model = GCN_edgeweight(hidden_channels, encoding_dim)\n",
    "\n",
    "    \n",
    "    # model_name, path_saghve = get_model_path_name(hidden_channels, encoding_dim, dir_path)\n",
    "\n",
    "    model_name = \"/model\"\n",
    "    for i in hidden_channels: \n",
    "      model_name = model_name + str(i) + \"_\"\n",
    "    model_name = model_name + \"emb\" + str(encoding_dim) + \".pt\"\n",
    "    path_save = dir_path + model_name\n",
    "\n",
    "    # model_name = str(hidden_channels[0]) + \"_\" + str(hidden_channels[1]) + \".pt\"\n",
    "    # path_save = '/content/gdrive/MyDrive/GDS/model' + model_name\n",
    "\n",
    "    model.load_state_dict(torch.load(path_save))\n",
    "    # print(model)\n",
    "    val_loss, scores_df_rankings, score = scores(model, pyg_graph, neigh_matrix, negative_array, seed_pages_used, edge_weight = True)\n",
    "    \n",
    "    results_dict[model_name] = dict()\n",
    "    results_dict[model_name][\"loss\"] = val_loss\n",
    "    results_dict[model_name][\"score\"] = score\n",
    "    results_dict[model_name][\"ranking\"] = scores_df_rankings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zpTCqfdpEpVj",
    "outputId": "5ca7d18c-e05d-4f78-dbfa-1df9e78a9150"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                     /universal-credit\n",
       "1     /government/collections/financial-support-for-...\n",
       "2                                           /find-a-job\n",
       "3                   /browse/working/rights-trade-unions\n",
       "4                                      /browse/business\n",
       "5                              /browse/employing-people\n",
       "6                      /apply-national-insurance-number\n",
       "7                       /student-finance-register-login\n",
       "8                                    /state-pension-age\n",
       "9                             /sign-in-universal-credit\n",
       "10    /guidance/redundancy-help-finding-work-and-cla...\n",
       "11                        /browse/working/state-pension\n",
       "12                                  /browse/citizenship\n",
       "13                                 /check-state-pension\n",
       "14                                    /carers-allowance\n",
       "15                                        /contact-hmrc\n",
       "16                   /check-job-applicant-right-to-work\n",
       "17                            /browse/benefits/families\n",
       "18                       /view-prove-immigration-status\n",
       "19                                   /become-apprentice\n",
       "Name: page, dtype: object"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dict[model_name][\"ranking\"][\"page\"].iloc[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IAM_fHkLX7Vu"
   },
   "outputs": [],
   "source": [
    "results_dict[model_name][\"ranking\"].to_csv(\"ranking_1308_allnodes_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N-_6J2D3KVk5",
    "outputId": "ffc1f495-3aee-4757-faef-f262931818f6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dump_pickle_file(results_dict, \"results_2seeds\", dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YH9PxXeUuon"
   },
   "source": [
    "### Load rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vrJRyZBQUuoo"
   },
   "outputs": [],
   "source": [
    "dir_path = \"/content/gdrive/MyDrive/GDS/pickles/removed_nodes/edge_weight\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YmKjWYZ6Uuop"
   },
   "outputs": [],
   "source": [
    "results_dict_3seeds = load_pickle_file(\"results_3seeds\", dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XWIijvCLUuop",
    "outputId": "94db4552-ee3e-496c-ca4f-f307af2f4ab3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/model64_32_emb5.pt  Loss:  1.2861221  Score:  -0.03394674058482847\n",
      "/model128_10_emb5.pt  Loss:  1.2850363  Score:  0.04725127203482413\n",
      "/model128_16_emb5.pt  Loss:  1.29036  Score:  -0.010739121481419797\n",
      "/model32_16_emb5.pt  Loss:  1.2867513  Score:  0.01626754403019327\n",
      "/model64_32_emb10.pt  Loss:  1.2936168  Score:  -0.012003119997807263\n",
      "/model128_10_emb10.pt  Loss:  1.2870036  Score:  0.017889384538335128\n",
      "/model128_16_emb10.pt  Loss:  1.282854  Score:  0.026711971229589994\n",
      "/model32_16_emb10.pt  Loss:  1.2901719  Score:  -0.03625320877435465\n",
      "/model64_32_emb15.pt  Loss:  1.2929231  Score:  -0.005242023538842063\n",
      "/model128_10_emb15.pt  Loss:  1.288457  Score:  0.02368291985322333\n",
      "/model128_16_emb15.pt  Loss:  1.2833053  Score:  0.024045519054049218\n",
      "/model32_16_emb15.pt  Loss:  1.2863146  Score:  -0.02014692500267952\n"
     ]
    }
   ],
   "source": [
    "for model_name in results_dict_3seeds.keys():\n",
    "  print(model_name, \" Loss: \", results_dict_3seeds[model_name][\"loss\"].detach().numpy(), \" Score: \", results_dict_3seeds[model_name][\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GFcNQj7MUuop",
    "outputId": "dcf9cf1a-187f-4479-a44d-8ea7f43346fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                     /universal-credit\n",
       "1                                           /find-a-job\n",
       "2     /government/collections/financial-support-for-...\n",
       "3                                       /browse/working\n",
       "4                                  /prove-right-to-work\n",
       "5       /topic/further-education-skills/apprenticeships\n",
       "6     /government/organisations/department-for-work-...\n",
       "7                  /browse/visas-immigration/work-visas\n",
       "8                               /contact-jobcentre-plus\n",
       "9                                 /apply-apprenticeship\n",
       "10                         /browse/working/armed-forces\n",
       "11                        /request-copy-criminal-record\n",
       "12                       /view-prove-immigration-status\n",
       "13                                     /browse/benefits\n",
       "14                             /apply-to-come-to-the-uk\n",
       "15                        /browse/working/state-pension\n",
       "16                     /browse/working/tax-minimum-wage\n",
       "17                                  /browse/citizenship\n",
       "18                     /employers-checks-job-applicants\n",
       "19                                 /check-state-pension\n",
       "Name: page, dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_selected_score = '/model128_16_emb10.pt'\n",
    "results_dict_3seeds[model_selected_score][\"ranking\"].iloc[:20,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UgPAXdyuVe8X"
   },
   "outputs": [],
   "source": [
    "# results_dict_3seeds[model_selected_score][\"ranking\"].iloc[:60,0].to_csv(dir_path + \"/ranking_seed3.csv\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Copy of Fit_nsupervised_gnns_clean. Introduction jr.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
